{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependencies installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /usr/local/lib/python3.11/dist-packages (25.0.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade pip\n",
    "%pip install llama-index llama-index-vector-stores-chroma==0.4.1 huggingface_hub==0.27.1 \\\n",
    "llama-index-embeddings-huggingface==0.5.1 llama-index-llms-huggingface==0.4.2 accelerate>=0.26.0 \\\n",
    "ragas==0.2.12 lmcache==0.1.4 lmcache_vllm==0.6.2.3 pandas==2.2.3 typing_extensions==4.12.2 \\\n",
    "pydantic==2.10.6 chromadb==0.6.3 sentence-transformers==3.4.0 tqdm==4.67.1 transformers==4.48.1 \\\n",
    "datasets==3.2.0 duckdb==1.1.3 cudf-cu12 cuml-cu12 --extra-index-url=https://pypi.nvidia.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries import\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/deepeval/__init__.py:52: UserWarning: You are using deepeval version 2.2.6, however version 2.5.1 is available. You should consider upgrading via the \"pip install --upgrade deepeval\" command.\n",
      "  warnings.warn(\n",
      "\u001b[33mINFO LMCache: \u001b[0mInitializing lmcache_vllm version 0.6.2.3, supporting vllm versions: ['0.6.1.post2', '0.6.1.dev238+ge2c6e0a82'] [2025-03-07 12:55:43,834] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/__init__.py:35\n",
      "\u001b[33mINFO LMCache: \u001b[0mLoading LMCache config file ./lmcache_config.yaml [2025-03-07 12:55:43,888] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:123\n"
     ]
    }
   ],
   "source": [
    "import os, time, threading, sys, datetime, json, math, ast\n",
    "import chromadb\n",
    "import pandas as pd\n",
    "import torch\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "from io import StringIO\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoTokenizer, TextIteratorStreamer, AutoModelForCausalLM\n",
    "from huggingface_hub import login\n",
    "from llama_index.core.vector_stores import VectorStoreQuery\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from llama_index.core import Document, VectorStoreIndex, StorageContext\n",
    "from llama_index.core.schema import TextNode\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core.llama_pack import download_llama_pack\n",
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "os.environ[\"LMCACHE_CONFIG_FILE\"] = \"./lmcache_config.yaml\"\n",
    "import lmcache_vllm.vllm as vllm\n",
    "from lmcache_vllm.vllm import LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom RAPTOR import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RaptorPack = download_llama_pack(\"RaptorPack\", \"./custom_raptor_pack\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Directory: /workspace\n",
      "Files in 'raptor_pack/llama_index' Directory: ['naive_rag_cache_07-03-2025', 'advanced_rag_raptor_07-03-2025', 'naive_rag_07-03-2025', 'sampled_summaries.pkl', 'lmcache_config.yaml', 'vector_store_raptor', 'vector_store_naive_rag', 'Faster-RAG-Experiments.ipynb', 'custom_raptor_pack', '.deepeval', '.deepeval_telemtry.txt', '=0.26.0', '.ipynb_checkpoints']\n",
      "âœ… Using locally modified RaptorPack!\n"
     ]
    }
   ],
   "source": [
    "# Print the current working directory\n",
    "print(\"Current Directory:\", os.getcwd())\n",
    "\n",
    "raptor_pack_path = sys.path.insert(0, os.path.join(os.getcwd(), \"custom_raptor_pack\"))\n",
    "\n",
    "# Verify the files in the 'raptor_pack/llama_index' directory\n",
    "print(\"Files in 'raptor_pack/llama_index' Directory:\", os.listdir(raptor_pack_path))\n",
    "\n",
    "# Import RaptorPack\n",
    "from llama_index.packs.raptor import RaptorRetriever\n",
    "from custom_raptor_pack.llama_index.packs.raptor.base import RaptorPack\n",
    "print(\"âœ… Using locally modified RaptorPack!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enable GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### APIs keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeepEval Key: oxzVhgRVKo8eEspdkldpsdskod5+CvN/IUZbNyNKzs1cJbPw=\n",
      "OpenAI API Key: sk-proj-i4R5gYfQudspsdllsddlsdsakrzyh--\n",
      "/usr/local/lib/python3.11/dist-packages/deepeval/__init__.py:52: UserWarning: You are using deepeval version 2.2.6, however version 2.5.1 is available. You should consider upgrading via the \"pip install --upgrade deepeval\" command.\n",
      "  warnings.warn(\n",
      "Welcome to \u001b[1mDeepEval\u001b[0m!\n",
      "Login and grab your API key here: \u001b]8;id=517129;https://app.confident-ai.com\u001b\\\u001b[4;94mhttps://app.confident-ai.com\u001b[0m\u001b]8;;\u001b\\ \n",
      "Congratulations! Login successful ðŸ™Œ \n",
      "If you are new to DeepEval, follow our quickstart tutorial here: \n",
      "\u001b]8;id=374700;https://docs.confident-ai.com/docs/getting-started\u001b\\\u001b[1;4;94mhttps://docs.confident-ai.com/docs/getting-started\u001b[0m\u001b]8;;\u001b\\\n"
     ]
    }
   ],
   "source": [
    "# Huggingface\n",
    "login(token=\"your hugginface token\")\n",
    "\n",
    "# Open AI\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"your openai api key\"\n",
    "\n",
    "# Verify that the environment variables are set\n",
    "print(\"OpenAI API Key:\", os.environ.get(\"OPENAI_API_KEY\")) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_id, tokenizer, optimized=False):\n",
    "    \"\"\"Load the language model with or without LMCache optimization.\"\"\"\n",
    "    if optimized:\n",
    "        # Instantiate a synchronous LMCache-enabled LLM.\n",
    "        # Adjust parameters such as gpu_memory_utilization and max_model_len as needed.\n",
    "        return LLM(\n",
    "            model=model_id,\n",
    "            gpu_memory_utilization=0.8,\n",
    "            enable_chunked_prefill=False,\n",
    "            max_model_len=32768\n",
    "        )\n",
    "    else:\n",
    "        # Load the non-optimized model via Hugging Face and send it to GPU if available.\n",
    "        return AutoModelForCausalLM.from_pretrained(\n",
    "            model_id,\n",
    "            torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32\n",
    "        ).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAG pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_queries(queries, retriever, model, tokenizer, experiment_name):\n",
    "    # Setup folder and file names.\n",
    "    date_str = datetime.date.today().strftime(\"%d-%m-%Y\")\n",
    "    folder_name = f\"{experiment_name}_{date_str}\"\n",
    "    if not os.path.exists(folder_name):\n",
    "        os.makedirs(folder_name)\n",
    "    csv_filename = os.path.join(folder_name, f\"queries_results_{experiment_name}_{date_str}.csv\")\n",
    "    metrics_filename = os.path.join(folder_name, f\"overall_metrics_{experiment_name}_{date_str}.txt\")\n",
    "    \n",
    "    # Load cached results if available.\n",
    "    if os.path.exists(csv_filename):\n",
    "        print(f\"{csv_filename} exists. Loading cached results...\")\n",
    "        df = pd.read_csv(csv_filename)\n",
    "        responses = df.to_dict(orient=\"records\")\n",
    "        for row in responses:\n",
    "            metadata = {\n",
    "                \"tfft\": row.get(\"tfft\"),\n",
    "                \"e2e_latency\": row.get(\"e2e_latency\"),\n",
    "                \"itl\": row.get(\"itl\"),\n",
    "                \"tps\": row.get(\"tps\"),\n",
    "                \"retriever_time\": row.get(\"retriever_time\"),\n",
    "                \"input_context_size\": row.get(\"input_context_size\"),\n",
    "                \"output_tokens_size\": row.get(\"output_tokens_size\")\n",
    "            }\n",
    "            context = row.get(\"context\", \"\")\n",
    "\n",
    "        return responses\n",
    "\n",
    "    responses = []\n",
    "    \n",
    "    # Lists for metrics.\n",
    "    all_tfft = []\n",
    "    all_e2e_latency = []\n",
    "    all_itl = []\n",
    "    all_tps = []\n",
    "    all_retriever_time = []\n",
    "    all_input_context_size = []\n",
    "    all_output_tokens_size = []\n",
    "    \n",
    "    overall_start_time = time.time()\n",
    "    today_date_verbose = datetime.date.today().strftime(\"%d %B %Y\")\n",
    "    \n",
    "    for i, row in tqdm(queries.iterrows(), desc=\"Processing Queries\", total=len(queries)):\n",
    "        query = row['question']['text']\n",
    "        query_obj = VectorStoreQuery(query_str=query, similarity_top_k=50)\n",
    "        \n",
    "        # Retrieve context.\n",
    "        retriever_start_time = time.time()\n",
    "        results = retriever.query(query_obj)\n",
    "        retriever_time = time.time() - retriever_start_time\n",
    "        all_retriever_time.append(retriever_time)\n",
    "        retrieved_nodes = results.nodes\n",
    "        context = \"\\n\".join(node.get_content() for node in retrieved_nodes)\n",
    "\n",
    "        # Build the prompt.\n",
    "        prompt = (\n",
    "            \"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\"\n",
    "            f\"Cutting Knowledge Date: December 2023\\n\"\n",
    "            f\"Today Date: {today_date_verbose}\\n\\n\"\n",
    "            \"You are a helpful assistant. Please provide a concise and accurate answer in one short paragraph based on the context.\\n\"\n",
    "            \"Use the provided context only, do not invent.\\n\"\n",
    "            \"Avoid repeating yourself\\n\"\n",
    "            \"<|eot_id|>\\n\"\n",
    "            \"<|start_header_id|>user<|end_header_id|>\\n\"\n",
    "            f\"Question: {query}\\n\"\n",
    "            f\"Context:\\n{context}\\n\"\n",
    "            \"<|eot_id|>\\n\"\n",
    "            \"<|start_header_id|>assistant<|end_header_id|>\\n\"\n",
    "        )\n",
    "        \n",
    "        # Tokenize to get input context size.\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "        input_context_size = inputs.input_ids.shape[1]\n",
    "        all_input_context_size.append(input_context_size)\n",
    "        \n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        inputs = inputs.to(device)\n",
    "        \n",
    "        if \"cache\" in experiment_name:\n",
    "            # --- LMCache branch ---\n",
    "            sampling_params = vllm.SamplingParams(temperature=0.0, max_tokens=128)\n",
    "            generation_start = time.time()\n",
    "            outputs = model.generate([prompt], sampling_params)\n",
    "            generation_end = time.time()\n",
    "            \n",
    "            # Extract metrics from the first output.\n",
    "            request_output = outputs[0]\n",
    "            metrics_obj = request_output.metrics\n",
    "            tfft = metrics_obj.first_token_time - metrics_obj.arrival_time\n",
    "        \n",
    "            answer = request_output.outputs[0].text.strip()\n",
    "            e2e_latency = generation_end - generation_start\n",
    "            token_count = len(tokenizer.encode(answer))\n",
    "            itl = (e2e_latency / token_count) if token_count > 1 else 0.0\n",
    "            tps = (token_count / e2e_latency) if e2e_latency > 0 else 0.0\n",
    "        \n",
    "            all_tfft.append(tfft)\n",
    "        else:\n",
    "            # --- Non-cache branch: using synchronous generation with a streamer ---\n",
    "            start_time_sync = time.time()\n",
    "            streamer = TextIteratorStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "            generation_thread = threading.Thread(\n",
    "                target=model.generate,\n",
    "                kwargs={\"inputs\": inputs.input_ids, \"max_new_tokens\": 128, \"streamer\": streamer}\n",
    "            )\n",
    "            generation_thread.start()\n",
    "            token_times = []\n",
    "            generated_text = \"\"\n",
    "            for token in streamer:\n",
    "                now = time.time()\n",
    "                token_times.append(now)\n",
    "                if len(token_times) == 1:\n",
    "                    tfft = now - start_time_sync\n",
    "                    all_tfft.append(tfft)\n",
    "                generated_text += token\n",
    "            generation_thread.join()\n",
    "            answer = generated_text.strip()\n",
    "            end_time_sync = time.time()\n",
    "            e2e_latency = end_time_sync - start_time_sync\n",
    "            if len(token_times) > 1:\n",
    "                diffs = [t2 - t1 for t1, t2 in zip(token_times[:-1], token_times[1:])]\n",
    "                itl = sum(diffs) / len(diffs)\n",
    "                generation_time = token_times[-1] - token_times[0]\n",
    "                tps = len(token_times) / generation_time if generation_time > 0 else 0.0\n",
    "            else:\n",
    "                itl = 0.0\n",
    "                tps = 0.0\n",
    "\n",
    "        all_e2e_latency.append(e2e_latency)\n",
    "        all_itl.append(itl)\n",
    "        all_tps.append(tps)\n",
    "        \n",
    "        token_count = len(tokenizer.encode(answer))\n",
    "        all_output_tokens_size.append(token_count)\n",
    "        \n",
    "        metadata = {\n",
    "            \"tfft\": tfft,\n",
    "            \"e2e_latency\": e2e_latency,\n",
    "            \"itl\": itl,\n",
    "            \"tps\": tps,\n",
    "            \"retriever_time\": retriever_time,\n",
    "            \"input_context_size\": input_context_size,\n",
    "            \"output_tokens_size\": token_count\n",
    "        }\n",
    "        \n",
    "        responses.append({\n",
    "            \"query\": query,\n",
    "            \"context\": context,\n",
    "            \"answer\": answer,\n",
    "            \"expected_output\": row['answers'][0]['text'],\n",
    "            **metadata\n",
    "        })\n",
    "    \n",
    "    overall_end_time = time.time()\n",
    "    total_time = overall_end_time - overall_start_time\n",
    "    rps = len(queries) / total_time if total_time > 0 else 0.0\n",
    "    avg_tfft = sum(all_tfft) / len(all_tfft) if all_tfft else 0.0\n",
    "    avg_e2e = sum(all_e2e_latency) / len(all_e2e_latency) if all_e2e_latency else 0.0\n",
    "    avg_itl = sum(all_itl) / len(all_itl) if all_itl else 0.0\n",
    "    avg_tps = sum(all_tps) / len(all_tps) if all_tps else 0.0\n",
    "    avg_retriever = sum(all_retriever_time) / len(all_retriever_time) if all_retriever_time else 0.0\n",
    "    avg_input_ctx = sum(all_input_context_size) / len(all_input_context_size) if all_input_context_size else 0.0\n",
    "    avg_output_tokens = sum(all_output_tokens_size) / len(all_output_tokens_size) if all_output_tokens_size else 0.0\n",
    "    \n",
    "    metrics_str = (\n",
    "        f\"Overall RPS: {rps:.2f} req/sec, \"\n",
    "        f\"Average TTFT: {avg_tfft:.2f} sec, \"\n",
    "        f\"Average E2E Latency: {avg_e2e:.2f} sec, \"\n",
    "        f\"Average ITL: {avg_itl:.2f} sec, \"\n",
    "        f\"Average TPS: {avg_tps:.2f} tokens/sec, \"\n",
    "        f\"Average Retriever Time: {avg_retriever:.2f} sec, \"\n",
    "        f\"Average Input Context Size: {avg_input_ctx:.0f} tokens, \"\n",
    "        f\"Average Output Tokens Size: {avg_output_tokens:.0f} tokens\"\n",
    "    )\n",
    "    print(\"\\nOverall Metrics:\")\n",
    "    print(metrics_str)\n",
    "    \n",
    "    with open(metrics_filename, \"w\") as mf:\n",
    "        mf.write(metrics_str)\n",
    "    print(f\"Saved overall metrics to {metrics_filename}\")\n",
    "    \n",
    "    df = pd.DataFrame(responses)\n",
    "    df.to_csv(csv_filename, index=False)\n",
    "    print(f\"Saved intermediate results to {csv_filename}\")\n",
    "    \n",
    "    return responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset loading and preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2370702a3d634bd4bd487e3d15c4936f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/10.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96aaf04f4a48455dbe92fda7aa71861d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fba9d77a5664e10a98068eb0d4230c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1aabaaf0d386453eaaa3ca901d370090",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0/24 [00:00<?, ?files/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c28bc9078df443af98deeec770030e7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00003-of-00024.parquet:   0%|          | 0.00/27.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88be6db5af9842e8aec604cc000145c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00004-of-00024.parquet:   0%|          | 0.00/88.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9b1088e7bdf45298715b92f4600a7c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00024.parquet:   0%|          | 0.00/9.80M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbb76be8540045179b382136625b9620",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00014-of-00024.parquet:   0%|          | 0.00/35.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "567e16922bfd405ab4411ffe5e9f798c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00013-of-00024.parquet:   0%|          | 0.00/136M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2367fdbbf234e329fcb4896e22708b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00001-of-00024.parquet:   0%|          | 0.00/67.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e8a96d4d5244d8c9bd7e471fe095808",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00006-of-00024.parquet:   0%|          | 0.00/39.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14bc557c2c2f40109b3b354331f7d755",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00007-of-00024.parquet:   0%|          | 0.00/132M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79f8faa76ffd4932b2e90729bf6ee99d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00008-of-00024.parquet:   0%|          | 0.00/10.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "965e5b209ef64b2f9cda27f7665cd8a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00011-of-00024.parquet:   0%|          | 0.00/13.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cc544f736d24579a9cf76922b16b9c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00009-of-00024.parquet:   0%|          | 0.00/49.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92a29b2d3e654796ac6e573868ec2353",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00015-of-00024.parquet:   0%|          | 0.00/73.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fed06b8b886e493d9c8d2271ab405bc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00005-of-00024.parquet:   0%|          | 0.00/206M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "468a2ff208aa4740b09720f6a1b49e3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00002-of-00024.parquet:   0%|          | 0.00/233M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5032ec8b15be41b6a291336cb9cdb231",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00010-of-00024.parquet:   0%|          | 0.00/126M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "770012b771144dfa934522b88d81d2b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00016-of-00024.parquet:   0%|          | 0.00/194M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e268448a501649da8951c3873a292411",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00017-of-00024.parquet:   0%|          | 0.00/61.6M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a46d88040684578849ffe1797b4df34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00018-of-00024.parquet:   0%|          | 0.00/107M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec4333b8340c4bf9bffced9026a06074",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00019-of-00024.parquet:   0%|          | 0.00/195M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f811a9c22d254dc59a015e4ee188894f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00020-of-00024.parquet:   0%|          | 0.00/74.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aee7e26a564244d895d28e6f46910a93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00021-of-00024.parquet:   0%|          | 0.00/178M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "854833ea778b48f698a2e937c81169b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00012-of-00024.parquet:   0%|          | 0.00/105M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98173d24cbe94994a39d9af9e25f541f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00022-of-00024.parquet:   0%|          | 0.00/11.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf94caea5ce1497ab6faf20b08e691bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00023-of-00024.parquet:   0%|          | 0.00/97.8M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "299f0866c5f041ab9bfbadeb03e9fba6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test-00000-of-00008.parquet:   0%|          | 0.00/8.56M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae76b78b600f461e90cb26dff4c881ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test-00001-of-00008.parquet:   0%|          | 0.00/44.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f71ab37fd34141b7b867f05801992f71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test-00002-of-00008.parquet:   0%|          | 0.00/101M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f63815eae0424bda9cb269dfa082ba70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test-00003-of-00008.parquet:   0%|          | 0.00/222M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e12f6dbc2658461eb8d000a1fd0d9e60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test-00004-of-00008.parquet:   0%|          | 0.00/60.8M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bacd2c53bbcb400b959a0de8b7b75986",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test-00005-of-00008.parquet:   0%|          | 0.00/121M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ab9202aed844a5885481e229f00c18a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test-00006-of-00008.parquet:   0%|          | 0.00/243M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "083f743a76df46f8aab9a837a8601f15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test-00007-of-00008.parquet:   0%|          | 0.00/58.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ae8a6bde6364d0798fc929836a200f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation-00000-of-00003.parquet:   0%|          | 0.00/10.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d27b50a82df44519aa39c453cdee5071",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation-00001-of-00003.parquet:   0%|          | 0.00/24.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eab2470fbabd42459e4797082540713b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation-00002-of-00003.parquet:   0%|          | 0.00/68.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3935c41afb0f412a8691872282798950",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/32747 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95c806fe846d469d84afca3a02ba2b78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/10557 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "274e79736d364b7a9798f21536641b05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/3461 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc1622c44c174aa5ad53840dfbae53b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['document', 'question', 'answers'],\n",
      "        num_rows: 32747\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['document', 'question', 'answers'],\n",
      "        num_rows: 10557\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['document', 'question', 'answers'],\n",
      "        num_rows: 3461\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "ds = load_dataset('deepmind/narrativeqa')\n",
    "print(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows across all splits: 46765\n",
      "Number of unique summaries: 1572\n",
      "Number of sampled rows: 1572\n",
      "Total rows after repeating each sampled row 10 times: 1572\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document</th>\n",
       "      <th>question</th>\n",
       "      <th>answers</th>\n",
       "      <th>summary_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'id': '09809dc4e1e63a28e32c8182d8493667a91b54...</td>\n",
       "      <td>{'text': 'What does Mrs. Tittlemouse keep besi...</td>\n",
       "      <td>[{'text': 'A dust pan and brush', 'tokens': ['...</td>\n",
       "      <td>Mrs. Tittlemouse is a tale in which no humans...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'id': 'bebb3d64b5731fd2cf2e4555686b4959eb2ac4...</td>\n",
       "      <td>{'text': 'What has developed in the Earth's co...</td>\n",
       "      <td>[{'text': 'An advanced race', 'tokens': ['An',...</td>\n",
       "      <td>The plot concerns an advanced race which has ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'id': '2daf99dd0a17e8141b6697c4dad2621082cf54...</td>\n",
       "      <td>{'text': 'Why does Alan Stanwyk think Fletch i...</td>\n",
       "      <td>[{'text': 'Because Fletch is posing as a junki...</td>\n",
       "      <td>Los Angeles Times reporter Irwin \"Fletch\" Fle...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'id': '7e2bef0b43cf243f513853e82e482d695801e4...</td>\n",
       "      <td>{'text': 'The Titus Brothers Contractors won a...</td>\n",
       "      <td>[{'text': 'Peru', 'tokens': ['Peru']}, {'text'...</td>\n",
       "      <td>The Titus Brothers Contractors company have w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'id': '7805e001c28ab9aa85f51b3244a9f57fe972f5...</td>\n",
       "      <td>{'text': 'What does the Pope declare has been ...</td>\n",
       "      <td>[{'text': 'Dire Offense ', 'tokens': ['Dire', ...</td>\n",
       "      <td>In the beginning of this mock-epic, Pope decl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>{'id': '8ec2acc82d024a645fe81f51270d6dbba8b798...</td>\n",
       "      <td>{'text': 'Where does this story take place?', ...</td>\n",
       "      <td>[{'text': 'In Socrates Cell?', 'tokens': ['In'...</td>\n",
       "      <td>The dialogue takes place in Socrates' prison ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>{'id': '7abce7387dae92c0d09c03d26bf6407237a8c7...</td>\n",
       "      <td>{'text': 'Why does Hammerly want to block the ...</td>\n",
       "      <td>[{'text': 'He believes it'll harm the privacy ...</td>\n",
       "      <td>In the 1990s, U.S. National Security Agency o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>{'id': '3feb46d105b7ef3bdb248064761dc309c18314...</td>\n",
       "      <td>{'text': 'How many siblings does Chip have?', ...</td>\n",
       "      <td>[{'text': '1', 'tokens': ['1']}, {'text': 'One...</td>\n",
       "      <td>Beverly Sutphin appears to be a typical subur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>{'id': '739ac60705adb8084f1656b8bdb29e58e41f49...</td>\n",
       "      <td>{'text': 'When he leaves baseball, where does ...</td>\n",
       "      <td>[{'text': 'In New York City.', 'tokens': ['In'...</td>\n",
       "      <td>Miguel \"Sugar\" Santos (Perez Soto) spends his...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>{'id': '0edf4e67b33906e690dc45a9abae06c34ca3e1...</td>\n",
       "      <td>{'text': 'Which of the brother's kills the loc...</td>\n",
       "      <td>[{'text': 'Tuvia.', 'tokens': ['Tuvia', '.']},...</td>\n",
       "      <td>The film opens with on-screen text stating: \"...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            document  \\\n",
       "0  {'id': '09809dc4e1e63a28e32c8182d8493667a91b54...   \n",
       "1  {'id': 'bebb3d64b5731fd2cf2e4555686b4959eb2ac4...   \n",
       "2  {'id': '2daf99dd0a17e8141b6697c4dad2621082cf54...   \n",
       "3  {'id': '7e2bef0b43cf243f513853e82e482d695801e4...   \n",
       "4  {'id': '7805e001c28ab9aa85f51b3244a9f57fe972f5...   \n",
       "5  {'id': '8ec2acc82d024a645fe81f51270d6dbba8b798...   \n",
       "6  {'id': '7abce7387dae92c0d09c03d26bf6407237a8c7...   \n",
       "7  {'id': '3feb46d105b7ef3bdb248064761dc309c18314...   \n",
       "8  {'id': '739ac60705adb8084f1656b8bdb29e58e41f49...   \n",
       "9  {'id': '0edf4e67b33906e690dc45a9abae06c34ca3e1...   \n",
       "\n",
       "                                            question  \\\n",
       "0  {'text': 'What does Mrs. Tittlemouse keep besi...   \n",
       "1  {'text': 'What has developed in the Earth's co...   \n",
       "2  {'text': 'Why does Alan Stanwyk think Fletch i...   \n",
       "3  {'text': 'The Titus Brothers Contractors won a...   \n",
       "4  {'text': 'What does the Pope declare has been ...   \n",
       "5  {'text': 'Where does this story take place?', ...   \n",
       "6  {'text': 'Why does Hammerly want to block the ...   \n",
       "7  {'text': 'How many siblings does Chip have?', ...   \n",
       "8  {'text': 'When he leaves baseball, where does ...   \n",
       "9  {'text': 'Which of the brother's kills the loc...   \n",
       "\n",
       "                                             answers  \\\n",
       "0  [{'text': 'A dust pan and brush', 'tokens': ['...   \n",
       "1  [{'text': 'An advanced race', 'tokens': ['An',...   \n",
       "2  [{'text': 'Because Fletch is posing as a junki...   \n",
       "3  [{'text': 'Peru', 'tokens': ['Peru']}, {'text'...   \n",
       "4  [{'text': 'Dire Offense ', 'tokens': ['Dire', ...   \n",
       "5  [{'text': 'In Socrates Cell?', 'tokens': ['In'...   \n",
       "6  [{'text': 'He believes it'll harm the privacy ...   \n",
       "7  [{'text': '1', 'tokens': ['1']}, {'text': 'One...   \n",
       "8  [{'text': 'In New York City.', 'tokens': ['In'...   \n",
       "9  [{'text': 'Tuvia.', 'tokens': ['Tuvia', '.']},...   \n",
       "\n",
       "                                        summary_text  \n",
       "0   Mrs. Tittlemouse is a tale in which no humans...  \n",
       "1   The plot concerns an advanced race which has ...  \n",
       "2   Los Angeles Times reporter Irwin \"Fletch\" Fle...  \n",
       "3   The Titus Brothers Contractors company have w...  \n",
       "4   In the beginning of this mock-epic, Pope decl...  \n",
       "5   The dialogue takes place in Socrates' prison ...  \n",
       "6   In the 1990s, U.S. National Security Agency o...  \n",
       "7   Beverly Sutphin appears to be a typical subur...  \n",
       "8   Miguel \"Sugar\" Santos (Perez Soto) spends his...  \n",
       "9   The film opens with on-screen text stating: \"...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# File to store the sampled summaries as a pickle.\n",
    "sampled_filename = \"sampled_summaries.pkl\"\n",
    "\n",
    "# Combine all splits into a single DataFrame (assuming ds is your dataset object)\n",
    "dfs = []\n",
    "for split in ['train', 'test', 'validation']:\n",
    "    split_df = ds[split].to_pandas()\n",
    "    dfs.append(split_df)\n",
    "df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Extract summary text into a new column.\n",
    "df['summary_text'] = df['document'].apply(lambda doc: doc['summary']['text'])\n",
    "\n",
    "# Remove duplicates based on the 'summary_text' column.\n",
    "unique_summaries = df.drop_duplicates(subset='summary_text').reset_index(drop=True)\n",
    "\n",
    "# Check if the sample already exists.\n",
    "if os.path.exists(sampled_filename):\n",
    "    sampled_summaries = pd.read_pickle(sampled_filename)\n",
    "else:\n",
    "    # Sample 100 random rows from unique_summaries.\n",
    "    sampled_summaries = unique_summaries.sample(n=1572, random_state=42)\n",
    "    # If the \"question\" column doesn't exist or you want to create it based on summary_text:\n",
    "    if \"question\" not in sampled_summaries.columns:\n",
    "        sampled_summaries[\"question\"] = sampled_summaries[\"summary_text\"].apply(lambda x: {\"text\": x})\n",
    "    # Save the sample using pickle.\n",
    "    sampled_summaries.to_pickle(sampled_filename)\n",
    "\n",
    "# Repeat each row 10 times to get 1000 rows overall.\n",
    "repeated_summaries = sampled_summaries.loc[sampled_summaries.index.repeat(1)].reset_index(drop=True)\n",
    "\n",
    "# Print stats for verification.\n",
    "print(f\"Total rows across all splits: {len(df)}\")\n",
    "print(f\"Number of unique summaries: {len(unique_summaries)}\")\n",
    "print(f\"Number of sampled rows: {len(sampled_summaries)}\")\n",
    "print(f\"Total rows after repeating each sampled row 10 times: {len(repeated_summaries)}\")\n",
    "\n",
    "# Optionally display the first few rows.\n",
    "display(repeated_summaries.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total documents created: 1572\n",
      "Total chunks created: 14738\n",
      "Total chunks documents extracted: 14738\n",
      "Total ids created: 14738\n"
     ]
    }
   ],
   "source": [
    "documents = [Document(text=row['summary_text']) for _, row in unique_summaries.iterrows()]\n",
    "parser = SentenceSplitter(chunk_size=100, chunk_overlap=0)\n",
    "\n",
    "chunks = parser.get_nodes_from_documents(documents)\n",
    "chunk_documents = [chunk.get_content() for chunk in chunks]\n",
    "ids = [f\"chunk_{i}\" for i in range(len(chunks))]\n",
    "\n",
    "print(f\"Total documents created: {len(documents)}\")\n",
    "print(f\"Total chunks created: {len(chunks)}\")\n",
    "print(f\"Total chunks documents extracted: {len(chunk_documents)}\")\n",
    "print(f\"Total ids created: {len(ids)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexing collections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ChromaDB client config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33mINFO LMCache: \u001b[0mLoad pretrained SentenceTransformer: multi-qa-mpnet-base-cos-v1 [2025-03-07 12:57:44,423] -- /usr/local/lib/python3.11/dist-packages/sentence_transformers/SentenceTransformer.py:218\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f014fffba1ec41989f8c3d1316850f84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "274efe79710e4f6aa3c017a5ebdd7ed6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00621099eef34dc9b88748219dc9b646",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/9.25k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8132e4976fa4bbcb63fd8385f7914e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87bde27e656c4096afce11943710707f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bac81f9e9b20434b82a92c500ea9325f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "498acc387145475fa261dde49d716c5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1dbab7d7bf9d4fa4a7dfbee3bd229e0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f11446707a60435983e752703c7c91a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "965a4f9ca84f41fd98e92dfaed4d1e37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b081e691e06d432a81d445536f8f4c3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33mINFO LMCache: \u001b[0m2 prompts are loaded, with the keys: ['query', 'text'] [2025-03-07 12:57:48,876] -- /usr/local/lib/python3.11/dist-packages/sentence_transformers/SentenceTransformer.py:357\n"
     ]
    }
   ],
   "source": [
    "client_naive = chromadb.PersistentClient(path=\"./vector_store_naive_rag\")\n",
    "client_raptor = chromadb.PersistentClient(path=\"./vector_store_raptor\")\n",
    "embed_model = HuggingFaceEmbedding(\"multi-qa-mpnet-base-cos-v1\", device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NarrativeQA: Naive-RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection(name=narrativeqa-naive)\n"
     ]
    }
   ],
   "source": [
    "collection_name_naive = \"narrativeqa-naive\"\n",
    "#client.delete_collection(name=collection_name_naive)\n",
    "collection_naive = client_naive.get_or_create_collection(collection_name_naive)\n",
    "print(collection_naive)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total documents in narrativeqa-naive collection after indexing: 14738\n"
     ]
    }
   ],
   "source": [
    "if collection_naive.count () == 0:\n",
    "    embeddings = embed_model.get_text_embedding_batch(\n",
    "        chunk_documents,\n",
    "        device=device,\n",
    "        show_progress=True\n",
    "    )\n",
    "        \n",
    "    collection_naive.add(\n",
    "        documents=chunk_documents,\n",
    "        ids=ids,\n",
    "        embeddings=embeddings\n",
    "    )\n",
    "    \n",
    "vector_store_naive_rag = ChromaVectorStore(\n",
    "    client=client_naive, \n",
    "    chroma_collection=collection_naive\n",
    ")\n",
    "\n",
    "# Saving naive rag vector store to avoid re-indexing\n",
    "storage_context_naive = StorageContext.from_defaults(vector_store=vector_store_naive_rag)\n",
    "\n",
    "print(f\"Total documents in {collection_name_naive} collection after indexing: {collection_naive.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NarrativeQA: RAPTOR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection(name=narrativeqa-raptor)\n"
     ]
    }
   ],
   "source": [
    "collection_name_raptor = \"narrativeqa-raptor\"\n",
    "#client_raptor.delete_collection(name=collection_name_raptor)\n",
    "collection_raptor = client_raptor.get_or_create_collection(collection_name_raptor)\n",
    "print(collection_raptor)\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total documents in narrativeqa-raptor collection after indexing: 15825\n"
     ]
    }
   ],
   "source": [
    "vector_store_raptor = ChromaVectorStore(chroma_collection=collection_raptor)\n",
    "storage_context_raptor = StorageContext.from_defaults(vector_store=vector_store_raptor)\n",
    "\n",
    "if collection_raptor.count() == 0:\n",
    "    raptor_pack = RaptorPack(\n",
    "        documents=chunk_documents,\n",
    "        ids=ids,\n",
    "        embed_model=embed_model,\n",
    "        llm=OpenAI(model=\"gpt-3.5-turbo\", temperature=0.1),\n",
    "        vector_store=vector_store_raptor, \n",
    "        mode=\"collapsed\",\n",
    "        #transformations=[parser]\n",
    "    )\n",
    "\n",
    "print(f\"Total documents in {collection_name_raptor} collection after indexing: {collection_raptor.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models and tokenizer config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9eadef7b5b3a4682b3e93cee5962122c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/55.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b36b883c04b144b58e636a7e2a540e8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30f719cdc18b497a938bdb71c525f182",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_id = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_model = load_model(model_id, tokenizer, optimized=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment configs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive-RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_naive_rag(queries, vector_store, tokenizer, model, experiment_name):\n",
    "    return process_queries(queries, vector_store, model, tokenizer, experiment_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Advanced RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_advanced_rag(queries, vector_store, tokenizer, model, experiment_name):\n",
    "    return process_queries(queries, vector_store, model, tokenizer, experiment_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive-RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Running Naive RAG\")\n",
    "naive_rag_responses = run_naive_rag(repeated_summaries, vector_store_naive_rag, tokenizer, generator_model, \"naive_rag\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced RAG (Raptor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Running Advanced RAG (Raptor)\")\n",
    "advanced_rag_responses = run_advanced_rag(repeated_summaries, vector_store_raptor, tokenizer, generator_model, \"advanced_rag_raptor\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean generator model from GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del generator_model\n",
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator optimized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-07 13:44:17 llm_engine.py:226] Initializing an LLM engine (v0.6.1.dev238+ge2c6e0a82) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, multi_step_stream_outputs=False, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, mm_processor_kwargs=None)\n",
      "INFO 03-07 13:44:18 model_runner.py:1014] Starting to load model meta-llama/Llama-3.1-8B-Instruct...\n",
      "INFO 03-07 13:44:18 weight_utils.py:242] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a35ecdf55e044e3a4878b0ddf1e9ef7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-07 13:44:22 model_runner.py:1025] Loading model weights took 14.9575 GB\n",
      "INFO 03-07 13:44:26 gpu_executor.py:122] # GPU blocks: 9811, # CPU blocks: 2048\n",
      "INFO 03-07 13:44:27 model_runner.py:1329] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 03-07 13:44:27 model_runner.py:1333] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 03-07 13:44:51 model_runner.py:1456] Graph capturing finished in 24 secs.\n"
     ]
    }
   ],
   "source": [
    "generator_optimized_model = load_model(model_id, tokenizer, optimized=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive RAG (LM Cache Optimized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Naive RAG (LM Cache Optimized)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Queries:   0%|          | 0/1000 [00:00<?, ?it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:00:36,340] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:00:36,346] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 0 chunks [2025-03-07 13:00:36,347] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:385\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mReturning the original input! [2025-03-07 13:00:36,349] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:663\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.PREFILL: 1>] [2025-03-07 13:00:36,909] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the put() method [2025-03-07 13:00:36,997] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:99\n",
      "\u001b[33mINFO LMCache: \u001b[0mStored/updated 16 chunks, total time 0.00s, make chunks time 0.00s [2025-03-07 13:00:36,998] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:315\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 0 tokens and then stores 4049 tokens [2025-03-07 13:00:36,999] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.11s/it, est. speed input: 3657.95 toks/s, output: 14.45 toks/s]\u001b[A\n",
      "Processing Queries:   0%|          | 1/1000 [00:01<19:43,  1.18s/it]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:00:37,485] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:00:37,487] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 16 chunks -- elapsed time 0.0003371965140104294 [2025-03-07 13:00:37,489] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 16 chunks (4049 tokens in total) --hit rate 50.00% -- elapsed time 0.0023543424904346466 [2025-03-07 13:00:37,489] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4048 [2025-03-07 13:00:37,490] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:00:37,496] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:00:37,522] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4049 tokens and then stores 0 tokens [2025-03-07 13:00:37,524] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.12it/s, est. speed input: 8633.80 toks/s, output: 34.12 toks/s]\u001b[A\n",
      "Processing Queries:   0%|          | 2/1000 [00:01<13:06,  1.27it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:00:37,986] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:00:37,988] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 16 chunks -- elapsed time 0.0002519916743040085 [2025-03-07 13:00:37,989] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 16 chunks (4049 tokens in total) --hit rate 66.67% -- elapsed time 0.002116503193974495 [2025-03-07 13:00:37,990] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4048 [2025-03-07 13:00:37,991] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:00:37,996] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:00:38,022] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4049 tokens and then stores 0 tokens [2025-03-07 13:00:38,023] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.15it/s, est. speed input: 8743.43 toks/s, output: 34.55 toks/s]\u001b[A\n",
      "Processing Queries:   0%|          | 3/1000 [00:02<10:52,  1.53it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:00:38,482] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:00:38,484] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 16 chunks -- elapsed time 0.0004909709095954895 [2025-03-07 13:00:38,486] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 16 chunks (4049 tokens in total) --hit rate 75.00% -- elapsed time 0.0024713892489671707 [2025-03-07 13:00:38,487] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4048 [2025-03-07 13:00:38,488] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:00:38,493] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:00:38,518] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4049 tokens and then stores 0 tokens [2025-03-07 13:00:38,520] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.14it/s, est. speed input: 8732.15 toks/s, output: 34.50 toks/s]\u001b[A\n",
      "Processing Queries:   0%|          | 4/1000 [00:02<09:50,  1.69it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:00:38,981] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:00:38,983] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 16 chunks -- elapsed time 0.00026776641607284546 [2025-03-07 13:00:38,984] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 16 chunks (4049 tokens in total) --hit rate 80.00% -- elapsed time 0.00226416252553463 [2025-03-07 13:00:38,985] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4048 [2025-03-07 13:00:38,987] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:00:38,992] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:00:39,017] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4049 tokens and then stores 0 tokens [2025-03-07 13:00:39,019] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.14it/s, est. speed input: 8717.42 toks/s, output: 34.45 toks/s]\u001b[A\n",
      "Processing Queries:   0%|          | 5/1000 [00:03<09:16,  1.79it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:00:39,483] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:00:39,485] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 16 chunks -- elapsed time 0.000255391001701355 [2025-03-07 13:00:39,486] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 16 chunks (4049 tokens in total) --hit rate 83.33% -- elapsed time 0.0021905992180109024 [2025-03-07 13:00:39,487] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4048 [2025-03-07 13:00:39,488] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:00:39,494] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:00:39,519] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4049 tokens and then stores 0 tokens [2025-03-07 13:00:39,521] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.14it/s, est. speed input: 8701.59 toks/s, output: 34.38 toks/s]\u001b[A\n",
      "Processing Queries:   1%|          | 6/1000 [00:03<08:56,  1.85it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:00:39,981] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:00:39,983] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 16 chunks -- elapsed time 0.0002617407590150833 [2025-03-07 13:00:39,985] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 16 chunks (4049 tokens in total) --hit rate 85.71% -- elapsed time 0.0021305587142705917 [2025-03-07 13:00:39,985] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4048 [2025-03-07 13:00:39,986] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:00:39,991] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:00:40,017] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4049 tokens and then stores 0 tokens [2025-03-07 13:00:40,019] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.15it/s, est. speed input: 8721.94 toks/s, output: 34.46 toks/s]\u001b[A\n",
      "Processing Queries:   1%|          | 7/1000 [00:04<08:41,  1.90it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:00:40,476] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:00:40,478] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 16 chunks -- elapsed time 0.000439399853348732 [2025-03-07 13:00:40,479] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 16 chunks (4049 tokens in total) --hit rate 87.50% -- elapsed time 0.002235567197203636 [2025-03-07 13:00:40,480] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4048 [2025-03-07 13:00:40,482] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:00:40,488] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:00:40,513] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4049 tokens and then stores 0 tokens [2025-03-07 13:00:40,515] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.11it/s, est. speed input: 8611.45 toks/s, output: 34.03 toks/s]\u001b[A\n",
      "Processing Queries:   1%|          | 8/1000 [00:04<08:34,  1.93it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:00:40,985] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:00:40,988] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 16 chunks -- elapsed time 0.0005106702446937561 [2025-03-07 13:00:40,989] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 16 chunks (4049 tokens in total) --hit rate 88.89% -- elapsed time 0.0025632865726947784 [2025-03-07 13:00:40,990] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4048 [2025-03-07 13:00:40,992] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:00:40,997] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:00:41,023] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4049 tokens and then stores 0 tokens [2025-03-07 13:00:41,025] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.12it/s, est. speed input: 8649.30 toks/s, output: 34.18 toks/s]\u001b[A\n",
      "Processing Queries:   1%|          | 9/1000 [00:05<08:30,  1.94it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:00:41,487] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:00:41,489] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 16 chunks -- elapsed time 0.0002545863389968872 [2025-03-07 13:00:41,490] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 16 chunks (4049 tokens in total) --hit rate 90.00% -- elapsed time 0.0021693073213100433 [2025-03-07 13:00:41,491] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4048 [2025-03-07 13:00:41,492] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:00:41,497] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:00:41,522] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4049 tokens and then stores 0 tokens [2025-03-07 13:00:41,524] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.14it/s, est. speed input: 8703.40 toks/s, output: 34.39 toks/s]\u001b[A\n",
      "Processing Queries:   1%|          | 10/1000 [00:05<08:24,  1.96it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:00:41,986] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:00:41,988] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 0 chunks [2025-03-07 13:00:41,989] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:385\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mReturning the original input! [2025-03-07 13:00:41,990] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:663\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.PREFILL: 1>] [2025-03-07 13:00:42,552] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the put() method [2025-03-07 13:00:42,636] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:99\n",
      "\u001b[33mINFO LMCache: \u001b[0mStored/updated 16 chunks, total time 0.00s, make chunks time 0.00s [2025-03-07 13:00:42,637] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:315\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 0 tokens and then stores 4046 tokens [2025-03-07 13:00:42,638] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.16s/it, est. speed input: 3481.17 toks/s, output: 16.35 toks/s]\u001b[A\n",
      "Processing Queries:   1%|          | 11/1000 [00:06<11:51,  1.39it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:00:43,183] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:00:43,185] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 16 chunks -- elapsed time 0.00042044930160045624 [2025-03-07 13:00:43,186] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 16 chunks (4046 tokens in total) --hit rate 83.34% -- elapsed time 0.0019776318222284317 [2025-03-07 13:00:43,187] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4045 [2025-03-07 13:00:43,188] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:00:43,192] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:00:43,218] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4046 tokens and then stores 0 tokens [2025-03-07 13:00:43,220] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.84it/s, est. speed input: 7463.03 toks/s, output: 35.05 toks/s]\u001b[A\n",
      "Processing Queries:   1%|          | 12/1000 [00:07<11:08,  1.48it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:00:43,761] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:00:43,763] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 16 chunks -- elapsed time 0.00025215186178684235 [2025-03-07 13:00:43,765] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 16 chunks (4046 tokens in total) --hit rate 84.62% -- elapsed time 0.0021280013024806976 [2025-03-07 13:00:43,765] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4045 [2025-03-07 13:00:43,766] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:00:43,771] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:00:43,797] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4046 tokens and then stores 0 tokens [2025-03-07 13:00:43,798] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.82it/s, est. speed input: 7397.74 toks/s, output: 34.74 toks/s]\u001b[A\n",
      "Processing Queries:   1%|â–         | 13/1000 [00:08<10:39,  1.54it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:00:44,343] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:00:44,345] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 16 chunks -- elapsed time 0.0002542492002248764 [2025-03-07 13:00:44,347] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 16 chunks (4046 tokens in total) --hit rate 85.72% -- elapsed time 0.0021324418485164642 [2025-03-07 13:00:44,347] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4045 [2025-03-07 13:00:44,349] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:00:44,353] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:00:44,379] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4046 tokens and then stores 0 tokens [2025-03-07 13:00:44,380] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.83it/s, est. speed input: 7457.16 toks/s, output: 35.02 toks/s]\u001b[A\n",
      "Processing Queries:   1%|â–         | 14/1000 [00:08<10:18,  1.59it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:00:44,920] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:00:44,922] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 16 chunks -- elapsed time 0.0002468470484018326 [2025-03-07 13:00:44,923] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 16 chunks (4046 tokens in total) --hit rate 86.67% -- elapsed time 0.002086268737912178 [2025-03-07 13:00:44,924] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4045 [2025-03-07 13:00:44,925] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:00:44,930] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:00:44,955] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4046 tokens and then stores 0 tokens [2025-03-07 13:00:44,957] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.83it/s, est. speed input: 7451.51 toks/s, output: 34.99 toks/s]\u001b[A\n",
      "Processing Queries:   2%|â–         | 15/1000 [00:09<10:02,  1.63it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:00:45,498] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:00:45,500] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 16 chunks -- elapsed time 0.00046811066567897797 [2025-03-07 13:00:45,501] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 16 chunks (4046 tokens in total) --hit rate 87.50% -- elapsed time 0.002048565074801445 [2025-03-07 13:00:45,502] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4045 [2025-03-07 13:00:45,503] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:00:45,508] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:00:45,534] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4046 tokens and then stores 0 tokens [2025-03-07 13:00:45,536] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.83it/s, est. speed input: 7459.95 toks/s, output: 35.03 toks/s]\u001b[A\n",
      "Processing Queries:   2%|â–         | 16/1000 [00:09<09:52,  1.66it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:00:46,078] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:00:46,080] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 16 chunks -- elapsed time 0.00025824829936027527 [2025-03-07 13:00:46,081] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 16 chunks (4046 tokens in total) --hit rate 88.24% -- elapsed time 0.0021038856357336044 [2025-03-07 13:00:46,082] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4045 [2025-03-07 13:00:46,083] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:00:46,088] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:00:46,113] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4046 tokens and then stores 0 tokens [2025-03-07 13:00:46,115] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.83it/s, est. speed input: 7459.26 toks/s, output: 35.03 toks/s]\u001b[A\n",
      "Processing Queries:   2%|â–         | 17/1000 [00:10<09:44,  1.68it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:00:46,655] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:00:46,657] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 16 chunks -- elapsed time 0.00026454217731952667 [2025-03-07 13:00:46,658] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 16 chunks (4046 tokens in total) --hit rate 88.89% -- elapsed time 0.0021428000181913376 [2025-03-07 13:00:46,659] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4045 [2025-03-07 13:00:46,661] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:00:46,665] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:00:46,691] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4046 tokens and then stores 0 tokens [2025-03-07 13:00:46,693] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.83it/s, est. speed input: 7449.16 toks/s, output: 34.98 toks/s]\u001b[A\n",
      "Processing Queries:   2%|â–         | 18/1000 [00:10<09:39,  1.70it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:00:47,237] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:00:47,239] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 16 chunks -- elapsed time 0.00024954043328762054 [2025-03-07 13:00:47,240] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 16 chunks (4046 tokens in total) --hit rate 89.47% -- elapsed time 0.002054862678050995 [2025-03-07 13:00:47,241] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4045 [2025-03-07 13:00:47,242] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:00:47,247] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:00:47,272] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4046 tokens and then stores 0 tokens [2025-03-07 13:00:47,273] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.84it/s, est. speed input: 7465.04 toks/s, output: 35.05 toks/s]\u001b[A\n",
      "Processing Queries:   2%|â–         | 19/1000 [00:11<09:35,  1.70it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:00:47,815] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:00:47,817] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 16 chunks -- elapsed time 0.0002697110176086426 [2025-03-07 13:00:47,818] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 16 chunks (4046 tokens in total) --hit rate 90.00% -- elapsed time 0.002116778865456581 [2025-03-07 13:00:47,819] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4045 [2025-03-07 13:00:47,820] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:00:47,824] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:00:47,850] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4046 tokens and then stores 0 tokens [2025-03-07 13:00:47,852] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.84it/s, est. speed input: 7464.71 toks/s, output: 35.05 toks/s]\u001b[A\n",
      "Processing Queries:   2%|â–         | 20/1000 [00:12<09:32,  1.71it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:00:48,394] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:00:48,396] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 0 chunks [2025-03-07 13:00:48,396] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:385\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mReturning the original input! [2025-03-07 13:00:48,397] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:663\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.PREFILL: 1>] [2025-03-07 13:00:48,958] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the put() method [2025-03-07 13:00:49,042] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:99\n",
      "\u001b[33mINFO LMCache: \u001b[0mStored/updated 16 chunks, total time 0.00s, make chunks time 0.00s [2025-03-07 13:00:49,042] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:315\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 0 tokens and then stores 4052 tokens [2025-03-07 13:00:49,044] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.06s/it, est. speed input: 3831.79 toks/s, output: 14.18 toks/s]\u001b[A\n",
      "Processing Queries:   2%|â–         | 21/1000 [00:13<12:01,  1.36it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:00:49,488] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:00:49,490] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 16 chunks -- elapsed time 0.00040564127266407013 [2025-03-07 13:00:49,491] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 16 chunks (4052 tokens in total) --hit rate 86.36% -- elapsed time 0.002012835815548897 [2025-03-07 13:00:49,492] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4051 [2025-03-07 13:00:49,493] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:00:49,498] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:00:49,523] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4052 tokens and then stores 0 tokens [2025-03-07 13:00:49,525] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.26it/s, est. speed input: 9238.98 toks/s, output: 34.20 toks/s]\u001b[A\n",
      "Processing Queries:   2%|â–         | 22/1000 [00:13<10:44,  1.52it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:00:49,976] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:00:49,979] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 16 chunks -- elapsed time 0.00039610452950000763 [2025-03-07 13:00:49,981] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 16 chunks (4052 tokens in total) --hit rate 86.95% -- elapsed time 0.0024707354605197906 [2025-03-07 13:00:49,981] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4051 [2025-03-07 13:00:49,983] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:00:49,989] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:00:50,015] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4052 tokens and then stores 0 tokens [2025-03-07 13:00:50,017] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.22it/s, est. speed input: 9053.74 toks/s, output: 33.51 toks/s]\u001b[A\n",
      "Processing Queries:   2%|â–         | 23/1000 [00:14<09:55,  1.64it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:00:50,461] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:00:50,463] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 16 chunks -- elapsed time 0.00029980018734931946 [2025-03-07 13:00:50,465] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 16 chunks (4052 tokens in total) --hit rate 87.50% -- elapsed time 0.002324100583791733 [2025-03-07 13:00:50,465] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4051 [2025-03-07 13:00:50,467] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:00:50,474] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:00:50,500] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4052 tokens and then stores 0 tokens [2025-03-07 13:00:50,502] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.22it/s, est. speed input: 9063.05 toks/s, output: 33.55 toks/s]\u001b[A\n",
      "Processing Queries:   2%|â–         | 24/1000 [00:14<09:18,  1.75it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:00:50,948] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:00:50,950] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 16 chunks -- elapsed time 0.0003071073442697525 [2025-03-07 13:00:50,951] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 16 chunks (4052 tokens in total) --hit rate 88.00% -- elapsed time 0.0022410713136196136 [2025-03-07 13:00:50,952] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4051 [2025-03-07 13:00:50,953] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:00:50,958] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:00:50,984] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4052 tokens and then stores 0 tokens [2025-03-07 13:00:50,986] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.24it/s, est. speed input: 9140.71 toks/s, output: 33.84 toks/s]\u001b[A\n",
      "Processing Queries:   2%|â–Ž         | 25/1000 [00:15<08:51,  1.83it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:00:51,426] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:00:51,428] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 16 chunks -- elapsed time 0.0002907998859882355 [2025-03-07 13:00:51,430] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 16 chunks (4052 tokens in total) --hit rate 88.46% -- elapsed time 0.002133052796125412 [2025-03-07 13:00:51,430] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4051 [2025-03-07 13:00:51,432] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:00:51,437] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:00:51,462] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4052 tokens and then stores 0 tokens [2025-03-07 13:00:51,464] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.24it/s, est. speed input: 9099.49 toks/s, output: 33.68 toks/s]\u001b[A\n",
      "Processing Queries:   3%|â–Ž         | 26/1000 [00:15<08:32,  1.90it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:00:51,908] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:00:51,910] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 16 chunks -- elapsed time 0.0003085881471633911 [2025-03-07 13:00:51,912] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 16 chunks (4052 tokens in total) --hit rate 88.89% -- elapsed time 0.002314811572432518 [2025-03-07 13:00:51,912] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4051 [2025-03-07 13:00:51,913] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:00:51,918] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:00:51,944] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4052 tokens and then stores 0 tokens [2025-03-07 13:00:51,946] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.22it/s, est. speed input: 9057.72 toks/s, output: 33.53 toks/s]\u001b[A\n",
      "Processing Queries:   3%|â–Ž         | 27/1000 [00:16<08:19,  1.95it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:00:52,402] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:00:52,404] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 16 chunks -- elapsed time 0.0004089009016752243 [2025-03-07 13:00:52,405] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 16 chunks (4052 tokens in total) --hit rate 89.29% -- elapsed time 0.0014595426619052887 [2025-03-07 13:00:52,405] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4051 [2025-03-07 13:00:52,406] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:00:52,411] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:00:52,436] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4052 tokens and then stores 0 tokens [2025-03-07 13:00:52,437] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.26it/s, est. speed input: 9176.04 toks/s, output: 33.97 toks/s]\u001b[A\n",
      "Processing Queries:   3%|â–Ž         | 28/1000 [00:16<08:11,  1.98it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:00:52,877] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:00:52,879] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 16 chunks -- elapsed time 0.0005288925021886826 [2025-03-07 13:00:52,880] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 16 chunks (4052 tokens in total) --hit rate 89.65% -- elapsed time 0.0021072328090667725 [2025-03-07 13:00:52,881] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4051 [2025-03-07 13:00:52,882] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:00:52,887] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:00:52,912] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4052 tokens and then stores 0 tokens [2025-03-07 13:00:52,914] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.23it/s, est. speed input: 9092.30 toks/s, output: 33.66 toks/s]\u001b[A\n",
      "Processing Queries:   3%|â–Ž         | 29/1000 [00:17<08:03,  2.01it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:00:53,358] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:00:53,359] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 16 chunks -- elapsed time 0.00030208006501197815 [2025-03-07 13:00:53,360] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 16 chunks (4052 tokens in total) --hit rate 90.00% -- elapsed time 0.0021960530430078506 [2025-03-07 13:00:53,361] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4051 [2025-03-07 13:00:53,362] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:00:53,368] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:00:53,393] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4052 tokens and then stores 0 tokens [2025-03-07 13:00:53,395] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.23it/s, est. speed input: 9102.48 toks/s, output: 33.69 toks/s]\u001b[A\n",
      "Processing Queries:   3%|â–Ž         | 30/1000 [00:17<07:58,  2.03it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:00:53,849] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:00:53,850] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 0 chunks [2025-03-07 13:00:53,851] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:385\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mReturning the original input! [2025-03-07 13:00:53,852] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:663\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.PREFILL: 1>] [2025-03-07 13:00:54,414] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the put() method [2025-03-07 13:00:54,519] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:99\n",
      "\u001b[33mINFO LMCache: \u001b[0mStored/updated 16 chunks, total time 0.00s, make chunks time 0.00s [2025-03-07 13:00:54,520] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:315\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 0 tokens and then stores 4050 tokens [2025-03-07 13:00:54,520] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.22s/it, est. speed input: 3325.25 toks/s, output: 16.42 toks/s]\u001b[A\n",
      "Processing Queries:   3%|â–Ž         | 31/1000 [00:18<11:41,  1.38it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:00:55,100] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:00:55,102] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 16 chunks -- elapsed time 0.0002537723630666733 [2025-03-07 13:00:55,103] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 16 chunks (4050 tokens in total) --hit rate 87.50% -- elapsed time 0.002085551619529724 [2025-03-07 13:00:55,104] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4049 [2025-03-07 13:00:55,105] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:00:55,110] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:00:55,135] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4050 tokens and then stores 0 tokens [2025-03-07 13:00:55,137] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.75it/s, est. speed input: 7121.10 toks/s, output: 35.17 toks/s]\u001b[A\n",
      "Processing Queries:   3%|â–Ž         | 32/1000 [00:19<11:05,  1.45it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:00:55,702] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:00:55,703] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 16 chunks -- elapsed time 0.0002594888210296631 [2025-03-07 13:00:55,705] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 16 chunks (4050 tokens in total) --hit rate 87.88% -- elapsed time 0.0020565614104270935 [2025-03-07 13:00:55,705] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4049 [2025-03-07 13:00:55,707] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:00:55,711] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:00:55,737] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4050 tokens and then stores 0 tokens [2025-03-07 13:00:55,739] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.74it/s, est. speed input: 7070.17 toks/s, output: 34.91 toks/s]\u001b[A\n",
      "Processing Queries:   3%|â–Ž         | 33/1000 [00:20<10:41,  1.51it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:00:56,306] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:00:56,308] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 16 chunks -- elapsed time 0.0002465061843395233 [2025-03-07 13:00:56,310] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 16 chunks (4050 tokens in total) --hit rate 88.23% -- elapsed time 0.002081058919429779 [2025-03-07 13:00:56,310] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4049 [2025-03-07 13:00:56,311] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:00:56,317] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:00:56,342] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4050 tokens and then stores 0 tokens [2025-03-07 13:00:56,344] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.75it/s, est. speed input: 7108.05 toks/s, output: 35.10 toks/s]\u001b[A\n",
      "Processing Queries:   3%|â–Ž         | 34/1000 [00:20<10:22,  1.55it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:00:56,910] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:00:56,912] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 16 chunks -- elapsed time 0.00025241076946258545 [2025-03-07 13:00:56,913] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 16 chunks (4050 tokens in total) --hit rate 88.57% -- elapsed time 0.0020372699946165085 [2025-03-07 13:00:56,914] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4049 [2025-03-07 13:00:56,915] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:00:56,920] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:00:56,945] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4050 tokens and then stores 0 tokens [2025-03-07 13:00:56,947] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.75it/s, est. speed input: 7127.06 toks/s, output: 35.19 toks/s]\u001b[A\n",
      "Processing Queries:   4%|â–Ž         | 35/1000 [00:21<10:09,  1.58it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:00:57,510] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:00:57,512] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 16 chunks -- elapsed time 0.0002513360232114792 [2025-03-07 13:00:57,513] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 16 chunks (4050 tokens in total) --hit rate 88.89% -- elapsed time 0.0020485632121562958 [2025-03-07 13:00:57,514] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4049 [2025-03-07 13:00:57,515] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:00:57,520] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:00:57,545] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4050 tokens and then stores 0 tokens [2025-03-07 13:00:57,547] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.75it/s, est. speed input: 7124.79 toks/s, output: 35.18 toks/s]\u001b[A\n",
      "Processing Queries:   4%|â–Ž         | 36/1000 [00:21<10:00,  1.61it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:00:58,113] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:00:58,114] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 16 chunks -- elapsed time 0.0002486240118741989 [2025-03-07 13:00:58,116] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 16 chunks (4050 tokens in total) --hit rate 89.19% -- elapsed time 0.002119109034538269 [2025-03-07 13:00:58,116] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4049 [2025-03-07 13:00:58,118] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:00:58,122] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:00:58,148] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4050 tokens and then stores 0 tokens [2025-03-07 13:00:58,149] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.75it/s, est. speed input: 7114.91 toks/s, output: 35.13 toks/s]\u001b[A\n",
      "Processing Queries:   4%|â–Ž         | 37/1000 [00:22<09:54,  1.62it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:00:58,715] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:00:58,717] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 16 chunks -- elapsed time 0.0002543814480304718 [2025-03-07 13:00:58,718] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 16 chunks (4050 tokens in total) --hit rate 89.47% -- elapsed time 0.002101801335811615 [2025-03-07 13:00:58,719] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4049 [2025-03-07 13:00:58,720] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:00:58,725] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:00:58,751] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4050 tokens and then stores 0 tokens [2025-03-07 13:00:58,752] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.74it/s, est. speed input: 7101.87 toks/s, output: 35.07 toks/s]\u001b[A\n",
      "Processing Queries:   4%|â–         | 38/1000 [00:23<09:49,  1.63it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:00:59,319] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:00:59,321] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 16 chunks -- elapsed time 0.00026628002524375916 [2025-03-07 13:00:59,323] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 16 chunks (4050 tokens in total) --hit rate 89.74% -- elapsed time 0.002139478921890259 [2025-03-07 13:00:59,323] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4049 [2025-03-07 13:00:59,324] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:00:59,329] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:00:59,354] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4050 tokens and then stores 0 tokens [2025-03-07 13:00:59,356] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.75it/s, est. speed input: 7098.99 toks/s, output: 35.06 toks/s]\u001b[A\n",
      "Processing Queries:   4%|â–         | 39/1000 [00:23<09:46,  1.64it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:00:59,924] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:00:59,926] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 16 chunks -- elapsed time 0.00048220157623291016 [2025-03-07 13:00:59,928] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 16 chunks (4050 tokens in total) --hit rate 90.00% -- elapsed time 0.002091236412525177 [2025-03-07 13:00:59,928] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4049 [2025-03-07 13:00:59,929] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:00:59,934] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:00:59,959] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4050 tokens and then stores 0 tokens [2025-03-07 13:00:59,961] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.74it/s, est. speed input: 7100.60 toks/s, output: 35.06 toks/s]\u001b[A\n",
      "Processing Queries:   4%|â–         | 40/1000 [00:24<09:44,  1.64it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:01:00,539] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:01:00,540] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 0 chunks [2025-03-07 13:01:00,541] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:385\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mReturning the original input! [2025-03-07 13:01:00,542] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:663\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.PREFILL: 1>] [2025-03-07 13:01:01,105] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the put() method [2025-03-07 13:01:01,188] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:99\n",
      "\u001b[33mINFO LMCache: \u001b[0mStored/updated 16 chunks, total time 0.00s, make chunks time 0.00s [2025-03-07 13:01:01,189] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:315\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 0 tokens and then stores 4047 tokens [2025-03-07 13:01:01,191] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.06s/it, est. speed input: 3827.92 toks/s, output: 14.19 toks/s]\u001b[A\n",
      "Processing Queries:   4%|â–         | 41/1000 [00:25<12:05,  1.32it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:01:01,640] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:01:01,642] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 16 chunks -- elapsed time 0.00024417974054813385 [2025-03-07 13:01:01,643] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 16 chunks (4047 tokens in total) --hit rate 88.10% -- elapsed time 0.0020738989114761353 [2025-03-07 13:01:01,643] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4046 [2025-03-07 13:01:01,645] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:01:01,649] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:01:01,675] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4047 tokens and then stores 0 tokens [2025-03-07 13:01:01,677] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.27it/s, est. speed input: 9252.03 toks/s, output: 34.29 toks/s]\u001b[A\n",
      "Processing Queries:   4%|â–         | 42/1000 [00:25<10:45,  1.48it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:01:02,112] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:01:02,113] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 16 chunks -- elapsed time 0.00025796517729759216 [2025-03-07 13:01:02,115] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 16 chunks (4047 tokens in total) --hit rate 88.37% -- elapsed time 0.0020687952637672424 [2025-03-07 13:01:02,115] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4046 [2025-03-07 13:01:02,117] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:01:02,121] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:01:02,147] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4047 tokens and then stores 0 tokens [2025-03-07 13:01:02,149] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.27it/s, est. speed input: 9239.44 toks/s, output: 34.24 toks/s]\u001b[A\n",
      "Processing Queries:   4%|â–         | 43/1000 [00:26<09:47,  1.63it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:01:02,592] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:01:02,594] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 16 chunks -- elapsed time 0.0005193967372179031 [2025-03-07 13:01:02,596] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 16 chunks (4047 tokens in total) --hit rate 88.64% -- elapsed time 0.00210445374250412 [2025-03-07 13:01:02,596] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4046 [2025-03-07 13:01:02,597] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:01:02,602] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:01:02,628] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4047 tokens and then stores 0 tokens [2025-03-07 13:01:02,630] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.24it/s, est. speed input: 9139.53 toks/s, output: 33.87 toks/s]\u001b[A\n",
      "Processing Queries:   4%|â–         | 44/1000 [00:26<09:09,  1.74it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:01:03,073] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:01:03,075] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 16 chunks -- elapsed time 0.0002944190055131912 [2025-03-07 13:01:03,076] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 16 chunks (4047 tokens in total) --hit rate 88.89% -- elapsed time 0.002147449180483818 [2025-03-07 13:01:03,077] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4046 [2025-03-07 13:01:03,078] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:01:03,083] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:01:03,108] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4047 tokens and then stores 0 tokens [2025-03-07 13:01:03,111] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.24it/s, est. speed input: 9116.85 toks/s, output: 33.79 toks/s]\u001b[A\n",
      "Processing Queries:   4%|â–         | 45/1000 [00:27<08:42,  1.83it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:01:03,559] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:01:03,561] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 16 chunks -- elapsed time 0.0003235768526792526 [2025-03-07 13:01:03,563] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 16 chunks (4047 tokens in total) --hit rate 89.13% -- elapsed time 0.0022437963634729385 [2025-03-07 13:01:03,563] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4046 [2025-03-07 13:01:03,565] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:01:03,570] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:01:03,595] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4047 tokens and then stores 0 tokens [2025-03-07 13:01:03,598] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.22it/s, est. speed input: 9079.09 toks/s, output: 33.65 toks/s]\u001b[A\n",
      "Processing Queries:   5%|â–         | 46/1000 [00:27<08:25,  1.89it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:01:04,051] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:01:04,053] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 16 chunks -- elapsed time 0.0003315471112728119 [2025-03-07 13:01:04,055] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 16 chunks (4047 tokens in total) --hit rate 89.36% -- elapsed time 0.002263912931084633 [2025-03-07 13:01:04,055] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4046 [2025-03-07 13:01:04,056] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:01:04,062] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:01:04,088] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4047 tokens and then stores 0 tokens [2025-03-07 13:01:04,090] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.21it/s, est. speed input: 9007.31 toks/s, output: 33.38 toks/s]\u001b[A\n",
      "Processing Queries:   5%|â–         | 47/1000 [00:28<08:14,  1.93it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:01:04,549] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:01:04,551] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 16 chunks -- elapsed time 0.00030142441391944885 [2025-03-07 13:01:04,553] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 16 chunks (4047 tokens in total) --hit rate 89.58% -- elapsed time 0.0025162119418382645 [2025-03-07 13:01:04,553] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4046 [2025-03-07 13:01:04,555] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:01:04,560] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:01:04,586] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4047 tokens and then stores 0 tokens [2025-03-07 13:01:04,589] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.21it/s, est. speed input: 8985.20 toks/s, output: 33.30 toks/s]\u001b[A\n",
      "Processing Queries:   5%|â–         | 48/1000 [00:28<08:07,  1.95it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:01:05,041] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:01:05,043] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 16 chunks -- elapsed time 0.0003194548189640045 [2025-03-07 13:01:05,044] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 16 chunks (4047 tokens in total) --hit rate 89.80% -- elapsed time 0.0015536993741989136 [2025-03-07 13:01:05,044] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4046 [2025-03-07 13:01:05,045] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:01:05,050] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:01:05,075] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4047 tokens and then stores 0 tokens [2025-03-07 13:01:05,077] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.23it/s, est. speed input: 9096.63 toks/s, output: 33.71 toks/s]\u001b[A\n",
      "Processing Queries:   5%|â–         | 49/1000 [00:29<08:00,  1.98it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:01:05,527] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:01:05,530] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 16 chunks -- elapsed time 0.0003183707594871521 [2025-03-07 13:01:05,532] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 16 chunks (4047 tokens in total) --hit rate 90.00% -- elapsed time 0.003011498600244522 [2025-03-07 13:01:05,533] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4046 [2025-03-07 13:01:05,534] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:01:05,540] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:01:05,566] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4047 tokens and then stores 0 tokens [2025-03-07 13:01:05,569] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.20it/s, est. speed input: 8951.51 toks/s, output: 33.18 toks/s]\u001b[A\n",
      "Processing Queries:   5%|â–Œ         | 50/1000 [00:29<07:56,  1.99it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:01:06,018] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:01:06,020] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 0 chunks [2025-03-07 13:01:06,021] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:385\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mReturning the original input! [2025-03-07 13:01:06,023] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:663\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.PREFILL: 1>] [2025-03-07 13:01:06,587] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the put() method [2025-03-07 13:01:06,687] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:99\n",
      "\u001b[33mINFO LMCache: \u001b[0mStored/updated 16 chunks, total time 0.00s, make chunks time 0.00s [2025-03-07 13:01:06,689] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:315\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 0 tokens and then stores 4044 tokens [2025-03-07 13:01:06,690] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:04<00:00,  4.12s/it, est. speed input: 981.99 toks/s, output: 31.08 toks/s]\u001b[A\n",
      "Processing Queries:   5%|â–Œ         | 51/1000 [00:33<25:17,  1.60s/it]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:01:10,177] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:01:10,179] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 16 chunks -- elapsed time 0.0002685319632291794 [2025-03-07 13:01:10,180] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 16 chunks (4044 tokens in total) --hit rate 88.46% -- elapsed time 0.002129124477505684 [2025-03-07 13:01:10,181] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4043 [2025-03-07 13:01:10,182] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:01:10,187] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:01:10,212] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4044 tokens and then stores 0 tokens [2025-03-07 13:01:10,214] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:03<00:00,  3.43s/it, est. speed input: 1178.81 toks/s, output: 37.31 toks/s]\u001b[A\n",
      "Processing Queries:   5%|â–Œ         | 52/1000 [00:37<34:07,  2.16s/it]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:01:13,651] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:01:13,653] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 16 chunks -- elapsed time 0.0002731252461671829 [2025-03-07 13:01:13,654] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 16 chunks (4044 tokens in total) --hit rate 88.68% -- elapsed time 0.002232896164059639 [2025-03-07 13:01:13,655] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4043 [2025-03-07 13:01:13,656] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:01:13,661] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:01:13,686] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4044 tokens and then stores 0 tokens [2025-03-07 13:01:13,688] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:03<00:00,  3.43s/it, est. speed input: 1179.48 toks/s, output: 37.33 toks/s]\u001b[A\n",
      "Processing Queries:   5%|â–Œ         | 53/1000 [00:40<40:18,  2.55s/it]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:01:17,119] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:01:17,121] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 16 chunks -- elapsed time 0.0003042444586753845 [2025-03-07 13:01:17,122] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 16 chunks (4044 tokens in total) --hit rate 88.89% -- elapsed time 0.0020169150084257126 [2025-03-07 13:01:17,123] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4043 [2025-03-07 13:01:17,124] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:01:17,129] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:01:17,154] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4044 tokens and then stores 0 tokens [2025-03-07 13:01:17,156] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:03<00:00,  3.45s/it, est. speed input: 1173.02 toks/s, output: 37.13 toks/s]\u001b[A\n",
      "Processing Queries:   5%|â–Œ         | 54/1000 [00:44<44:41,  2.83s/it]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:01:20,621] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:01:20,623] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 16 chunks -- elapsed time 0.00031072087585926056 [2025-03-07 13:01:20,624] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 16 chunks (4044 tokens in total) --hit rate 89.09% -- elapsed time 0.0022303294390439987 [2025-03-07 13:01:20,625] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4043 [2025-03-07 13:01:20,626] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:01:20,631] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:01:20,657] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4044 tokens and then stores 0 tokens [2025-03-07 13:01:20,660] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:03<00:00,  3.46s/it, est. speed input: 1170.56 toks/s, output: 37.05 toks/s]\u001b[A\n",
      "Processing Queries:   6%|â–Œ         | 55/1000 [00:47<47:49,  3.04s/it]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:01:24,119] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:01:24,121] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 16 chunks -- elapsed time 0.0003096815198659897 [2025-03-07 13:01:24,122] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 16 chunks (4044 tokens in total) --hit rate 89.29% -- elapsed time 0.0022697430104017258 [2025-03-07 13:01:24,123] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4043 [2025-03-07 13:01:24,124] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:01:24,129] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:01:24,154] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4044 tokens and then stores 0 tokens [2025-03-07 13:01:24,155] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:03<00:00,  3.45s/it, est. speed input: 1173.11 toks/s, output: 37.13 toks/s]\u001b[A\n",
      "Processing Queries:   6%|â–Œ         | 56/1000 [00:51<49:54,  3.17s/it]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:01:27,611] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:01:27,613] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 16 chunks -- elapsed time 0.00031340308487415314 [2025-03-07 13:01:27,614] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 16 chunks (4044 tokens in total) --hit rate 89.47% -- elapsed time 0.0021064002066850662 [2025-03-07 13:01:27,615] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4043 [2025-03-07 13:01:27,616] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:01:27,621] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:01:27,647] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4044 tokens and then stores 0 tokens [2025-03-07 13:01:27,649] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:03<00:00,  3.47s/it, est. speed input: 1166.09 toks/s, output: 36.91 toks/s]\u001b[A\n",
      "Processing Queries:   6%|â–Œ         | 57/1000 [00:54<51:27,  3.27s/it]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:01:31,120] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:01:31,122] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 16 chunks -- elapsed time 0.0003175456076860428 [2025-03-07 13:01:31,123] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 16 chunks (4044 tokens in total) --hit rate 89.66% -- elapsed time 0.0022479314357042313 [2025-03-07 13:01:31,124] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4043 [2025-03-07 13:01:31,125] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:01:31,130] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:01:31,157] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4044 tokens and then stores 0 tokens [2025-03-07 13:01:31,159] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:03<00:00,  3.47s/it, est. speed input: 1167.27 toks/s, output: 36.95 toks/s]\u001b[A\n",
      "Processing Queries:   6%|â–Œ         | 58/1000 [00:58<52:30,  3.34s/it]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:01:34,626] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:01:34,628] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 16 chunks -- elapsed time 0.0003092791885137558 [2025-03-07 13:01:34,630] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 16 chunks (4044 tokens in total) --hit rate 89.83% -- elapsed time 0.0022311918437480927 [2025-03-07 13:01:34,630] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4043 [2025-03-07 13:01:34,631] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:01:34,637] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:01:34,662] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4044 tokens and then stores 0 tokens [2025-03-07 13:01:34,664] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:03<00:00,  3.43s/it, est. speed input: 1179.54 toks/s, output: 37.33 toks/s]\u001b[A\n",
      "Processing Queries:   6%|â–Œ         | 59/1000 [01:01<53:02,  3.38s/it]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:01:38,093] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:01:38,095] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 16 chunks -- elapsed time 0.00026876479387283325 [2025-03-07 13:01:38,096] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 16 chunks (4044 tokens in total) --hit rate 90.00% -- elapsed time 0.002099430188536644 [2025-03-07 13:01:38,097] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4043 [2025-03-07 13:01:38,098] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:01:38,103] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:01:38,128] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4044 tokens and then stores 0 tokens [2025-03-07 13:01:38,130] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:03<00:00,  3.43s/it, est. speed input: 1179.32 toks/s, output: 37.33 toks/s]\u001b[A\n",
      "Processing Queries:   6%|â–Œ         | 60/1000 [01:05<53:23,  3.41s/it]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:01:41,560] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:01:41,562] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 0 chunks [2025-03-07 13:01:41,563] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:385\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mReturning the original input! [2025-03-07 13:01:41,564] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:663\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.PREFILL: 1>] [2025-03-07 13:01:42,132] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the put() method [2025-03-07 13:01:42,215] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:99\n",
      "\u001b[33mINFO LMCache: \u001b[0mStored/updated 16 chunks, total time 0.00s, make chunks time 0.00s [2025-03-07 13:01:42,216] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:315\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 0 tokens and then stores 4047 tokens [2025-03-07 13:01:42,217] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.09s/it, est. speed input: 3707.40 toks/s, output: 14.66 toks/s]\u001b[A\n",
      "Processing Queries:   6%|â–Œ         | 61/1000 [01:06<42:37,  2.72s/it]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:01:42,687] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:01:42,689] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 16 chunks -- elapsed time 0.0004071444272994995 [2025-03-07 13:01:42,690] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 16 chunks (4047 tokens in total) --hit rate 88.71% -- elapsed time 0.0019956715404987335 [2025-03-07 13:01:42,691] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4046 [2025-03-07 13:01:42,692] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:01:42,697] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:01:42,722] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4047 tokens and then stores 0 tokens [2025-03-07 13:01:42,724] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.15it/s, est. speed input: 8743.21 toks/s, output: 34.57 toks/s]\u001b[A\n",
      "Processing Queries:   6%|â–Œ         | 62/1000 [01:06<32:08,  2.06s/it]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:01:43,182] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:01:43,184] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 16 chunks -- elapsed time 0.00025467388331890106 [2025-03-07 13:01:43,185] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 16 chunks (4047 tokens in total) --hit rate 88.89% -- elapsed time 0.0020172037184238434 [2025-03-07 13:01:43,186] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4046 [2025-03-07 13:01:43,187] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:01:43,192] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:01:43,217] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4047 tokens and then stores 0 tokens [2025-03-07 13:01:43,219] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.15it/s, est. speed input: 8734.30 toks/s, output: 34.53 toks/s]\u001b[A\n",
      "Processing Queries:   6%|â–‹         | 63/1000 [01:07<24:47,  1.59s/it]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:01:43,679] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:01:43,681] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 16 chunks -- elapsed time 0.0002576839178800583 [2025-03-07 13:01:43,682] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 16 chunks (4047 tokens in total) --hit rate 89.06% -- elapsed time 0.0020739641040563583 [2025-03-07 13:01:43,683] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4046 [2025-03-07 13:01:43,684] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:01:43,689] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:01:43,714] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4047 tokens and then stores 0 tokens [2025-03-07 13:01:43,716] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.14it/s, est. speed input: 8734.01 toks/s, output: 34.53 toks/s]\u001b[A\n",
      "Processing Queries:   6%|â–‹         | 64/1000 [01:07<19:40,  1.26s/it]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:01:44,178] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:01:44,180] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 16 chunks -- elapsed time 0.0002537611871957779 [2025-03-07 13:01:44,181] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 16 chunks (4047 tokens in total) --hit rate 89.23% -- elapsed time 0.0020616892725229263 [2025-03-07 13:01:44,181] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4046 [2025-03-07 13:01:44,183] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:01:44,187] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:01:44,213] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4047 tokens and then stores 0 tokens [2025-03-07 13:01:44,214] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.15it/s, est. speed input: 8754.49 toks/s, output: 34.61 toks/s]\u001b[A\n",
      "Processing Queries:   6%|â–‹         | 65/1000 [01:08<16:04,  1.03s/it]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:01:44,672] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:01:44,674] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 16 chunks -- elapsed time 0.0002568364143371582 [2025-03-07 13:01:44,676] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 16 chunks (4047 tokens in total) --hit rate 89.39% -- elapsed time 0.002076653763651848 [2025-03-07 13:01:44,676] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4046 [2025-03-07 13:01:44,677] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:01:44,682] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:01:44,708] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4047 tokens and then stores 0 tokens [2025-03-07 13:01:44,710] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.14it/s, est. speed input: 8707.42 toks/s, output: 34.42 toks/s]\u001b[A\n",
      "Processing Queries:   7%|â–‹         | 66/1000 [01:08<13:33,  1.15it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:01:45,175] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:01:45,177] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 16 chunks -- elapsed time 0.00025055184960365295 [2025-03-07 13:01:45,178] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 16 chunks (4047 tokens in total) --hit rate 89.55% -- elapsed time 0.002107923850417137 [2025-03-07 13:01:45,179] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4046 [2025-03-07 13:01:45,180] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:01:45,185] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:01:45,210] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4047 tokens and then stores 0 tokens [2025-03-07 13:01:45,212] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.15it/s, est. speed input: 8736.98 toks/s, output: 34.54 toks/s]\u001b[A\n",
      "Processing Queries:   7%|â–‹         | 67/1000 [01:09<11:49,  1.32it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:01:45,673] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:01:45,675] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 16 chunks -- elapsed time 0.0004108194261789322 [2025-03-07 13:01:45,676] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 16 chunks (4047 tokens in total) --hit rate 89.71% -- elapsed time 0.001979392021894455 [2025-03-07 13:01:45,677] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4046 [2025-03-07 13:01:45,678] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:01:45,683] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:01:45,708] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4047 tokens and then stores 0 tokens [2025-03-07 13:01:45,710] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.15it/s, est. speed input: 8749.62 toks/s, output: 34.59 toks/s]\u001b[A\n",
      "Processing Queries:   7%|â–‹         | 68/1000 [01:09<10:35,  1.47it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:01:46,172] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:01:46,174] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 16 chunks -- elapsed time 0.0002521965652704239 [2025-03-07 13:01:46,175] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 16 chunks (4047 tokens in total) --hit rate 89.86% -- elapsed time 0.0020611900836229324 [2025-03-07 13:01:46,176] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4046 [2025-03-07 13:01:46,177] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:01:46,181] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:01:46,207] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4047 tokens and then stores 0 tokens [2025-03-07 13:01:46,208] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.15it/s, est. speed input: 8748.26 toks/s, output: 34.59 toks/s]\u001b[A\n",
      "Processing Queries:   7%|â–‹         | 69/1000 [01:10<09:43,  1.60it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:01:46,667] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:01:46,669] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 16 chunks -- elapsed time 0.00025298260152339935 [2025-03-07 13:01:46,670] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 16 chunks (4047 tokens in total) --hit rate 90.00% -- elapsed time 0.0020859166979789734 [2025-03-07 13:01:46,671] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4046 [2025-03-07 13:01:46,672] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:01:46,677] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:01:46,702] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4047 tokens and then stores 0 tokens [2025-03-07 13:01:46,704] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.14it/s, est. speed input: 8730.11 toks/s, output: 34.51 toks/s]\u001b[A\n",
      "Processing Queries:   7%|â–‹         | 70/1000 [01:10<09:06,  1.70it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:01:47,167] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:01:47,169] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 0 chunks [2025-03-07 13:01:47,169] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:385\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mReturning the original input! [2025-03-07 13:01:47,170] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:663\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.PREFILL: 1>] [2025-03-07 13:01:47,739] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the put() method [2025-03-07 13:01:47,825] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:99\n",
      "\u001b[33mINFO LMCache: \u001b[0mStored/updated 16 chunks, total time 0.00s, make chunks time 0.00s [2025-03-07 13:01:47,826] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:315\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 0 tokens and then stores 4044 tokens [2025-03-07 13:01:47,827] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.01it/s, est. speed input: 4090.43 toks/s, output: 12.14 toks/s]\u001b[A\n",
      "Processing Queries:   7%|â–‹         | 71/1000 [01:11<11:07,  1.39it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:01:48,190] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:01:48,191] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 16 chunks -- elapsed time 0.0003970582038164139 [2025-03-07 13:01:48,193] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 16 chunks (4044 tokens in total) --hit rate 88.89% -- elapsed time 0.0019603539258241653 [2025-03-07 13:01:48,193] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4043 [2025-03-07 13:01:48,195] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:01:48,199] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:01:48,225] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4044 tokens and then stores 0 tokens [2025-03-07 13:01:48,227] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.42it/s, est. speed input: 9855.24 toks/s, output: 34.12 toks/s]\u001b[A\n",
      "Processing Queries:   7%|â–‹         | 72/1000 [01:12<09:50,  1.57it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:01:48,639] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:01:48,641] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 16 chunks -- elapsed time 0.000248027965426445 [2025-03-07 13:01:48,643] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 16 chunks (4044 tokens in total) --hit rate 89.04% -- elapsed time 0.0020557567477226257 [2025-03-07 13:01:48,643] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4043 [2025-03-07 13:01:48,644] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:01:48,649] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:01:48,674] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4044 tokens and then stores 0 tokens [2025-03-07 13:01:48,676] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.42it/s, est. speed input: 9849.06 toks/s, output: 34.10 toks/s]\u001b[A\n",
      "Processing Queries:   7%|â–‹         | 73/1000 [01:12<08:58,  1.72it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:01:49,082] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:01:49,083] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 16 chunks -- elapsed time 0.00025097280740737915 [2025-03-07 13:01:49,085] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 16 chunks (4044 tokens in total) --hit rate 89.19% -- elapsed time 0.0020026229321956635 [2025-03-07 13:01:49,085] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4043 [2025-03-07 13:01:49,087] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:01:49,091] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:01:49,117] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4044 tokens and then stores 0 tokens [2025-03-07 13:01:49,118] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.42it/s, est. speed input: 9860.30 toks/s, output: 34.13 toks/s]\u001b[A\n",
      "Processing Queries:   7%|â–‹         | 74/1000 [01:13<08:19,  1.86it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:01:49,525] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:01:49,527] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 16 chunks -- elapsed time 0.00026280246675014496 [2025-03-07 13:01:49,528] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 16 chunks (4044 tokens in total) --hit rate 89.33% -- elapsed time 0.001757228747010231 [2025-03-07 13:01:49,529] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4043 [2025-03-07 13:01:49,530] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:01:49,534] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:01:49,559] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4044 tokens and then stores 0 tokens [2025-03-07 13:01:49,561] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.41it/s, est. speed input: 9812.60 toks/s, output: 33.97 toks/s]\u001b[A\n",
      "Processing Queries:   8%|â–Š         | 75/1000 [01:13<07:52,  1.96it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:01:49,972] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:01:49,974] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 16 chunks -- elapsed time 0.0002514161169528961 [2025-03-07 13:01:49,975] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 16 chunks (4044 tokens in total) --hit rate 89.47% -- elapsed time 0.0019751712679862976 [2025-03-07 13:01:49,975] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4043 [2025-03-07 13:01:49,977] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:01:49,981] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:01:50,006] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4044 tokens and then stores 0 tokens [2025-03-07 13:01:50,008] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.40it/s, est. speed input: 9766.36 toks/s, output: 33.81 toks/s]\u001b[A\n",
      "Processing Queries:   8%|â–Š         | 76/1000 [01:14<07:34,  2.03it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:01:50,431] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:01:50,433] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 16 chunks -- elapsed time 0.0003043040633201599 [2025-03-07 13:01:50,435] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 16 chunks (4044 tokens in total) --hit rate 89.61% -- elapsed time 0.0021711960434913635 [2025-03-07 13:01:50,435] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4043 [2025-03-07 13:01:50,436] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:01:50,442] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:01:50,467] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4044 tokens and then stores 0 tokens [2025-03-07 13:01:50,469] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.38it/s, est. speed input: 9686.76 toks/s, output: 33.53 toks/s]\u001b[A\n",
      "Processing Queries:   8%|â–Š         | 77/1000 [01:14<07:26,  2.07it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:01:50,892] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:01:50,894] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 16 chunks -- elapsed time 0.0003456752747297287 [2025-03-07 13:01:50,895] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 16 chunks (4044 tokens in total) --hit rate 89.74% -- elapsed time 0.002344030886888504 [2025-03-07 13:01:50,896] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4043 [2025-03-07 13:01:50,897] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:01:50,903] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:01:50,928] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4044 tokens and then stores 0 tokens [2025-03-07 13:01:50,930] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.38it/s, est. speed input: 9690.94 toks/s, output: 33.55 toks/s]\u001b[A\n",
      "Processing Queries:   8%|â–Š         | 78/1000 [01:15<07:18,  2.10it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:01:51,354] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:01:51,356] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 16 chunks -- elapsed time 0.000344008207321167 [2025-03-07 13:01:51,357] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 16 chunks (4044 tokens in total) --hit rate 89.87% -- elapsed time 0.0022582951933145523 [2025-03-07 13:01:51,358] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4043 [2025-03-07 13:01:51,359] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:01:51,365] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:01:51,391] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4044 tokens and then stores 0 tokens [2025-03-07 13:01:51,393] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.38it/s, est. speed input: 9658.03 toks/s, output: 33.43 toks/s]\u001b[A\n",
      "Processing Queries:   8%|â–Š         | 79/1000 [01:15<07:14,  2.12it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:01:51,808] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:01:51,810] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 16 chunks -- elapsed time 0.0002965293824672699 [2025-03-07 13:01:51,811] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 16 chunks (4044 tokens in total) --hit rate 90.00% -- elapsed time 0.0021995026618242264 [2025-03-07 13:01:51,812] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4043 [2025-03-07 13:01:51,813] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:01:51,818] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:01:51,844] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4044 tokens and then stores 0 tokens [2025-03-07 13:01:51,846] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.40it/s, est. speed input: 9747.10 toks/s, output: 33.74 toks/s]\u001b[A\n",
      "Processing Queries:   8%|â–Š         | 80/1000 [01:15<07:08,  2.15it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:01:52,260] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:01:52,262] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 0 chunks [2025-03-07 13:01:52,263] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:385\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mReturning the original input! [2025-03-07 13:01:52,264] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:663\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.PREFILL: 1>] [2025-03-07 13:01:52,832] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the put() method [2025-03-07 13:01:52,933] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:99\n",
      "\u001b[33mINFO LMCache: \u001b[0mStored/updated 16 chunks, total time 0.00s, make chunks time 0.00s [2025-03-07 13:01:52,934] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:315\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 0 tokens and then stores 4048 tokens [2025-03-07 13:01:52,935] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.19s/it, est. speed input: 3401.22 toks/s, output: 15.96 toks/s]\u001b[A\n",
      "Processing Queries:   8%|â–Š         | 81/1000 [01:17<10:38,  1.44it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:01:53,489] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:01:53,491] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 16 chunks -- elapsed time 0.0002919379621744156 [2025-03-07 13:01:53,492] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 16 chunks (4048 tokens in total) --hit rate 89.02% -- elapsed time 0.00216713547706604 [2025-03-07 13:01:53,493] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4047 [2025-03-07 13:01:53,494] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:01:53,499] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:01:53,524] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4048 tokens and then stores 0 tokens [2025-03-07 13:01:53,527] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.82it/s, est. speed input: 7411.78 toks/s, output: 34.79 toks/s]\u001b[A\n",
      "Processing Queries:   8%|â–Š         | 82/1000 [01:17<10:07,  1.51it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:01:54,074] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:01:54,076] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 16 chunks -- elapsed time 0.0003491714596748352 [2025-03-07 13:01:54,077] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 16 chunks (4048 tokens in total) --hit rate 89.16% -- elapsed time 0.0023190900683403015 [2025-03-07 13:01:54,078] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4047 [2025-03-07 13:01:54,079] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:01:54,084] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:01:54,110] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4048 tokens and then stores 0 tokens [2025-03-07 13:01:54,112] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.81it/s, est. speed input: 7371.28 toks/s, output: 34.60 toks/s]\u001b[A\n",
      "Processing Queries:   8%|â–Š         | 83/1000 [01:18<09:46,  1.56it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:01:54,663] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:01:54,665] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 16 chunks -- elapsed time 0.0004265345633029938 [2025-03-07 13:01:54,666] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 16 chunks (4048 tokens in total) --hit rate 89.29% -- elapsed time 0.002082912251353264 [2025-03-07 13:01:54,667] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4047 [2025-03-07 13:01:54,668] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:01:54,673] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:01:54,699] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4048 tokens and then stores 0 tokens [2025-03-07 13:01:54,701] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.82it/s, est. speed input: 7408.26 toks/s, output: 34.77 toks/s]\u001b[A\n",
      "Processing Queries:   8%|â–Š         | 84/1000 [01:18<09:31,  1.60it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:01:55,249] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:01:55,251] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 16 chunks -- elapsed time 0.00029129162430763245 [2025-03-07 13:01:55,252] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 16 chunks (4048 tokens in total) --hit rate 89.41% -- elapsed time 0.0022098831832408905 [2025-03-07 13:01:55,253] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4047 [2025-03-07 13:01:55,254] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:01:55,259] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:01:55,285] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4048 tokens and then stores 0 tokens [2025-03-07 13:01:55,287] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.83it/s, est. speed input: 7436.80 toks/s, output: 34.90 toks/s]\u001b[A\n",
      "Processing Queries:   8%|â–Š         | 85/1000 [01:19<09:19,  1.63it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:01:55,826] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:01:55,828] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 16 chunks -- elapsed time 0.00025094859302043915 [2025-03-07 13:01:55,829] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 16 chunks (4048 tokens in total) --hit rate 89.53% -- elapsed time 0.0019377302378416061 [2025-03-07 13:01:55,830] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4047 [2025-03-07 13:01:55,831] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:01:55,836] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:01:55,861] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4048 tokens and then stores 0 tokens [2025-03-07 13:01:55,863] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.83it/s, est. speed input: 7471.51 toks/s, output: 35.07 toks/s]\u001b[A\n",
      "Processing Queries:   9%|â–Š         | 86/1000 [01:20<09:09,  1.66it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:01:56,405] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:01:56,407] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 16 chunks -- elapsed time 0.0005556512624025345 [2025-03-07 13:01:56,409] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 16 chunks (4048 tokens in total) --hit rate 89.66% -- elapsed time 0.002697031944990158 [2025-03-07 13:01:56,410] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4047 [2025-03-07 13:01:56,411] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:01:56,416] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:01:56,441] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4048 tokens and then stores 0 tokens [2025-03-07 13:01:56,443] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.83it/s, est. speed input: 7434.37 toks/s, output: 34.89 toks/s]\u001b[A\n",
      "Processing Queries:   9%|â–Š         | 87/1000 [01:20<09:03,  1.68it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:01:56,984] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:01:56,986] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 16 chunks -- elapsed time 0.00024644099175930023 [2025-03-07 13:01:56,988] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 16 chunks (4048 tokens in total) --hit rate 89.77% -- elapsed time 0.002382885664701462 [2025-03-07 13:01:56,989] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4047 [2025-03-07 13:01:56,990] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:01:56,995] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:01:57,020] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4048 tokens and then stores 0 tokens [2025-03-07 13:01:57,022] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.83it/s, est. speed input: 7449.01 toks/s, output: 34.96 toks/s]\u001b[A\n",
      "Processing Queries:   9%|â–‰         | 88/1000 [01:21<08:57,  1.70it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:01:57,562] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:01:57,564] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 16 chunks -- elapsed time 0.0005760323256254196 [2025-03-07 13:01:57,565] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 16 chunks (4048 tokens in total) --hit rate 89.89% -- elapsed time 0.002378227189183235 [2025-03-07 13:01:57,566] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4047 [2025-03-07 13:01:57,568] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:01:57,573] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:01:57,599] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4048 tokens and then stores 0 tokens [2025-03-07 13:01:57,601] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.82it/s, est. speed input: 7402.40 toks/s, output: 34.74 toks/s]\u001b[A\n",
      "Processing Queries:   9%|â–‰         | 89/1000 [01:21<08:54,  1.70it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:01:58,144] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:01:58,146] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 16 chunks -- elapsed time 0.0002598743885755539 [2025-03-07 13:01:58,147] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 16 chunks (4048 tokens in total) --hit rate 90.00% -- elapsed time 0.0020934846252202988 [2025-03-07 13:01:58,148] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4047 [2025-03-07 13:01:58,149] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:01:58,154] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:01:58,179] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4048 tokens and then stores 0 tokens [2025-03-07 13:01:58,181] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.82it/s, est. speed input: 7378.90 toks/s, output: 34.63 toks/s]\u001b[A\n",
      "Processing Queries:   9%|â–‰         | 90/1000 [01:22<08:53,  1.71it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:01:58,738] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:01:58,740] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 0 chunks [2025-03-07 13:01:58,740] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:385\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mReturning the original input! [2025-03-07 13:01:58,741] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:663\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.PREFILL: 1>] [2025-03-07 13:01:59,311] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the put() method [2025-03-07 13:01:59,403] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:99\n",
      "\u001b[33mINFO LMCache: \u001b[0mStored/updated 16 chunks, total time 0.00s, make chunks time 0.00s [2025-03-07 13:01:59,404] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:315\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 0 tokens and then stores 4056 tokens [2025-03-07 13:01:59,405] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.32s/it, est. speed input: 3084.98 toks/s, output: 18.25 toks/s]\u001b[A\n",
      "Processing Queries:   9%|â–‰         | 91/1000 [01:23<12:23,  1.22it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:02:00,093] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:02:00,094] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 16 chunks -- elapsed time 0.00026971660554409027 [2025-03-07 13:02:00,095] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 16 chunks (4056 tokens in total) --hit rate 89.13% -- elapsed time 0.00119871087372303 [2025-03-07 13:02:00,095] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4055 [2025-03-07 13:02:00,095] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:02:00,100] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:02:00,125] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4056 tokens and then stores 0 tokens [2025-03-07 13:02:00,126] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.48it/s, est. speed input: 6017.41 toks/s, output: 35.61 toks/s]\u001b[A\n",
      "Processing Queries:   9%|â–‰         | 92/1000 [01:24<11:55,  1.27it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:02:00,804] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:02:00,806] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 16 chunks -- elapsed time 0.000520782545208931 [2025-03-07 13:02:00,808] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 16 chunks (4056 tokens in total) --hit rate 89.25% -- elapsed time 0.002325870096683502 [2025-03-07 13:02:00,808] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4055 [2025-03-07 13:02:00,810] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:02:00,815] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:02:00,840] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4056 tokens and then stores 0 tokens [2025-03-07 13:02:00,842] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.46it/s, est. speed input: 5956.17 toks/s, output: 35.24 toks/s]\u001b[A\n",
      "Processing Queries:   9%|â–‰         | 93/1000 [01:25<11:35,  1.30it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:02:01,521] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:02:01,523] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 16 chunks -- elapsed time 0.00025294721126556396 [2025-03-07 13:02:01,524] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 16 chunks (4056 tokens in total) --hit rate 89.36% -- elapsed time 0.0021229442209005356 [2025-03-07 13:02:01,525] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4055 [2025-03-07 13:02:01,526] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:02:01,531] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:02:01,556] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4056 tokens and then stores 0 tokens [2025-03-07 13:02:01,558] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.48it/s, est. speed input: 6011.80 toks/s, output: 35.57 toks/s]\u001b[A\n",
      "Processing Queries:   9%|â–‰         | 94/1000 [01:25<11:19,  1.33it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:02:02,238] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:02:02,239] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 16 chunks -- elapsed time 0.0002568960189819336 [2025-03-07 13:02:02,241] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 16 chunks (4056 tokens in total) --hit rate 89.47% -- elapsed time 0.0020382367074489594 [2025-03-07 13:02:02,241] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4055 [2025-03-07 13:02:02,242] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:02:02,247] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:02:02,273] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4056 tokens and then stores 0 tokens [2025-03-07 13:02:02,275] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.47it/s, est. speed input: 5992.19 toks/s, output: 35.46 toks/s]\u001b[A\n",
      "Processing Queries:  10%|â–‰         | 95/1000 [01:26<11:09,  1.35it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:02:02,951] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:02:02,953] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 16 chunks -- elapsed time 0.00024885497987270355 [2025-03-07 13:02:02,954] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 16 chunks (4056 tokens in total) --hit rate 89.58% -- elapsed time 0.002122504636645317 [2025-03-07 13:02:02,955] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4055 [2025-03-07 13:02:02,956] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:02:02,964] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:02:02,990] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4056 tokens and then stores 0 tokens [2025-03-07 13:02:02,993] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.46it/s, est. speed input: 5951.41 toks/s, output: 35.21 toks/s]\u001b[A\n",
      "Processing Queries:  10%|â–‰         | 96/1000 [01:27<11:02,  1.36it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:02:03,665] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:02:03,667] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 16 chunks -- elapsed time 0.00024871155619621277 [2025-03-07 13:02:03,668] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 16 chunks (4056 tokens in total) --hit rate 89.69% -- elapsed time 0.002074761316180229 [2025-03-07 13:02:03,669] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4055 [2025-03-07 13:02:03,670] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:02:03,675] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:02:03,700] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4056 tokens and then stores 0 tokens [2025-03-07 13:02:03,702] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.48it/s, est. speed input: 6024.53 toks/s, output: 35.65 toks/s]\u001b[A\n",
      "Processing Queries:  10%|â–‰         | 97/1000 [01:28<10:55,  1.38it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:02:04,374] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:02:04,376] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 16 chunks -- elapsed time 0.0002555008977651596 [2025-03-07 13:02:04,377] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 16 chunks (4056 tokens in total) --hit rate 89.80% -- elapsed time 0.0021048057824373245 [2025-03-07 13:02:04,378] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4055 [2025-03-07 13:02:04,379] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:02:04,384] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:02:04,409] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4056 tokens and then stores 0 tokens [2025-03-07 13:02:04,411] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.48it/s, est. speed input: 6028.39 toks/s, output: 35.67 toks/s]\u001b[A\n",
      "Processing Queries:  10%|â–‰         | 98/1000 [01:28<10:49,  1.39it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:02:05,080] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:02:05,082] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 16 chunks -- elapsed time 0.0002561323344707489 [2025-03-07 13:02:05,083] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 16 chunks (4056 tokens in total) --hit rate 89.90% -- elapsed time 0.002302311360836029 [2025-03-07 13:02:05,084] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4055 [2025-03-07 13:02:05,085] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:02:05,090] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:02:05,115] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4056 tokens and then stores 0 tokens [2025-03-07 13:02:05,117] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.48it/s, est. speed input: 6019.08 toks/s, output: 35.61 toks/s]\u001b[A\n",
      "Processing Queries:  10%|â–‰         | 99/1000 [01:29<10:45,  1.40it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:02:05,785] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:02:05,788] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 16 chunks -- elapsed time 0.0005712099373340607 [2025-03-07 13:02:05,789] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 16 chunks (4056 tokens in total) --hit rate 90.00% -- elapsed time 0.0024424679577350616 [2025-03-07 13:02:05,790] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4055 [2025-03-07 13:02:05,791] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:02:05,796] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:02:05,822] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4056 tokens and then stores 0 tokens [2025-03-07 13:02:05,824] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.48it/s, est. speed input: 6013.85 toks/s, output: 35.58 toks/s]\u001b[A\n",
      "Processing Queries:  10%|â–ˆ         | 100/1000 [01:30<10:41,  1.40it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:02:06,494] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:02:06,496] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 0 chunks [2025-03-07 13:02:06,497] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:385\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mReturning the original input! [2025-03-07 13:02:06,497] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:663\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.PREFILL: 1>] [2025-03-07 13:02:07,068] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the put() method [2025-03-07 13:02:07,150] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:99\n",
      "\u001b[33mINFO LMCache: \u001b[0mStored/updated 16 chunks, total time 0.00s, make chunks time 0.00s [2025-03-07 13:02:07,151] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:315\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 0 tokens and then stores 4048 tokens [2025-03-07 13:02:07,153] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:04<00:00,  4.07s/it, est. speed input: 995.64 toks/s, output: 31.48 toks/s]\u001b[A\n",
      "Processing Queries:  10%|â–ˆ         | 101/1000 [01:34<25:54,  1.73s/it]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:02:10,598] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:02:10,600] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 16 chunks -- elapsed time 0.00026227161288261414 [2025-03-07 13:02:10,602] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 16 chunks (4048 tokens in total) --hit rate 89.22% -- elapsed time 0.0021189935505390167 [2025-03-07 13:02:10,602] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4047 [2025-03-07 13:02:10,603] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:02:10,609] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:02:10,634] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4048 tokens and then stores 0 tokens [2025-03-07 13:02:10,637] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:03<00:00,  3.43s/it, est. speed input: 1181.06 toks/s, output: 37.35 toks/s]\u001b[A\n",
      "Processing Queries:  10%|â–ˆ         | 102/1000 [01:37<33:40,  2.25s/it]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:02:14,067] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:02:14,069] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 16 chunks -- elapsed time 0.00026450492441654205 [2025-03-07 13:02:14,070] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 16 chunks (4048 tokens in total) --hit rate 89.32% -- elapsed time 0.002171093598008156 [2025-03-07 13:02:14,071] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4047 [2025-03-07 13:02:14,072] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:02:14,077] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:02:14,102] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4048 tokens and then stores 0 tokens [2025-03-07 13:02:14,105] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:03<00:00,  3.44s/it, est. speed input: 1179.14 toks/s, output: 37.28 toks/s]\u001b[A\n",
      "Processing Queries:  10%|â–ˆ         | 103/1000 [01:41<39:07,  2.62s/it]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:02:17,542] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:02:17,544] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 16 chunks -- elapsed time 0.00025449134409427643 [2025-03-07 13:02:17,545] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 16 chunks (4048 tokens in total) --hit rate 89.42% -- elapsed time 0.002109317108988762 [2025-03-07 13:02:17,546] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4047 [2025-03-07 13:02:17,547] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:02:17,552] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:02:17,577] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4048 tokens and then stores 0 tokens [2025-03-07 13:02:17,578] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:03<00:00,  3.47s/it, est. speed input: 1167.51 toks/s, output: 36.92 toks/s]\u001b[A\n",
      "Processing Queries:  10%|â–ˆ         | 104/1000 [01:44<43:04,  2.88s/it]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:02:21,046] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:02:21,047] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 16 chunks -- elapsed time 0.000266464427113533 [2025-03-07 13:02:21,048] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 16 chunks (4048 tokens in total) --hit rate 89.52% -- elapsed time 0.001533186063170433 [2025-03-07 13:02:21,049] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4047 [2025-03-07 13:02:21,049] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:02:21,054] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:02:21,079] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4048 tokens and then stores 0 tokens [2025-03-07 13:02:21,081] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:03<00:00,  3.47s/it, est. speed input: 1165.54 toks/s, output: 36.85 toks/s]\u001b[A\n",
      "Processing Queries:  10%|â–ˆ         | 105/1000 [01:48<45:49,  3.07s/it]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:02:24,557] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:02:24,559] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 16 chunks -- elapsed time 0.000263141468167305 [2025-03-07 13:02:24,560] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 16 chunks (4048 tokens in total) --hit rate 89.62% -- elapsed time 0.0012988150119781494 [2025-03-07 13:02:24,560] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4047 [2025-03-07 13:02:24,561] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:02:24,565] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:02:24,590] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4048 tokens and then stores 0 tokens [2025-03-07 13:02:24,591] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:03<00:00,  3.43s/it, est. speed input: 1181.61 toks/s, output: 37.36 toks/s]\u001b[A\n",
      "Processing Queries:  11%|â–ˆ         | 106/1000 [01:51<47:31,  3.19s/it]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:02:28,016] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:02:28,018] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 16 chunks -- elapsed time 0.00025877170264720917 [2025-03-07 13:02:28,018] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 16 chunks (4048 tokens in total) --hit rate 89.72% -- elapsed time 0.0012346599251031876 [2025-03-07 13:02:28,019] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4047 [2025-03-07 13:02:28,019] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:02:28,024] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:02:28,049] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4048 tokens and then stores 0 tokens [2025-03-07 13:02:28,050] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:03<00:00,  3.42s/it, est. speed input: 1184.04 toks/s, output: 37.44 toks/s]\u001b[A\n",
      "Processing Queries:  11%|â–ˆ         | 107/1000 [01:55<48:38,  3.27s/it]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:02:31,470] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:02:31,471] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 16 chunks -- elapsed time 0.0002680998295545578 [2025-03-07 13:02:31,472] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 16 chunks (4048 tokens in total) --hit rate 89.81% -- elapsed time 0.001375081017613411 [2025-03-07 13:02:31,472] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4047 [2025-03-07 13:02:31,473] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:02:31,477] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:02:31,502] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4048 tokens and then stores 0 tokens [2025-03-07 13:02:31,503] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:03<00:00,  3.42s/it, est. speed input: 1183.13 toks/s, output: 37.41 toks/s]\u001b[A\n",
      "Processing Queries:  11%|â–ˆ         | 108/1000 [01:58<49:25,  3.32s/it]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:02:34,925] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:02:34,926] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 16 chunks -- elapsed time 0.00027816928923130035 [2025-03-07 13:02:34,927] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 16 chunks (4048 tokens in total) --hit rate 89.91% -- elapsed time 0.0012820567935705185 [2025-03-07 13:02:34,927] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4047 [2025-03-07 13:02:34,928] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:02:34,932] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:02:34,957] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4048 tokens and then stores 0 tokens [2025-03-07 13:02:34,958] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:03<00:00,  3.45s/it, est. speed input: 1172.46 toks/s, output: 37.07 toks/s]\u001b[A\n",
      "Processing Queries:  11%|â–ˆ         | 109/1000 [02:02<50:05,  3.37s/it]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:02:38,412] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:02:38,413] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 16 chunks -- elapsed time 0.0002611391246318817 [2025-03-07 13:02:38,414] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 16 chunks (4048 tokens in total) --hit rate 90.00% -- elapsed time 0.0012486334890127182 [2025-03-07 13:02:38,414] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4047 [2025-03-07 13:02:38,415] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:02:38,419] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:02:38,444] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4048 tokens and then stores 0 tokens [2025-03-07 13:02:38,445] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:03<00:00,  3.45s/it, est. speed input: 1172.46 toks/s, output: 37.07 toks/s]\u001b[A\n",
      "Processing Queries:  11%|â–ˆ         | 110/1000 [02:05<50:32,  3.41s/it]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:02:41,900] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:02:41,902] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 0 chunks [2025-03-07 13:02:41,902] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:385\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mReturning the original input! [2025-03-07 13:02:41,903] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:663\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.PREFILL: 1>] [2025-03-07 13:02:42,477] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the put() method [2025-03-07 13:02:42,558] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:99\n",
      "\u001b[33mINFO LMCache: \u001b[0mStored/updated 16 chunks, total time 0.00s, make chunks time 0.00s [2025-03-07 13:02:42,559] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:315\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 0 tokens and then stores 4046 tokens [2025-03-07 13:02:42,560] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:02<00:00,  2.19s/it, est. speed input: 1851.19 toks/s, output: 26.08 toks/s]\u001b[A\n",
      "Processing Queries:  11%|â–ˆ         | 111/1000 [02:07<45:13,  3.05s/it]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:02:44,124] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:02:44,126] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 16 chunks -- elapsed time 0.0002511683851480484 [2025-03-07 13:02:44,127] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 16 chunks (4046 tokens in total) --hit rate 89.29% -- elapsed time 0.0020907465368509293 [2025-03-07 13:02:44,128] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4045 [2025-03-07 13:02:44,129] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:02:44,133] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:02:44,159] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4046 tokens and then stores 0 tokens [2025-03-07 13:02:44,160] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.54s/it, est. speed input: 2628.14 toks/s, output: 37.02 toks/s]\u001b[A\n",
      "Processing Queries:  11%|â–ˆ         | 112/1000 [02:09<38:37,  2.61s/it]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:02:45,697] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:02:45,699] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 16 chunks -- elapsed time 0.00025529414415359497 [2025-03-07 13:02:45,700] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 16 chunks (4046 tokens in total) --hit rate 89.38% -- elapsed time 0.0020783431828022003 [2025-03-07 13:02:45,701] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4045 [2025-03-07 13:02:45,702] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:02:45,707] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:02:45,732] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4046 tokens and then stores 0 tokens [2025-03-07 13:02:45,734] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.54s/it, est. speed input: 2623.68 toks/s, output: 36.96 toks/s]\u001b[A\n",
      "Processing Queries:  11%|â–ˆâ–        | 113/1000 [02:10<33:59,  2.30s/it]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:02:47,278] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:02:47,280] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 16 chunks -- elapsed time 0.00026047416031360626 [2025-03-07 13:02:47,281] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 16 chunks (4046 tokens in total) --hit rate 89.47% -- elapsed time 0.0016887746751308441 [2025-03-07 13:02:47,282] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4045 [2025-03-07 13:02:47,282] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:02:47,287] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:02:47,313] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4046 tokens and then stores 0 tokens [2025-03-07 13:02:47,314] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.55s/it, est. speed input: 2622.78 toks/s, output: 36.95 toks/s]\u001b[A\n",
      "Processing Queries:  11%|â–ˆâ–        | 114/1000 [02:12<30:46,  2.08s/it]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:02:48,857] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:02:48,858] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 16 chunks -- elapsed time 0.00024594366550445557 [2025-03-07 13:02:48,860] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 16 chunks (4046 tokens in total) --hit rate 89.57% -- elapsed time 0.002059284597635269 [2025-03-07 13:02:48,860] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4045 [2025-03-07 13:02:48,861] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:02:48,866] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:02:48,891] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4046 tokens and then stores 0 tokens [2025-03-07 13:02:48,893] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.55s/it, est. speed input: 2620.20 toks/s, output: 36.91 toks/s]\u001b[A\n",
      "Processing Queries:  12%|â–ˆâ–        | 115/1000 [02:14<28:30,  1.93s/it]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:02:50,450] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:02:50,451] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 16 chunks -- elapsed time 0.000263262540102005 [2025-03-07 13:02:50,453] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 16 chunks (4046 tokens in total) --hit rate 89.66% -- elapsed time 0.0021017156541347504 [2025-03-07 13:02:50,453] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4045 [2025-03-07 13:02:50,455] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:02:50,459] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:02:50,485] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4046 tokens and then stores 0 tokens [2025-03-07 13:02:50,487] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.55s/it, est. speed input: 2619.40 toks/s, output: 36.90 toks/s]\u001b[A\n",
      "Processing Queries:  12%|â–ˆâ–        | 116/1000 [02:15<26:57,  1.83s/it]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:02:52,029] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:02:52,030] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 16 chunks -- elapsed time 0.0002570468932390213 [2025-03-07 13:02:52,031] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 16 chunks (4046 tokens in total) --hit rate 89.74% -- elapsed time 0.002145431935787201 [2025-03-07 13:02:52,032] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4045 [2025-03-07 13:02:52,033] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:02:52,038] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:02:52,063] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4046 tokens and then stores 0 tokens [2025-03-07 13:02:52,065] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.54s/it, est. speed input: 2625.89 toks/s, output: 36.99 toks/s]\u001b[A\n",
      "Processing Queries:  12%|â–ˆâ–        | 117/1000 [02:17<25:49,  1.75s/it]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:02:53,603] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:02:53,605] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 16 chunks -- elapsed time 0.00026878155767917633 [2025-03-07 13:02:53,607] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 16 chunks (4046 tokens in total) --hit rate 89.83% -- elapsed time 0.002104083076119423 [2025-03-07 13:02:53,607] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4045 [2025-03-07 13:02:53,608] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:02:53,613] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:02:53,639] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4046 tokens and then stores 0 tokens [2025-03-07 13:02:53,641] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.55s/it, est. speed input: 2621.62 toks/s, output: 36.93 toks/s]\u001b[A\n",
      "Processing Queries:  12%|â–ˆâ–        | 118/1000 [02:18<25:00,  1.70s/it]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:02:55,187] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:02:55,189] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 16 chunks -- elapsed time 0.0002740975469350815 [2025-03-07 13:02:55,190] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 16 chunks (4046 tokens in total) --hit rate 89.92% -- elapsed time 0.0022467225790023804 [2025-03-07 13:02:55,191] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4045 [2025-03-07 13:02:55,192] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:02:55,197] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:02:55,223] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4046 tokens and then stores 0 tokens [2025-03-07 13:02:55,224] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.55s/it, est. speed input: 2621.60 toks/s, output: 36.93 toks/s]\u001b[A\n",
      "Processing Queries:  12%|â–ˆâ–        | 119/1000 [02:20<24:27,  1.67s/it]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:02:56,763] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:02:56,765] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 16 chunks -- elapsed time 0.0002472344785928726 [2025-03-07 13:02:56,766] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 16 chunks (4046 tokens in total) --hit rate 90.00% -- elapsed time 0.0015010833740234375 [2025-03-07 13:02:56,766] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4045 [2025-03-07 13:02:56,767] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:02:56,771] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:02:56,797] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4046 tokens and then stores 0 tokens [2025-03-07 13:02:56,798] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.56s/it, est. speed input: 2606.63 toks/s, output: 36.72 toks/s]\u001b[A\n",
      "Processing Queries:  12%|â–ˆâ–        | 120/1000 [02:22<24:04,  1.64s/it]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:02:58,352] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:02:58,354] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 0 chunks [2025-03-07 13:02:58,355] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:385\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mReturning the original input! [2025-03-07 13:02:58,356] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:663\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.PREFILL: 1>] [2025-03-07 13:02:58,930] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the put() method [2025-03-07 13:02:59,031] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:99\n",
      "\u001b[33mINFO LMCache: \u001b[0mStored/updated 16 chunks, total time 0.00s, make chunks time 0.00s [2025-03-07 13:02:59,032] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:315\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 0 tokens and then stores 4048 tokens [2025-03-07 13:02:59,033] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.19s/it, est. speed input: 3406.07 toks/s, output: 15.15 toks/s]\u001b[A\n",
      "Processing Queries:  12%|â–ˆâ–        | 121/1000 [02:23<22:13,  1.52s/it]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:02:59,581] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:02:59,584] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 16 chunks -- elapsed time 0.0002987086772918701 [2025-03-07 13:02:59,585] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 16 chunks (4048 tokens in total) --hit rate 89.34% -- elapsed time 0.002262428402900696 [2025-03-07 13:02:59,586] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4047 [2025-03-07 13:02:59,587] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:02:59,593] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:02:59,619] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4048 tokens and then stores 0 tokens [2025-03-07 13:02:59,621] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.90it/s, est. speed input: 7746.20 toks/s, output: 34.44 toks/s]\u001b[A\n",
      "Processing Queries:  12%|â–ˆâ–        | 122/1000 [02:23<18:00,  1.23s/it]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:03:00,146] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:03:00,148] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 16 chunks -- elapsed time 0.0002988278865814209 [2025-03-07 13:03:00,150] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 16 chunks (4048 tokens in total) --hit rate 89.43% -- elapsed time 0.002449067309498787 [2025-03-07 13:03:00,151] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4047 [2025-03-07 13:03:00,152] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:03:00,158] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:03:00,183] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4048 tokens and then stores 0 tokens [2025-03-07 13:03:00,186] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.91it/s, est. speed input: 7803.13 toks/s, output: 34.70 toks/s]\u001b[A\n",
      "Processing Queries:  12%|â–ˆâ–        | 123/1000 [02:24<15:03,  1.03s/it]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:03:00,703] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:03:00,704] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 16 chunks -- elapsed time 0.0007105451077222824 [2025-03-07 13:03:00,706] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 16 chunks (4048 tokens in total) --hit rate 89.52% -- elapsed time 0.0025243237614631653 [2025-03-07 13:03:00,707] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4047 [2025-03-07 13:03:00,708] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:03:00,713] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:03:00,739] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4048 tokens and then stores 0 tokens [2025-03-07 13:03:00,741] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.93it/s, est. speed input: 7819.71 toks/s, output: 34.77 toks/s]\u001b[A\n",
      "Processing Queries:  12%|â–ˆâ–        | 124/1000 [02:24<12:56,  1.13it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:03:01,251] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:03:01,253] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 16 chunks -- elapsed time 0.000246390700340271 [2025-03-07 13:03:01,254] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 16 chunks (4048 tokens in total) --hit rate 89.60% -- elapsed time 0.0019113793969154358 [2025-03-07 13:03:01,255] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4047 [2025-03-07 13:03:01,255] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:03:01,260] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:03:01,285] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4048 tokens and then stores 0 tokens [2025-03-07 13:03:01,287] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.94it/s, est. speed input: 7890.57 toks/s, output: 35.09 toks/s]\u001b[A\n",
      "Processing Queries:  12%|â–ˆâ–Ž        | 125/1000 [02:25<11:26,  1.28it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:03:01,795] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:03:01,797] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 16 chunks -- elapsed time 0.00036123208701610565 [2025-03-07 13:03:01,798] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 16 chunks (4048 tokens in total) --hit rate 89.68% -- elapsed time 0.001835055649280548 [2025-03-07 13:03:01,798] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4047 [2025-03-07 13:03:01,799] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:03:01,804] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:03:01,829] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4048 tokens and then stores 0 tokens [2025-03-07 13:03:01,831] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.92it/s, est. speed input: 7802.26 toks/s, output: 34.69 toks/s]\u001b[A\n",
      "Processing Queries:  13%|â–ˆâ–Ž        | 126/1000 [02:26<10:24,  1.40it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:03:02,352] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:03:02,355] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 16 chunks -- elapsed time 0.0006012804806232452 [2025-03-07 13:03:02,356] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 16 chunks (4048 tokens in total) --hit rate 89.76% -- elapsed time 0.002795826643705368 [2025-03-07 13:03:02,357] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4047 [2025-03-07 13:03:02,359] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:03:02,364] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:03:02,390] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4048 tokens and then stores 0 tokens [2025-03-07 13:03:02,392] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.91it/s, est. speed input: 7756.19 toks/s, output: 34.49 toks/s]\u001b[A"
     ]
    }
   ],
   "source": [
    "print(\"Running Naive RAG (LM Cache Optimized)\")\n",
    "naive_rag_cache_responses = run_naive_rag(repeated_summaries, vector_store_naive_rag, tokenizer, generator_optimized_model, \"naive_rag_cache\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced RAG (Raptor + LM Cache Optimized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Advanced RAG (Raptor + LM Cache Optimized)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Queries:   0%|          | 0/1000 [00:00<?, ?it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:46:14,119] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:46:14,121] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 0 chunks [2025-03-07 13:46:14,122] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:385\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mReturning the original input! [2025-03-07 13:46:14,123] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:663\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.PREFILL: 1>] [2025-03-07 13:46:14,696] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the put() method [2025-03-07 13:46:14,797] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:99\n",
      "\u001b[33mINFO LMCache: \u001b[0mStored/updated 17 chunks, total time 0.00s, make chunks time 0.00s [2025-03-07 13:46:14,798] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:315\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 0 tokens and then stores 4117 tokens [2025-03-07 13:46:14,798] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.14s/it, est. speed input: 3628.50 toks/s, output: 14.10 toks/s]\u001b[A\n",
      "Processing Queries:   0%|          | 1/1000 [00:01<20:06,  1.21s/it]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:46:15,298] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:46:15,300] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.0003025177866220474 [2025-03-07 13:46:15,301] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4117 tokens in total) --hit rate 89.92% -- elapsed time 0.0021556075662374496 [2025-03-07 13:46:15,302] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4116 [2025-03-07 13:46:15,303] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:46:15,308] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:46:15,334] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4117 tokens and then stores 0 tokens [2025-03-07 13:46:15,336] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.10it/s, est. speed input: 8698.54 toks/s, output: 33.80 toks/s]\u001b[A\n",
      "Processing Queries:   0%|          | 2/1000 [00:01<13:20,  1.25it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:46:15,815] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:46:15,817] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.00029990077018737793 [2025-03-07 13:46:15,818] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4117 tokens in total) --hit rate 89.93% -- elapsed time 0.0022140350192785263 [2025-03-07 13:46:15,819] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4116 [2025-03-07 13:46:15,820] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:46:15,826] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:46:15,851] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4117 tokens and then stores 0 tokens [2025-03-07 13:46:15,854] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.09it/s, est. speed input: 8688.98 toks/s, output: 33.77 toks/s]\u001b[A\n",
      "Processing Queries:   0%|          | 3/1000 [00:02<11:10,  1.49it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:46:16,331] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:46:16,334] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.000298893079161644 [2025-03-07 13:46:16,335] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4117 tokens in total) --hit rate 89.94% -- elapsed time 0.002480335533618927 [2025-03-07 13:46:16,336] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4116 [2025-03-07 13:46:16,337] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:46:16,342] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:46:16,368] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4117 tokens and then stores 0 tokens [2025-03-07 13:46:16,370] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.09it/s, est. speed input: 8663.85 toks/s, output: 33.67 toks/s]\u001b[A\n",
      "Processing Queries:   0%|          | 4/1000 [00:02<10:08,  1.64it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:46:16,855] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:46:16,857] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.0003145132213830948 [2025-03-07 13:46:16,859] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4117 tokens in total) --hit rate 89.95% -- elapsed time 0.0022103916853666306 [2025-03-07 13:46:16,859] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4116 [2025-03-07 13:46:16,861] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:46:16,866] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:46:16,891] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4117 tokens and then stores 0 tokens [2025-03-07 13:46:16,893] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.09it/s, est. speed input: 8681.35 toks/s, output: 33.74 toks/s]\u001b[A\n",
      "Processing Queries:   0%|          | 5/1000 [00:03<09:36,  1.73it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:46:17,369] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:46:17,371] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.0005963239818811417 [2025-03-07 13:46:17,372] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4117 tokens in total) --hit rate 89.96% -- elapsed time 0.0024884920567274094 [2025-03-07 13:46:17,373] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4116 [2025-03-07 13:46:17,374] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:46:17,380] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:46:17,406] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4117 tokens and then stores 0 tokens [2025-03-07 13:46:17,408] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.12it/s, est. speed input: 8757.50 toks/s, output: 34.03 toks/s]\u001b[A\n",
      "Processing Queries:   1%|          | 6/1000 [00:03<09:12,  1.80it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:46:17,885] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:46:17,887] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.0003140438348054886 [2025-03-07 13:46:17,889] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4117 tokens in total) --hit rate 89.97% -- elapsed time 0.0022428669035434723 [2025-03-07 13:46:17,890] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4116 [2025-03-07 13:46:17,891] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:46:17,896] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:46:17,922] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4117 tokens and then stores 0 tokens [2025-03-07 13:46:17,924] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.09it/s, est. speed input: 8661.97 toks/s, output: 33.66 toks/s]\u001b[A\n",
      "Processing Queries:   1%|          | 7/1000 [00:04<09:00,  1.84it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:46:18,413] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:46:18,415] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.0003019999712705612 [2025-03-07 13:46:18,416] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4117 tokens in total) --hit rate 89.98% -- elapsed time 0.00217428058385849 [2025-03-07 13:46:18,417] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4116 [2025-03-07 13:46:18,418] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:46:18,424] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:46:18,449] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4117 tokens and then stores 0 tokens [2025-03-07 13:46:18,452] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.07it/s, est. speed input: 8570.31 toks/s, output: 33.31 toks/s]\u001b[A\n",
      "Processing Queries:   1%|          | 8/1000 [00:04<08:56,  1.85it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:46:18,934] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:46:18,936] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.00029686838388442993 [2025-03-07 13:46:18,938] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4117 tokens in total) --hit rate 89.99% -- elapsed time 0.0021400880068540573 [2025-03-07 13:46:18,938] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4116 [2025-03-07 13:46:18,939] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:46:18,945] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:46:18,970] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4117 tokens and then stores 0 tokens [2025-03-07 13:46:18,972] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.09it/s, est. speed input: 8650.54 toks/s, output: 33.62 toks/s]\u001b[A\n",
      "Processing Queries:   1%|          | 9/1000 [00:05<08:48,  1.88it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:46:19,454] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:46:19,456] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.00030243396759033203 [2025-03-07 13:46:19,457] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4117 tokens in total) --hit rate 90.00% -- elapsed time 0.00218033604323864 [2025-03-07 13:46:19,458] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4116 [2025-03-07 13:46:19,459] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:46:19,464] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:46:19,490] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4117 tokens and then stores 0 tokens [2025-03-07 13:46:19,492] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.09it/s, est. speed input: 8638.99 toks/s, output: 33.57 toks/s]\u001b[A\n",
      "Processing Queries:   1%|          | 10/1000 [00:05<08:43,  1.89it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:46:19,976] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:46:19,978] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 0 chunks [2025-03-07 13:46:19,978] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:385\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mReturning the original input! [2025-03-07 13:46:19,979] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:663\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.PREFILL: 1>] [2025-03-07 13:46:20,555] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the put() method [2025-03-07 13:46:20,643] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:99\n",
      "\u001b[33mINFO LMCache: \u001b[0mStored/updated 17 chunks, total time 0.00s, make chunks time 0.00s [2025-03-07 13:46:20,644] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:315\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 0 tokens and then stores 4114 tokens [2025-03-07 13:46:20,645] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.21s/it, est. speed input: 3410.86 toks/s, output: 15.75 toks/s]\u001b[A\n",
      "Processing Queries:   1%|          | 11/1000 [00:07<12:22,  1.33it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:46:21,222] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:46:21,224] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.00026888400316238403 [2025-03-07 13:46:21,226] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4114 tokens in total) --hit rate 89.92% -- elapsed time 0.002160150557756424 [2025-03-07 13:46:21,226] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4113 [2025-03-07 13:46:21,228] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:46:21,233] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:46:21,258] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4114 tokens and then stores 0 tokens [2025-03-07 13:46:21,260] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.79it/s, est. speed input: 7410.65 toks/s, output: 34.22 toks/s]\u001b[A\n",
      "Processing Queries:   1%|          | 12/1000 [00:07<11:34,  1.42it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:46:21,819] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:46:21,821] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.0004198681563138962 [2025-03-07 13:46:21,823] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4114 tokens in total) --hit rate 89.93% -- elapsed time 0.0020477380603551865 [2025-03-07 13:46:21,823] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4113 [2025-03-07 13:46:21,824] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:46:21,829] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:46:21,855] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4114 tokens and then stores 0 tokens [2025-03-07 13:46:21,857] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.80it/s, est. speed input: 7440.75 toks/s, output: 34.36 toks/s]\u001b[A\n",
      "Processing Queries:   1%|â–         | 13/1000 [00:08<11:01,  1.49it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:46:22,408] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:46:22,410] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.0002661999315023422 [2025-03-07 13:46:22,411] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4114 tokens in total) --hit rate 89.94% -- elapsed time 0.0014969836920499802 [2025-03-07 13:46:22,411] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4113 [2025-03-07 13:46:22,412] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:46:22,416] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:46:22,441] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4114 tokens and then stores 0 tokens [2025-03-07 13:46:22,444] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.79it/s, est. speed input: 7365.53 toks/s, output: 34.02 toks/s]\u001b[A\n",
      "Processing Queries:   1%|â–         | 14/1000 [00:08<10:37,  1.55it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:46:23,011] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:46:23,012] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.0002956632524728775 [2025-03-07 13:46:23,013] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4114 tokens in total) --hit rate 89.95% -- elapsed time 0.001424744725227356 [2025-03-07 13:46:23,013] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4113 [2025-03-07 13:46:23,014] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:46:23,019] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:46:23,044] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4114 tokens and then stores 0 tokens [2025-03-07 13:46:23,045] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.79it/s, est. speed input: 7393.89 toks/s, output: 34.15 toks/s]\u001b[A\n",
      "Processing Queries:   2%|â–         | 15/1000 [00:09<10:23,  1.58it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:46:23,608] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:46:23,610] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.0002968534827232361 [2025-03-07 13:46:23,610] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4114 tokens in total) --hit rate 89.96% -- elapsed time 0.0012660082429647446 [2025-03-07 13:46:23,611] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4113 [2025-03-07 13:46:23,611] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:46:23,616] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:46:23,641] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4114 tokens and then stores 0 tokens [2025-03-07 13:46:23,643] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.79it/s, est. speed input: 7385.18 toks/s, output: 34.11 toks/s]\u001b[A\n",
      "Processing Queries:   2%|â–         | 16/1000 [00:10<10:13,  1.60it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:46:24,210] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:46:24,212] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.00030136480927467346 [2025-03-07 13:46:24,214] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4114 tokens in total) --hit rate 89.97% -- elapsed time 0.0022144000977277756 [2025-03-07 13:46:24,214] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4113 [2025-03-07 13:46:24,215] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:46:24,221] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:46:24,246] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4114 tokens and then stores 0 tokens [2025-03-07 13:46:24,248] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.79it/s, est. speed input: 7404.35 toks/s, output: 34.19 toks/s]\u001b[A\n",
      "Processing Queries:   2%|â–         | 17/1000 [00:10<10:05,  1.62it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:46:24,807] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:46:24,809] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.0004911795258522034 [2025-03-07 13:46:24,810] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4114 tokens in total) --hit rate 89.98% -- elapsed time 0.0020789746195077896 [2025-03-07 13:46:24,811] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4113 [2025-03-07 13:46:24,812] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:46:24,818] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:46:24,843] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4114 tokens and then stores 0 tokens [2025-03-07 13:46:24,845] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.79it/s, est. speed input: 7387.60 toks/s, output: 34.12 toks/s]\u001b[A\n",
      "Processing Queries:   2%|â–         | 18/1000 [00:11<09:59,  1.64it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:46:25,405] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:46:25,407] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.0003125034272670746 [2025-03-07 13:46:25,409] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4114 tokens in total) --hit rate 89.99% -- elapsed time 0.0022281035780906677 [2025-03-07 13:46:25,409] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4113 [2025-03-07 13:46:25,410] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:46:25,416] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:46:25,441] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4114 tokens and then stores 0 tokens [2025-03-07 13:46:25,443] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.78it/s, est. speed input: 7371.30 toks/s, output: 34.04 toks/s]\u001b[A\n",
      "Processing Queries:   2%|â–         | 19/1000 [00:11<09:55,  1.65it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:46:26,014] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:46:26,016] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.00031205639243125916 [2025-03-07 13:46:26,017] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4114 tokens in total) --hit rate 90.00% -- elapsed time 0.0018273387104272842 [2025-03-07 13:46:26,018] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4113 [2025-03-07 13:46:26,019] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:46:26,024] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:46:26,049] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4114 tokens and then stores 0 tokens [2025-03-07 13:46:26,051] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.80it/s, est. speed input: 7425.93 toks/s, output: 34.29 toks/s]\u001b[A\n",
      "Processing Queries:   2%|â–         | 20/1000 [00:12<09:54,  1.65it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:46:26,612] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:46:26,614] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 0 chunks [2025-03-07 13:46:26,614] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:385\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mReturning the original input! [2025-03-07 13:46:26,616] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:663\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.PREFILL: 1>] [2025-03-07 13:46:27,190] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the put() method [2025-03-07 13:46:27,295] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:99\n",
      "\u001b[33mINFO LMCache: \u001b[0mStored/updated 17 chunks, total time 0.00s, make chunks time 0.00s [2025-03-07 13:46:27,296] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:315\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 0 tokens and then stores 4120 tokens [2025-03-07 13:46:27,297] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.34s/it, est. speed input: 3084.02 toks/s, output: 17.22 toks/s]\u001b[A\n",
      "Processing Queries:   2%|â–         | 21/1000 [00:13<13:41,  1.19it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:46:27,996] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:46:27,998] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.00030647777020931244 [2025-03-07 13:46:28,000] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4120 tokens in total) --hit rate 89.92% -- elapsed time 0.002302084118127823 [2025-03-07 13:46:28,000] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4119 [2025-03-07 13:46:28,002] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:46:28,007] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:46:28,033] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4120 tokens and then stores 0 tokens [2025-03-07 13:46:28,034] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.51it/s, est. speed input: 6249.42 toks/s, output: 34.89 toks/s]\u001b[A\n",
      "Processing Queries:   2%|â–         | 22/1000 [00:14<13:02,  1.25it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:46:28,696] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:46:28,698] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.00025349482893943787 [2025-03-07 13:46:28,700] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4120 tokens in total) --hit rate 89.93% -- elapsed time 0.0021828487515449524 [2025-03-07 13:46:28,700] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4119 [2025-03-07 13:46:28,702] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:46:28,706] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:46:28,732] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4120 tokens and then stores 0 tokens [2025-03-07 13:46:28,734] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.50it/s, est. speed input: 6203.31 toks/s, output: 34.63 toks/s]\u001b[A\n",
      "Processing Queries:   2%|â–         | 23/1000 [00:15<12:33,  1.30it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:46:29,400] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:46:29,402] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.00044435635209083557 [2025-03-07 13:46:29,403] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4120 tokens in total) --hit rate 89.94% -- elapsed time 0.002059396356344223 [2025-03-07 13:46:29,404] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4119 [2025-03-07 13:46:29,405] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:46:29,410] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:46:29,435] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4120 tokens and then stores 0 tokens [2025-03-07 13:46:29,437] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.50it/s, est. speed input: 6193.55 toks/s, output: 34.57 toks/s]\u001b[A\n",
      "Processing Queries:   2%|â–         | 24/1000 [00:16<12:12,  1.33it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:46:30,107] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:46:30,109] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.0002769641578197479 [2025-03-07 13:46:30,110] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4120 tokens in total) --hit rate 89.95% -- elapsed time 0.0021129362285137177 [2025-03-07 13:46:30,110] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4119 [2025-03-07 13:46:30,112] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:46:30,117] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:46:30,142] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4120 tokens and then stores 0 tokens [2025-03-07 13:46:30,144] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.49it/s, est. speed input: 6182.93 toks/s, output: 34.52 toks/s]\u001b[A\n",
      "Processing Queries:   2%|â–Ž         | 25/1000 [00:16<11:59,  1.35it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:46:30,814] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:46:30,816] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.00030320510268211365 [2025-03-07 13:46:30,817] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4120 tokens in total) --hit rate 89.96% -- elapsed time 0.002219660207629204 [2025-03-07 13:46:30,818] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4119 [2025-03-07 13:46:30,819] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:46:30,824] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:46:30,850] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4120 tokens and then stores 0 tokens [2025-03-07 13:46:30,852] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.49it/s, est. speed input: 6164.93 toks/s, output: 34.41 toks/s]\u001b[A\n",
      "Processing Queries:   3%|â–Ž         | 26/1000 [00:17<11:50,  1.37it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:46:31,525] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:46:31,527] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.00034195929765701294 [2025-03-07 13:46:31,528] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4120 tokens in total) --hit rate 89.97% -- elapsed time 0.0022221319377422333 [2025-03-07 13:46:31,529] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4119 [2025-03-07 13:46:31,530] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:46:31,535] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:46:31,561] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4120 tokens and then stores 0 tokens [2025-03-07 13:46:31,563] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.49it/s, est. speed input: 6166.08 toks/s, output: 34.42 toks/s]\u001b[A\n",
      "Processing Queries:   3%|â–Ž         | 27/1000 [00:18<11:44,  1.38it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:46:32,231] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:46:32,233] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.00026436522603034973 [2025-03-07 13:46:32,233] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4120 tokens in total) --hit rate 89.98% -- elapsed time 0.0016991645097732544 [2025-03-07 13:46:32,234] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4119 [2025-03-07 13:46:32,235] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:46:32,240] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:46:32,266] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4120 tokens and then stores 0 tokens [2025-03-07 13:46:32,268] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.52it/s, est. speed input: 6293.25 toks/s, output: 35.13 toks/s]\u001b[A\n",
      "Processing Queries:   3%|â–Ž         | 28/1000 [00:18<11:35,  1.40it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:46:32,927] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:46:32,929] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.00025315769016742706 [2025-03-07 13:46:32,930] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4120 tokens in total) --hit rate 89.99% -- elapsed time 0.00208393856883049 [2025-03-07 13:46:32,931] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4119 [2025-03-07 13:46:32,932] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:46:32,937] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:46:32,962] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4120 tokens and then stores 0 tokens [2025-03-07 13:46:32,964] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.52it/s, est. speed input: 6292.03 toks/s, output: 35.12 toks/s]\u001b[A\n",
      "Processing Queries:   3%|â–Ž         | 29/1000 [00:19<11:28,  1.41it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:46:33,618] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:46:33,620] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.00025101006031036377 [2025-03-07 13:46:33,621] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4120 tokens in total) --hit rate 90.00% -- elapsed time 0.0015267767012119293 [2025-03-07 13:46:33,621] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4119 [2025-03-07 13:46:33,622] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:46:33,626] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:46:33,651] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4120 tokens and then stores 0 tokens [2025-03-07 13:46:33,653] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.52it/s, est. speed input: 6290.59 toks/s, output: 35.12 toks/s]\u001b[A\n",
      "Processing Queries:   3%|â–Ž         | 30/1000 [00:20<11:22,  1.42it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:46:34,313] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:46:34,315] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 0 chunks [2025-03-07 13:46:34,316] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:385\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mReturning the original input! [2025-03-07 13:46:34,316] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:663\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.PREFILL: 1>] [2025-03-07 13:46:34,891] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the put() method [2025-03-07 13:46:34,992] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:99\n",
      "\u001b[33mINFO LMCache: \u001b[0mStored/updated 17 chunks, total time 0.00s, make chunks time 0.00s [2025-03-07 13:46:34,992] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:315\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 0 tokens and then stores 4118 tokens [2025-03-07 13:46:34,993] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.33s/it, est. speed input: 3108.72 toks/s, output: 17.36 toks/s]\u001b[A\n",
      "Processing Queries:   3%|â–Ž         | 31/1000 [00:21<14:34,  1.11it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:46:35,688] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:46:35,690] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.0003265291452407837 [2025-03-07 13:46:35,691] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4118 tokens in total) --hit rate 89.92% -- elapsed time 0.0016756486147642136 [2025-03-07 13:46:35,691] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4117 [2025-03-07 13:46:35,692] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:46:35,698] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:46:35,724] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4118 tokens and then stores 0 tokens [2025-03-07 13:46:35,726] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.71it/s, est. speed input: 7082.31 toks/s, output: 34.40 toks/s]\u001b[A\n",
      "Processing Queries:   3%|â–Ž         | 32/1000 [00:22<13:14,  1.22it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:46:36,313] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:46:36,314] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.00031050480902194977 [2025-03-07 13:46:36,316] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4118 tokens in total) --hit rate 89.93% -- elapsed time 0.0022416766732931137 [2025-03-07 13:46:36,317] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4117 [2025-03-07 13:46:36,318] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:46:36,323] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:46:36,349] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4118 tokens and then stores 0 tokens [2025-03-07 13:46:36,351] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.71it/s, est. speed input: 7077.73 toks/s, output: 34.37 toks/s]\u001b[A\n",
      "Processing Queries:   3%|â–Ž         | 33/1000 [00:22<12:16,  1.31it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:46:36,956] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:46:36,959] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.00029898807406425476 [2025-03-07 13:46:36,960] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4118 tokens in total) --hit rate 89.94% -- elapsed time 0.002357024699449539 [2025-03-07 13:46:36,961] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4117 [2025-03-07 13:46:36,962] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:46:36,968] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:46:36,994] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4118 tokens and then stores 0 tokens [2025-03-07 13:46:36,996] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.70it/s, est. speed input: 7014.37 toks/s, output: 34.07 toks/s]\u001b[A\n",
      "Processing Queries:   3%|â–Ž         | 34/1000 [00:23<11:42,  1.37it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:46:37,584] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:46:37,585] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.0003492962568998337 [2025-03-07 13:46:37,586] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4118 tokens in total) --hit rate 89.95% -- elapsed time 0.0014765355736017227 [2025-03-07 13:46:37,586] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4117 [2025-03-07 13:46:37,587] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:46:37,592] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:46:37,617] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4118 tokens and then stores 0 tokens [2025-03-07 13:46:37,619] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.70it/s, est. speed input: 7054.42 toks/s, output: 34.26 toks/s]\u001b[A\n",
      "Processing Queries:   4%|â–Ž         | 35/1000 [00:24<11:12,  1.43it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:46:38,215] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:46:38,218] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.0003080274909734726 [2025-03-07 13:46:38,219] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4118 tokens in total) --hit rate 89.96% -- elapsed time 0.002988569438457489 [2025-03-07 13:46:38,220] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4117 [2025-03-07 13:46:38,222] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:46:38,228] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:46:38,254] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4118 tokens and then stores 0 tokens [2025-03-07 13:46:38,256] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.69it/s, est. speed input: 7006.93 toks/s, output: 34.03 toks/s]\u001b[A\n",
      "Processing Queries:   4%|â–Ž         | 36/1000 [00:24<10:54,  1.47it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:46:38,856] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:46:38,858] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.00034481287002563477 [2025-03-07 13:46:38,860] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4118 tokens in total) --hit rate 89.97% -- elapsed time 0.0024683140218257904 [2025-03-07 13:46:38,860] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4117 [2025-03-07 13:46:38,862] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:46:38,867] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:46:38,893] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4118 tokens and then stores 0 tokens [2025-03-07 13:46:38,895] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.71it/s, est. speed input: 7078.53 toks/s, output: 34.38 toks/s]\u001b[A\n",
      "Processing Queries:   4%|â–Ž         | 37/1000 [00:25<10:40,  1.50it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:46:39,481] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:46:39,483] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.0004849117249250412 [2025-03-07 13:46:39,485] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4118 tokens in total) --hit rate 89.98% -- elapsed time 0.0020889490842819214 [2025-03-07 13:46:39,485] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4117 [2025-03-07 13:46:39,487] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:46:39,492] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:46:39,517] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4118 tokens and then stores 0 tokens [2025-03-07 13:46:39,519] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.72it/s, est. speed input: 7104.67 toks/s, output: 34.50 toks/s]\u001b[A\n",
      "Processing Queries:   4%|â–         | 38/1000 [00:26<10:28,  1.53it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:46:40,102] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:46:40,104] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.00047577545046806335 [2025-03-07 13:46:40,105] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4118 tokens in total) --hit rate 89.99% -- elapsed time 0.0020355749875307083 [2025-03-07 13:46:40,106] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4117 [2025-03-07 13:46:40,107] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:46:40,112] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:46:40,137] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4118 tokens and then stores 0 tokens [2025-03-07 13:46:40,139] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.72it/s, est. speed input: 7139.18 toks/s, output: 34.67 toks/s]\u001b[A\n",
      "Processing Queries:   4%|â–         | 39/1000 [00:26<10:17,  1.56it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:46:40,724] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:46:40,727] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.00046271830797195435 [2025-03-07 13:46:40,729] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4118 tokens in total) --hit rate 90.00% -- elapsed time 0.0034184549003839493 [2025-03-07 13:46:40,730] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4117 [2025-03-07 13:46:40,732] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:46:40,741] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:46:40,769] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4118 tokens and then stores 0 tokens [2025-03-07 13:46:40,772] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.66it/s, est. speed input: 6882.47 toks/s, output: 33.43 toks/s]\u001b[A\n",
      "Processing Queries:   4%|â–         | 40/1000 [00:27<10:16,  1.56it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:46:41,381] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:46:41,384] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 0 chunks [2025-03-07 13:46:41,385] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:385\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mReturning the original input! [2025-03-07 13:46:41,386] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:663\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.PREFILL: 1>] [2025-03-07 13:46:41,964] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the put() method [2025-03-07 13:46:42,065] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:99\n",
      "\u001b[33mINFO LMCache: \u001b[0mStored/updated 17 chunks, total time 0.00s, make chunks time 0.00s [2025-03-07 13:46:42,066] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:315\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 0 tokens and then stores 4115 tokens [2025-03-07 13:46:42,067] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.37s/it, est. speed input: 3017.38 toks/s, output: 17.60 toks/s]\u001b[A\n",
      "Processing Queries:   4%|â–         | 41/1000 [00:28<14:00,  1.14it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:46:42,791] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:46:42,793] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.00029924511909484863 [2025-03-07 13:46:42,795] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4115 tokens in total) --hit rate 89.92% -- elapsed time 0.002324247732758522 [2025-03-07 13:46:42,796] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4114 [2025-03-07 13:46:42,797] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:46:42,802] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:46:42,828] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4115 tokens and then stores 0 tokens [2025-03-07 13:46:42,831] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.44it/s, est. speed input: 5956.56 toks/s, output: 34.74 toks/s]\u001b[A\n",
      "Processing Queries:   4%|â–         | 42/1000 [00:29<13:19,  1.20it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:46:43,529] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:46:43,531] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.0003174152225255966 [2025-03-07 13:46:43,533] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4115 tokens in total) --hit rate 89.93% -- elapsed time 0.002774307504296303 [2025-03-07 13:46:43,534] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4114 [2025-03-07 13:46:43,535] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:46:43,541] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:46:43,567] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4115 tokens and then stores 0 tokens [2025-03-07 13:46:43,569] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.44it/s, est. speed input: 5963.40 toks/s, output: 34.78 toks/s]\u001b[A\n",
      "Processing Queries:   4%|â–         | 43/1000 [00:30<12:50,  1.24it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:46:44,268] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:46:44,270] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.00030698254704475403 [2025-03-07 13:46:44,272] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4115 tokens in total) --hit rate 89.94% -- elapsed time 0.002399018034338951 [2025-03-07 13:46:44,272] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4114 [2025-03-07 13:46:44,274] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:46:44,279] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:46:44,305] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4115 tokens and then stores 0 tokens [2025-03-07 13:46:44,307] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.44it/s, est. speed input: 5960.52 toks/s, output: 34.76 toks/s]\u001b[A\n",
      "Processing Queries:   4%|â–         | 44/1000 [00:30<12:31,  1.27it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:46:45,002] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:46:45,004] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.00030104443430900574 [2025-03-07 13:46:45,005] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4115 tokens in total) --hit rate 89.95% -- elapsed time 0.0022986922413110733 [2025-03-07 13:46:45,006] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4114 [2025-03-07 13:46:45,007] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:46:45,013] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:46:45,038] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4115 tokens and then stores 0 tokens [2025-03-07 13:46:45,041] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.44it/s, est. speed input: 5964.94 toks/s, output: 34.79 toks/s]\u001b[A\n",
      "Processing Queries:   4%|â–         | 45/1000 [00:31<12:15,  1.30it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:46:45,737] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:46:45,739] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.0002966374158859253 [2025-03-07 13:46:45,741] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4115 tokens in total) --hit rate 89.96% -- elapsed time 0.0029145684093236923 [2025-03-07 13:46:45,742] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4114 [2025-03-07 13:46:45,743] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:46:45,749] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:46:45,775] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4115 tokens and then stores 0 tokens [2025-03-07 13:46:45,777] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.44it/s, est. speed input: 5957.02 toks/s, output: 34.74 toks/s]\u001b[A\n",
      "Processing Queries:   5%|â–         | 46/1000 [00:32<12:05,  1.32it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:46:46,479] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:46:46,481] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.0003267396241426468 [2025-03-07 13:46:46,482] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4115 tokens in total) --hit rate 89.97% -- elapsed time 0.002359386533498764 [2025-03-07 13:46:46,483] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4114 [2025-03-07 13:46:46,484] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:46:46,490] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:46:46,515] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4115 tokens and then stores 0 tokens [2025-03-07 13:46:46,518] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.44it/s, est. speed input: 5960.69 toks/s, output: 34.76 toks/s]\u001b[A\n",
      "Processing Queries:   5%|â–         | 47/1000 [00:33<11:58,  1.33it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:46:47,216] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:46:47,218] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.0003072395920753479 [2025-03-07 13:46:47,220] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4115 tokens in total) --hit rate 89.98% -- elapsed time 0.003028208389878273 [2025-03-07 13:46:47,221] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4114 [2025-03-07 13:46:47,222] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:46:47,227] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:46:47,253] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4115 tokens and then stores 0 tokens [2025-03-07 13:46:47,255] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.45it/s, est. speed input: 5992.47 toks/s, output: 34.95 toks/s]\u001b[A\n",
      "Processing Queries:   5%|â–         | 48/1000 [00:33<11:51,  1.34it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:46:47,949] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:46:47,951] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.00047927163541316986 [2025-03-07 13:46:47,952] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4115 tokens in total) --hit rate 89.99% -- elapsed time 0.002088390290737152 [2025-03-07 13:46:47,953] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4114 [2025-03-07 13:46:47,954] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:46:47,959] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:46:47,985] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4115 tokens and then stores 0 tokens [2025-03-07 13:46:47,987] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.45it/s, est. speed input: 6011.83 toks/s, output: 35.06 toks/s]\u001b[A\n",
      "Processing Queries:   5%|â–         | 49/1000 [00:34<11:46,  1.35it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:46:48,672] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:46:48,674] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.0002584550529718399 [2025-03-07 13:46:48,675] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4115 tokens in total) --hit rate 90.00% -- elapsed time 0.00221414677798748 [2025-03-07 13:46:48,676] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4114 [2025-03-07 13:46:48,677] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:46:48,682] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:46:48,708] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4115 tokens and then stores 0 tokens [2025-03-07 13:46:48,710] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.46it/s, est. speed input: 6023.80 toks/s, output: 35.13 toks/s]\u001b[A\n",
      "Processing Queries:   5%|â–Œ         | 50/1000 [00:35<11:39,  1.36it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:46:49,395] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:46:49,397] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 0 chunks [2025-03-07 13:46:49,398] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:385\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mReturning the original input! [2025-03-07 13:46:49,399] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:663\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.PREFILL: 1>] [2025-03-07 13:46:49,977] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the put() method [2025-03-07 13:46:50,095] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:99\n",
      "\u001b[33mINFO LMCache: \u001b[0mStored/updated 17 chunks, total time 0.00s, make chunks time 0.00s [2025-03-07 13:46:50,096] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:315\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 0 tokens and then stores 4112 tokens [2025-03-07 13:46:50,097] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.60s/it, est. speed input: 2577.25 toks/s, output: 20.06 toks/s]\u001b[A\n",
      "Processing Queries:   5%|â–Œ         | 51/1000 [00:36<15:55,  1.01s/it]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:46:51,038] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:46:51,041] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.0003075040876865387 [2025-03-07 13:46:51,043] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4112 tokens in total) --hit rate 89.92% -- elapsed time 0.003204610198736191 [2025-03-07 13:46:51,044] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4111 [2025-03-07 13:46:51,045] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:46:51,051] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:46:51,076] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4112 tokens and then stores 0 tokens [2025-03-07 13:46:51,078] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.17it/s, est. speed input: 4821.98 toks/s, output: 35.18 toks/s]\u001b[A\n",
      "Processing Queries:   5%|â–Œ         | 52/1000 [00:37<15:22,  1.03it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:46:51,929] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:46:51,931] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.0002993680536746979 [2025-03-07 13:46:51,933] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4112 tokens in total) --hit rate 89.93% -- elapsed time 0.002196209505200386 [2025-03-07 13:46:51,933] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4111 [2025-03-07 13:46:51,935] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:46:51,940] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:46:51,966] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4112 tokens and then stores 0 tokens [2025-03-07 13:46:51,968] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.17it/s, est. speed input: 4835.63 toks/s, output: 35.28 toks/s]\u001b[A\n",
      "Processing Queries:   5%|â–Œ         | 53/1000 [00:38<14:58,  1.05it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:46:52,824] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:46:52,826] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.00042552314698696136 [2025-03-07 13:46:52,827] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4112 tokens in total) --hit rate 89.94% -- elapsed time 0.002057000994682312 [2025-03-07 13:46:52,828] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4111 [2025-03-07 13:46:52,829] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:46:52,834] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:46:52,860] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4112 tokens and then stores 0 tokens [2025-03-07 13:46:52,862] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.18it/s, est. speed input: 4850.98 toks/s, output: 35.39 toks/s]\u001b[A\n",
      "Processing Queries:   5%|â–Œ         | 54/1000 [00:39<14:41,  1.07it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:46:53,716] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:46:53,718] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.00030230171978473663 [2025-03-07 13:46:53,720] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4112 tokens in total) --hit rate 89.95% -- elapsed time 0.0029749050736427307 [2025-03-07 13:46:53,721] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4111 [2025-03-07 13:46:53,722] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:46:53,728] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:46:53,754] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4112 tokens and then stores 0 tokens [2025-03-07 13:46:53,757] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.17it/s, est. speed input: 4831.00 toks/s, output: 35.24 toks/s]\u001b[A\n",
      "Processing Queries:   6%|â–Œ         | 55/1000 [00:40<14:29,  1.09it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:46:54,605] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:46:54,607] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.00030547380447387695 [2025-03-07 13:46:54,608] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4112 tokens in total) --hit rate 89.96% -- elapsed time 0.0017674583941698074 [2025-03-07 13:46:54,608] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4111 [2025-03-07 13:46:54,610] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:46:54,615] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:46:54,641] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4112 tokens and then stores 0 tokens [2025-03-07 13:46:54,643] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.17it/s, est. speed input: 4840.37 toks/s, output: 35.31 toks/s]\u001b[A\n",
      "Processing Queries:   6%|â–Œ         | 56/1000 [00:41<14:19,  1.10it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:46:55,497] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:46:55,498] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.00030081160366535187 [2025-03-07 13:46:55,499] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4112 tokens in total) --hit rate 89.97% -- elapsed time 0.0015748441219329834 [2025-03-07 13:46:55,500] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4111 [2025-03-07 13:46:55,500] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:46:55,505] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:46:55,530] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4112 tokens and then stores 0 tokens [2025-03-07 13:46:55,532] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.18it/s, est. speed input: 4862.51 toks/s, output: 35.47 toks/s]\u001b[A\n",
      "Processing Queries:   6%|â–Œ         | 57/1000 [00:42<14:13,  1.11it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:46:56,386] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:46:56,387] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.0003031603991985321 [2025-03-07 13:46:56,388] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4112 tokens in total) --hit rate 89.98% -- elapsed time 0.0016285106539726257 [2025-03-07 13:46:56,389] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4111 [2025-03-07 13:46:56,389] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:46:56,394] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:46:56,419] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4112 tokens and then stores 0 tokens [2025-03-07 13:46:56,421] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.18it/s, est. speed input: 4862.20 toks/s, output: 35.47 toks/s]\u001b[A\n",
      "Processing Queries:   6%|â–Œ         | 58/1000 [00:43<14:07,  1.11it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:46:57,276] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:46:57,277] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.00031409040093421936 [2025-03-07 13:46:57,278] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4112 tokens in total) --hit rate 89.99% -- elapsed time 0.001455044373869896 [2025-03-07 13:46:57,279] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4111 [2025-03-07 13:46:57,279] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:46:57,284] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:46:57,309] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4112 tokens and then stores 0 tokens [2025-03-07 13:46:57,311] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.18it/s, est. speed input: 4875.42 toks/s, output: 35.57 toks/s]\u001b[A\n",
      "Processing Queries:   6%|â–Œ         | 59/1000 [00:44<14:03,  1.12it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:46:58,161] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:46:58,163] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.00029707513749599457 [2025-03-07 13:46:58,164] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4112 tokens in total) --hit rate 90.00% -- elapsed time 0.0014395229518413544 [2025-03-07 13:46:58,164] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4111 [2025-03-07 13:46:58,165] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:46:58,170] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:46:58,195] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4112 tokens and then stores 0 tokens [2025-03-07 13:46:58,196] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.19it/s, est. speed input: 4893.79 toks/s, output: 35.70 toks/s]\u001b[A\n",
      "Processing Queries:   6%|â–Œ         | 60/1000 [00:44<13:58,  1.12it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:46:59,050] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:46:59,051] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 0 chunks [2025-03-07 13:46:59,052] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:385\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mReturning the original input! [2025-03-07 13:46:59,052] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:663\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.PREFILL: 1>] [2025-03-07 13:46:59,634] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the put() method [2025-03-07 13:46:59,721] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:99\n",
      "\u001b[33mINFO LMCache: \u001b[0mStored/updated 17 chunks, total time 0.00s, make chunks time 0.00s [2025-03-07 13:46:59,722] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:315\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 0 tokens and then stores 4115 tokens [2025-03-07 13:46:59,724] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.37s/it, est. speed input: 3015.92 toks/s, output: 18.32 toks/s]\u001b[A\n",
      "Processing Queries:   6%|â–Œ         | 61/1000 [00:46<16:24,  1.05s/it]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:47:00,453] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:47:00,454] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.00025709904730319977 [2025-03-07 13:47:00,455] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4115 tokens in total) --hit rate 89.92% -- elapsed time 0.00136541947722435 [2025-03-07 13:47:00,455] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4114 [2025-03-07 13:47:00,456] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:47:00,461] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:47:00,486] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4115 tokens and then stores 0 tokens [2025-03-07 13:47:00,487] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.42it/s, est. speed input: 5837.78 toks/s, output: 35.47 toks/s]\u001b[A\n",
      "Processing Queries:   6%|â–Œ         | 62/1000 [00:47<14:56,  1.05it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:47:01,191] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:47:01,193] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.00026180408895015717 [2025-03-07 13:47:01,194] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4115 tokens in total) --hit rate 89.93% -- elapsed time 0.0013516657054424286 [2025-03-07 13:47:01,194] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4114 [2025-03-07 13:47:01,195] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:47:01,199] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:47:01,225] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4115 tokens and then stores 0 tokens [2025-03-07 13:47:01,227] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.41it/s, est. speed input: 5828.15 toks/s, output: 35.41 toks/s]\u001b[A\n",
      "Processing Queries:   6%|â–‹         | 63/1000 [00:47<13:55,  1.12it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:47:01,941] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:47:01,943] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.00026219338178634644 [2025-03-07 13:47:01,945] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4115 tokens in total) --hit rate 89.94% -- elapsed time 0.002137118950486183 [2025-03-07 13:47:01,945] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4114 [2025-03-07 13:47:01,946] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:47:01,951] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:47:01,976] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4115 tokens and then stores 0 tokens [2025-03-07 13:47:01,977] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.41it/s, est. speed input: 5827.36 toks/s, output: 35.40 toks/s]\u001b[A\n",
      "Processing Queries:   6%|â–‹         | 64/1000 [00:48<13:14,  1.18it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:47:02,682] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:47:02,683] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.00025795772671699524 [2025-03-07 13:47:02,684] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4115 tokens in total) --hit rate 89.95% -- elapsed time 0.0014403387904167175 [2025-03-07 13:47:02,684] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4114 [2025-03-07 13:47:02,685] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:47:02,689] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:47:02,714] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4115 tokens and then stores 0 tokens [2025-03-07 13:47:02,715] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.42it/s, est. speed input: 5836.36 toks/s, output: 35.46 toks/s]\u001b[A\n",
      "Processing Queries:   6%|â–‹         | 65/1000 [00:49<12:42,  1.23it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:47:03,424] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:47:03,426] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.0002569742500782013 [2025-03-07 13:47:03,427] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4115 tokens in total) --hit rate 89.96% -- elapsed time 0.002123303711414337 [2025-03-07 13:47:03,428] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4114 [2025-03-07 13:47:03,429] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:47:03,434] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:47:03,459] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4115 tokens and then stores 0 tokens [2025-03-07 13:47:03,460] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.41it/s, est. speed input: 5811.08 toks/s, output: 35.30 toks/s]\u001b[A\n",
      "Processing Queries:   7%|â–‹         | 66/1000 [00:50<12:22,  1.26it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:47:04,171] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:47:04,173] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.00027256086468696594 [2025-03-07 13:47:04,175] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4115 tokens in total) --hit rate 89.97% -- elapsed time 0.0022145528346300125 [2025-03-07 13:47:04,175] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4114 [2025-03-07 13:47:04,177] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:47:04,182] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:47:04,207] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4115 tokens and then stores 0 tokens [2025-03-07 13:47:04,209] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.41it/s, est. speed input: 5807.03 toks/s, output: 35.28 toks/s]\u001b[A\n",
      "Processing Queries:   7%|â–‹         | 67/1000 [00:50<12:08,  1.28it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:47:04,920] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:47:04,922] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.0002556499093770981 [2025-03-07 13:47:04,923] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4115 tokens in total) --hit rate 89.98% -- elapsed time 0.0020831841975450516 [2025-03-07 13:47:04,924] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4114 [2025-03-07 13:47:04,925] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:47:04,930] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:47:04,955] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4115 tokens and then stores 0 tokens [2025-03-07 13:47:04,957] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.41it/s, est. speed input: 5815.48 toks/s, output: 35.33 toks/s]\u001b[A\n",
      "Processing Queries:   7%|â–‹         | 68/1000 [00:51<11:58,  1.30it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:47:05,665] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:47:05,667] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.00026013515889644623 [2025-03-07 13:47:05,669] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4115 tokens in total) --hit rate 89.99% -- elapsed time 0.0022805072367191315 [2025-03-07 13:47:05,669] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4114 [2025-03-07 13:47:05,671] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:47:05,676] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:47:05,701] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4115 tokens and then stores 0 tokens [2025-03-07 13:47:05,703] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.40it/s, est. speed input: 5802.94 toks/s, output: 35.25 toks/s]\u001b[A\n",
      "Processing Queries:   7%|â–‹         | 69/1000 [00:52<11:51,  1.31it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:47:06,413] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:47:06,415] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.00025660544633865356 [2025-03-07 13:47:06,417] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4115 tokens in total) --hit rate 90.00% -- elapsed time 0.0020897146314382553 [2025-03-07 13:47:06,417] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4114 [2025-03-07 13:47:06,418] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:47:06,423] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:47:06,449] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4115 tokens and then stores 0 tokens [2025-03-07 13:47:06,450] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.41it/s, est. speed input: 5811.17 toks/s, output: 35.30 toks/s]\u001b[A\n",
      "Processing Queries:   7%|â–‹         | 70/1000 [00:53<11:45,  1.32it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:47:07,162] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:47:07,164] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 0 chunks [2025-03-07 13:47:07,165] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:385\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mReturning the original input! [2025-03-07 13:47:07,166] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:663\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.PREFILL: 1>] [2025-03-07 13:47:07,748] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the put() method [2025-03-07 13:47:07,834] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:99\n",
      "\u001b[33mINFO LMCache: \u001b[0mStored/updated 17 chunks, total time 0.00s, make chunks time 0.00s [2025-03-07 13:47:07,835] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:315\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 0 tokens and then stores 4112 tokens [2025-03-07 13:47:07,836] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.18s/it, est. speed input: 3482.55 toks/s, output: 15.24 toks/s]\u001b[A\n",
      "Processing Queries:   7%|â–‹         | 71/1000 [00:54<13:53,  1.11it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:47:08,386] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:47:08,387] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.0002627931535243988 [2025-03-07 13:47:08,389] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4112 tokens in total) --hit rate 89.92% -- elapsed time 0.0020836368203163147 [2025-03-07 13:47:08,389] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4111 [2025-03-07 13:47:08,391] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:47:08,395] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:47:08,421] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4112 tokens and then stores 0 tokens [2025-03-07 13:47:08,423] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.91it/s, est. speed input: 7877.03 toks/s, output: 34.48 toks/s]\u001b[A\n",
      "Processing Queries:   7%|â–‹         | 72/1000 [00:54<12:20,  1.25it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:47:08,949] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:47:08,951] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.0002607051283121109 [2025-03-07 13:47:08,952] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4112 tokens in total) --hit rate 89.93% -- elapsed time 0.00215030275285244 [2025-03-07 13:47:08,953] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4111 [2025-03-07 13:47:08,954] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:47:08,959] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:47:08,984] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4112 tokens and then stores 0 tokens [2025-03-07 13:47:08,986] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.91it/s, est. speed input: 7877.95 toks/s, output: 34.48 toks/s]\u001b[A\n",
      "Processing Queries:   7%|â–‹         | 73/1000 [00:55<11:14,  1.37it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:47:09,512] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:47:09,513] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.0002530291676521301 [2025-03-07 13:47:09,515] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4112 tokens in total) --hit rate 89.94% -- elapsed time 0.0020757876336574554 [2025-03-07 13:47:09,515] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4111 [2025-03-07 13:47:09,517] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:47:09,521] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:47:09,547] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4112 tokens and then stores 0 tokens [2025-03-07 13:47:09,549] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.90it/s, est. speed input: 7877.91 toks/s, output: 34.48 toks/s]\u001b[A\n",
      "Processing Queries:   7%|â–‹         | 74/1000 [00:55<10:28,  1.47it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:47:10,073] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:47:10,075] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.00025817565619945526 [2025-03-07 13:47:10,076] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4112 tokens in total) --hit rate 89.95% -- elapsed time 0.0017149019986391068 [2025-03-07 13:47:10,077] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4111 [2025-03-07 13:47:10,077] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:47:10,082] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:47:10,107] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4112 tokens and then stores 0 tokens [2025-03-07 13:47:10,109] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.91it/s, est. speed input: 7891.77 toks/s, output: 34.54 toks/s]\u001b[A\n",
      "Processing Queries:   8%|â–Š         | 75/1000 [00:56<09:54,  1.56it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:47:10,640] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:47:10,642] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.00025562383234500885 [2025-03-07 13:47:10,643] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4112 tokens in total) --hit rate 89.96% -- elapsed time 0.0021254532039165497 [2025-03-07 13:47:10,644] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4111 [2025-03-07 13:47:10,645] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:47:10,650] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:47:10,675] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4112 tokens and then stores 0 tokens [2025-03-07 13:47:10,677] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.91it/s, est. speed input: 7880.11 toks/s, output: 34.49 toks/s]\u001b[A\n",
      "Processing Queries:   8%|â–Š         | 76/1000 [00:57<09:32,  1.61it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:47:11,199] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:47:11,201] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.00025861524045467377 [2025-03-07 13:47:11,202] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4112 tokens in total) --hit rate 89.97% -- elapsed time 0.0020974762737751007 [2025-03-07 13:47:11,203] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4111 [2025-03-07 13:47:11,204] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:47:11,209] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:47:11,234] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4112 tokens and then stores 0 tokens [2025-03-07 13:47:11,236] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.90it/s, est. speed input: 7869.65 toks/s, output: 34.45 toks/s]\u001b[A\n",
      "Processing Queries:   8%|â–Š         | 77/1000 [00:57<09:15,  1.66it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:47:11,763] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:47:11,765] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.00026933103799819946 [2025-03-07 13:47:11,766] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4112 tokens in total) --hit rate 89.98% -- elapsed time 0.002174902707338333 [2025-03-07 13:47:11,767] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4111 [2025-03-07 13:47:11,768] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:47:11,773] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:47:11,798] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4112 tokens and then stores 0 tokens [2025-03-07 13:47:11,800] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.91it/s, est. speed input: 7888.96 toks/s, output: 34.53 toks/s]\u001b[A\n",
      "Processing Queries:   8%|â–Š         | 78/1000 [00:58<09:04,  1.69it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:47:12,321] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:47:12,322] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.00025313161313533783 [2025-03-07 13:47:12,324] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4112 tokens in total) --hit rate 89.99% -- elapsed time 0.0020693447440862656 [2025-03-07 13:47:12,324] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4111 [2025-03-07 13:47:12,326] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:47:12,330] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:47:12,356] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4112 tokens and then stores 0 tokens [2025-03-07 13:47:12,358] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.91it/s, est. speed input: 7877.19 toks/s, output: 34.48 toks/s]\u001b[A\n",
      "Processing Queries:   8%|â–Š         | 79/1000 [00:58<08:54,  1.72it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:47:12,884] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:47:12,886] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.0004015956073999405 [2025-03-07 13:47:12,888] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4112 tokens in total) --hit rate 90.00% -- elapsed time 0.0020720772445201874 [2025-03-07 13:47:12,888] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4111 [2025-03-07 13:47:12,889] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:47:12,894] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:47:12,920] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4112 tokens and then stores 0 tokens [2025-03-07 13:47:12,922] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.91it/s, est. speed input: 7883.16 toks/s, output: 34.51 toks/s]\u001b[A\n",
      "Processing Queries:   8%|â–Š         | 80/1000 [00:59<08:49,  1.74it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:47:13,450] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:47:13,452] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 0 chunks [2025-03-07 13:47:13,453] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:385\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mReturning the original input! [2025-03-07 13:47:13,454] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:663\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.PREFILL: 1>] [2025-03-07 13:47:14,035] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the put() method [2025-03-07 13:47:14,120] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:99\n",
      "\u001b[33mINFO LMCache: \u001b[0mStored/updated 17 chunks, total time 0.00s, make chunks time 0.00s [2025-03-07 13:47:14,121] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:315\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 0 tokens and then stores 4116 tokens [2025-03-07 13:47:14,122] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.03s/it, est. speed input: 4011.19 toks/s, output: 11.69 toks/s]\u001b[A\n",
      "Processing Queries:   8%|â–Š         | 81/1000 [01:00<11:05,  1.38it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:47:14,516] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:47:14,518] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.0002474021166563034 [2025-03-07 13:47:14,519] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4116 tokens in total) --hit rate 89.92% -- elapsed time 0.002088945358991623 [2025-03-07 13:47:14,520] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4115 [2025-03-07 13:47:14,521] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:47:14,526] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:47:14,551] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4116 tokens and then stores 0 tokens [2025-03-07 13:47:14,553] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.74it/s, est. speed input: 11373.75 toks/s, output: 33.16 toks/s]\u001b[A\n",
      "Processing Queries:   8%|â–Š         | 82/1000 [01:00<09:36,  1.59it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:47:14,917] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:47:14,919] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.0002507772296667099 [2025-03-07 13:47:14,920] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4116 tokens in total) --hit rate 89.93% -- elapsed time 0.0020561572164297104 [2025-03-07 13:47:14,921] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4115 [2025-03-07 13:47:14,922] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:47:14,926] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:47:14,952] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4116 tokens and then stores 0 tokens [2025-03-07 13:47:14,954] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.75it/s, est. speed input: 11388.69 toks/s, output: 33.20 toks/s]\u001b[A\n",
      "Processing Queries:   8%|â–Š         | 83/1000 [01:01<08:32,  1.79it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:47:15,316] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:47:15,317] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.00026020966470241547 [2025-03-07 13:47:15,319] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4116 tokens in total) --hit rate 89.94% -- elapsed time 0.0020671337842941284 [2025-03-07 13:47:15,319] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4115 [2025-03-07 13:47:15,320] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:47:15,325] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:47:15,351] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4116 tokens and then stores 0 tokens [2025-03-07 13:47:15,353] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.74it/s, est. speed input: 11361.15 toks/s, output: 33.12 toks/s]\u001b[A\n",
      "Processing Queries:   8%|â–Š         | 84/1000 [01:01<07:48,  1.96it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:47:15,715] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:47:15,717] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.00025343894958496094 [2025-03-07 13:47:15,718] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4116 tokens in total) --hit rate 89.95% -- elapsed time 0.002079661935567856 [2025-03-07 13:47:15,719] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4115 [2025-03-07 13:47:15,720] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:47:15,725] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:47:15,750] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4116 tokens and then stores 0 tokens [2025-03-07 13:47:15,752] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.75it/s, est. speed input: 11393.13 toks/s, output: 33.21 toks/s]\u001b[A\n",
      "Processing Queries:   8%|â–Š         | 85/1000 [01:02<07:16,  2.09it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:47:16,116] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:47:16,118] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.0002595726400613785 [2025-03-07 13:47:16,119] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4116 tokens in total) --hit rate 89.96% -- elapsed time 0.002081824466586113 [2025-03-07 13:47:16,120] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4115 [2025-03-07 13:47:16,121] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:47:16,126] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:47:16,151] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4116 tokens and then stores 0 tokens [2025-03-07 13:47:16,153] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.74it/s, est. speed input: 11375.09 toks/s, output: 33.16 toks/s]\u001b[A\n",
      "Processing Queries:   9%|â–Š         | 86/1000 [01:02<06:55,  2.20it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:47:16,514] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:47:16,516] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.00025178492069244385 [2025-03-07 13:47:16,517] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4116 tokens in total) --hit rate 89.97% -- elapsed time 0.002091437578201294 [2025-03-07 13:47:16,518] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4115 [2025-03-07 13:47:16,519] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:47:16,524] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:47:16,550] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4116 tokens and then stores 0 tokens [2025-03-07 13:47:16,552] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.74it/s, est. speed input: 11366.98 toks/s, output: 33.14 toks/s]\u001b[A\n",
      "Processing Queries:   9%|â–Š         | 87/1000 [01:02<06:39,  2.28it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:47:16,913] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:47:16,915] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.0002514924854040146 [2025-03-07 13:47:16,916] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4116 tokens in total) --hit rate 89.98% -- elapsed time 0.002114802598953247 [2025-03-07 13:47:16,917] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4115 [2025-03-07 13:47:16,918] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:47:16,923] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:47:16,948] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4116 tokens and then stores 0 tokens [2025-03-07 13:47:16,950] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.74it/s, est. speed input: 11366.63 toks/s, output: 33.14 toks/s]\u001b[A\n",
      "Processing Queries:   9%|â–‰         | 88/1000 [01:03<06:28,  2.35it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:47:17,313] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:47:17,314] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.0002478361129760742 [2025-03-07 13:47:17,316] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4116 tokens in total) --hit rate 89.99% -- elapsed time 0.0021131429821252823 [2025-03-07 13:47:17,316] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4115 [2025-03-07 13:47:17,318] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:47:17,322] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:47:17,348] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4116 tokens and then stores 0 tokens [2025-03-07 13:47:17,349] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.75it/s, est. speed input: 11391.50 toks/s, output: 33.21 toks/s]\u001b[A\n",
      "Processing Queries:   9%|â–‰         | 89/1000 [01:03<06:20,  2.39it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:47:17,713] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:47:17,715] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.0002496093511581421 [2025-03-07 13:47:17,717] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4116 tokens in total) --hit rate 90.00% -- elapsed time 0.002043401822447777 [2025-03-07 13:47:17,717] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4115 [2025-03-07 13:47:17,718] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:47:17,723] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:47:17,748] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4116 tokens and then stores 0 tokens [2025-03-07 13:47:17,750] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.75it/s, est. speed input: 11402.02 toks/s, output: 33.24 toks/s]\u001b[A\n",
      "Processing Queries:   9%|â–‰         | 90/1000 [01:04<06:15,  2.42it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:47:18,112] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:47:18,114] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 0 chunks [2025-03-07 13:47:18,115] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:385\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mReturning the original input! [2025-03-07 13:47:18,116] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:663\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.PREFILL: 1>] [2025-03-07 13:47:18,696] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the put() method [2025-03-07 13:47:18,784] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:99\n",
      "\u001b[33mINFO LMCache: \u001b[0mStored/updated 17 chunks, total time 0.00s, make chunks time 0.00s [2025-03-07 13:47:18,785] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:315\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 0 tokens and then stores 4124 tokens [2025-03-07 13:47:18,786] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.35s/it, est. speed input: 3071.07 toks/s, output: 17.87 toks/s]\u001b[A\n",
      "Processing Queries:   9%|â–‰         | 91/1000 [01:05<10:39,  1.42it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:47:19,499] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:47:19,501] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.0002526380121707916 [2025-03-07 13:47:19,502] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4124 tokens in total) --hit rate 89.93% -- elapsed time 0.002155628055334091 [2025-03-07 13:47:19,503] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4123 [2025-03-07 13:47:19,504] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:47:19,509] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:47:19,534] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4124 tokens and then stores 0 tokens [2025-03-07 13:47:19,536] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.46it/s, est. speed input: 6037.15 toks/s, output: 35.13 toks/s]\u001b[A\n",
      "Processing Queries:   9%|â–‰         | 92/1000 [01:06<10:44,  1.41it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:47:20,219] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:47:20,221] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.0002623405307531357 [2025-03-07 13:47:20,223] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4124 tokens in total) --hit rate 89.93% -- elapsed time 0.0020999759435653687 [2025-03-07 13:47:20,223] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4123 [2025-03-07 13:47:20,224] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:47:20,229] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:47:20,255] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4124 tokens and then stores 0 tokens [2025-03-07 13:47:20,257] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.46it/s, est. speed input: 6037.37 toks/s, output: 35.13 toks/s]\u001b[A\n",
      "Processing Queries:   9%|â–‰         | 93/1000 [01:06<10:46,  1.40it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:47:20,953] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:47:20,955] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.00027574412524700165 [2025-03-07 13:47:20,956] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4124 tokens in total) --hit rate 89.94% -- elapsed time 0.0021051280200481415 [2025-03-07 13:47:20,957] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4123 [2025-03-07 13:47:20,958] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:47:20,963] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:47:20,989] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4124 tokens and then stores 0 tokens [2025-03-07 13:47:20,991] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.46it/s, est. speed input: 6034.90 toks/s, output: 35.12 toks/s]\u001b[A\n",
      "Processing Queries:   9%|â–‰         | 94/1000 [01:07<10:51,  1.39it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:47:21,685] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:47:21,688] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.0003318134695291519 [2025-03-07 13:47:21,689] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4124 tokens in total) --hit rate 89.95% -- elapsed time 0.0023930761963129044 [2025-03-07 13:47:21,690] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4123 [2025-03-07 13:47:21,691] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:47:21,697] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:47:21,723] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4124 tokens and then stores 0 tokens [2025-03-07 13:47:21,725] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.43it/s, est. speed input: 5925.35 toks/s, output: 34.48 toks/s]\u001b[A\n",
      "Processing Queries:  10%|â–‰         | 95/1000 [01:08<10:58,  1.37it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:47:22,433] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:47:22,436] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.0003115832805633545 [2025-03-07 13:47:22,438] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4124 tokens in total) --hit rate 89.96% -- elapsed time 0.0034381914883852005 [2025-03-07 13:47:22,439] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4123 [2025-03-07 13:47:22,440] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:47:22,446] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:47:22,472] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4124 tokens and then stores 0 tokens [2025-03-07 13:47:22,475] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.44it/s, est. speed input: 5958.54 toks/s, output: 34.68 toks/s]\u001b[A\n",
      "Processing Queries:  10%|â–‰         | 96/1000 [01:09<11:01,  1.37it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:47:23,172] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:47:23,174] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.0003156624734401703 [2025-03-07 13:47:23,175] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4124 tokens in total) --hit rate 89.97% -- elapsed time 0.0014077350497245789 [2025-03-07 13:47:23,175] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4123 [2025-03-07 13:47:23,175] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:47:23,181] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:47:23,206] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4124 tokens and then stores 0 tokens [2025-03-07 13:47:23,207] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.45it/s, est. speed input: 6007.69 toks/s, output: 34.96 toks/s]\u001b[A\n",
      "Processing Queries:  10%|â–‰         | 97/1000 [01:09<11:02,  1.36it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:47:23,911] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:47:23,913] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.00031194835901260376 [2025-03-07 13:47:23,915] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4124 tokens in total) --hit rate 89.98% -- elapsed time 0.003403853625059128 [2025-03-07 13:47:23,916] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4123 [2025-03-07 13:47:23,918] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:47:23,923] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:47:23,949] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4124 tokens and then stores 0 tokens [2025-03-07 13:47:23,951] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.44it/s, est. speed input: 5966.18 toks/s, output: 34.72 toks/s]\u001b[A\n",
      "Processing Queries:  10%|â–‰         | 98/1000 [01:10<11:03,  1.36it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:47:24,658] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:47:24,661] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.0003104880452156067 [2025-03-07 13:47:24,662] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4124 tokens in total) --hit rate 89.99% -- elapsed time 0.0023441407829523087 [2025-03-07 13:47:24,663] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4123 [2025-03-07 13:47:24,664] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:47:24,670] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:47:24,695] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4124 tokens and then stores 0 tokens [2025-03-07 13:47:24,697] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.43it/s, est. speed input: 5931.42 toks/s, output: 34.52 toks/s]\u001b[A\n",
      "Processing Queries:  10%|â–‰         | 99/1000 [01:11<11:07,  1.35it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:47:25,411] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:47:25,413] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.00025204941630363464 [2025-03-07 13:47:25,414] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4124 tokens in total) --hit rate 90.00% -- elapsed time 0.002126278355717659 [2025-03-07 13:47:25,415] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4123 [2025-03-07 13:47:25,416] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:47:25,421] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:47:25,446] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4124 tokens and then stores 0 tokens [2025-03-07 13:47:25,449] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.45it/s, est. speed input: 5972.82 toks/s, output: 34.76 toks/s]\u001b[A\n",
      "Processing Queries:  10%|â–ˆ         | 100/1000 [01:12<11:07,  1.35it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:47:26,154] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:47:26,156] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 0 chunks [2025-03-07 13:47:26,157] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:385\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mReturning the original input! [2025-03-07 13:47:26,158] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:663\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.PREFILL: 1>] [2025-03-07 13:47:26,741] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the put() method [2025-03-07 13:47:26,827] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:99\n",
      "\u001b[33mINFO LMCache: \u001b[0mStored/updated 17 chunks, total time 0.00s, make chunks time 0.00s [2025-03-07 13:47:26,828] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:315\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 0 tokens and then stores 4116 tokens [2025-03-07 13:47:26,828] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:04<00:00,  4.12s/it, est. speed input: 1000.91 toks/s, output: 31.13 toks/s]\u001b[A\n",
      "Processing Queries:  10%|â–ˆ         | 101/1000 [01:16<26:31,  1.77s/it]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:47:30,315] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:47:30,316] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.00026580505073070526 [2025-03-07 13:47:30,318] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4116 tokens in total) --hit rate 89.93% -- elapsed time 0.0021133292466402054 [2025-03-07 13:47:30,318] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4115 [2025-03-07 13:47:30,320] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:47:30,325] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:47:30,350] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4116 tokens and then stores 0 tokens [2025-03-07 13:47:30,352] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:03<00:00,  3.46s/it, est. speed input: 1189.77 toks/s, output: 37.00 toks/s]\u001b[A\n",
      "Processing Queries:  10%|â–ˆ         | 102/1000 [01:19<34:17,  2.29s/it]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:47:33,822] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:47:33,824] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.000268762931227684 [2025-03-07 13:47:33,826] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4116 tokens in total) --hit rate 89.94% -- elapsed time 0.002141021192073822 [2025-03-07 13:47:33,826] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4115 [2025-03-07 13:47:33,827] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:47:33,832] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:47:33,858] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4116 tokens and then stores 0 tokens [2025-03-07 13:47:33,860] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:03<00:00,  3.47s/it, est. speed input: 1185.46 toks/s, output: 36.87 toks/s]\u001b[A\n",
      "Processing Queries:  10%|â–ˆ         | 103/1000 [01:23<39:45,  2.66s/it]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:47:37,337] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:47:37,338] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.00027533434331417084 [2025-03-07 13:47:37,339] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4116 tokens in total) --hit rate 89.94% -- elapsed time 0.0011689569801092148 [2025-03-07 13:47:37,339] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4115 [2025-03-07 13:47:37,339] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:47:37,344] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:47:37,369] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4116 tokens and then stores 0 tokens [2025-03-07 13:47:37,370] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:03<00:00,  3.45s/it, est. speed input: 1194.73 toks/s, output: 37.15 toks/s]\u001b[A\n",
      "Processing Queries:  10%|â–ˆ         | 104/1000 [01:26<43:25,  2.91s/it]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:47:40,829] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:47:40,831] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.0004075411707162857 [2025-03-07 13:47:40,832] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4116 tokens in total) --hit rate 89.95% -- elapsed time 0.0020212288945913315 [2025-03-07 13:47:40,833] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4115 [2025-03-07 13:47:40,834] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:47:40,839] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:47:40,864] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4116 tokens and then stores 0 tokens [2025-03-07 13:47:40,866] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:03<00:00,  3.46s/it, est. speed input: 1191.77 toks/s, output: 37.06 toks/s]\u001b[A\n",
      "Processing Queries:  10%|â–ˆ         | 105/1000 [01:30<46:02,  3.09s/it]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:47:44,330] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:47:44,332] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.0002642367035150528 [2025-03-07 13:47:44,333] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4116 tokens in total) --hit rate 89.96% -- elapsed time 0.0021164827048778534 [2025-03-07 13:47:44,334] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4115 [2025-03-07 13:47:44,335] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:47:44,340] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:47:44,366] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4116 tokens and then stores 0 tokens [2025-03-07 13:47:44,367] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:03<00:00,  3.45s/it, est. speed input: 1194.98 toks/s, output: 37.16 toks/s]\u001b[A\n",
      "Processing Queries:  11%|â–ˆ         | 106/1000 [01:33<47:47,  3.21s/it]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:47:47,825] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:47:47,827] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.0003683213144540787 [2025-03-07 13:47:47,829] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4116 tokens in total) --hit rate 89.97% -- elapsed time 0.002526523545384407 [2025-03-07 13:47:47,830] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4115 [2025-03-07 13:47:47,831] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:47:47,836] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:47:47,862] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4116 tokens and then stores 0 tokens [2025-03-07 13:47:47,864] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:03<00:00,  3.49s/it, est. speed input: 1180.54 toks/s, output: 36.71 toks/s]\u001b[A\n",
      "Processing Queries:  11%|â–ˆ         | 107/1000 [01:37<49:12,  3.31s/it]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:47:51,358] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:47:51,360] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.000269988551735878 [2025-03-07 13:47:51,362] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4116 tokens in total) --hit rate 89.98% -- elapsed time 0.002179492264986038 [2025-03-07 13:47:51,362] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4115 [2025-03-07 13:47:51,364] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:47:51,369] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:47:51,394] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4116 tokens and then stores 0 tokens [2025-03-07 13:47:51,396] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:03<00:00,  3.49s/it, est. speed input: 1181.68 toks/s, output: 36.75 toks/s]\u001b[A\n",
      "Processing Queries:  11%|â–ˆ         | 108/1000 [01:40<50:09,  3.37s/it]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:47:54,891] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:47:54,893] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.00031378865242004395 [2025-03-07 13:47:54,895] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4116 tokens in total) --hit rate 89.99% -- elapsed time 0.0024287402629852295 [2025-03-07 13:47:54,896] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4115 [2025-03-07 13:47:54,897] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:47:54,902] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:47:54,928] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4116 tokens and then stores 0 tokens [2025-03-07 13:47:54,930] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:03<00:00,  3.47s/it, est. speed input: 1188.49 toks/s, output: 36.96 toks/s]\u001b[A\n",
      "Processing Queries:  11%|â–ˆ         | 109/1000 [01:44<50:43,  3.42s/it]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:47:58,398] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:47:58,399] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.0002688337117433548 [2025-03-07 13:47:58,401] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4116 tokens in total) --hit rate 90.00% -- elapsed time 0.0023863613605499268 [2025-03-07 13:47:58,402] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4115 [2025-03-07 13:47:58,403] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:47:58,408] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:47:58,434] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4116 tokens and then stores 0 tokens [2025-03-07 13:47:58,435] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:03<00:00,  3.46s/it, est. speed input: 1192.10 toks/s, output: 37.07 toks/s]\u001b[A\n",
      "Processing Queries:  11%|â–ˆ         | 110/1000 [01:47<51:01,  3.44s/it]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:48:01,898] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:48:01,900] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 0 chunks [2025-03-07 13:48:01,901] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:385\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mReturning the original input! [2025-03-07 13:48:01,901] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:663\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.PREFILL: 1>] [2025-03-07 13:48:02,490] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the put() method [2025-03-07 13:48:02,575] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:99\n",
      "\u001b[33mINFO LMCache: \u001b[0mStored/updated 17 chunks, total time 0.00s, make chunks time 0.00s [2025-03-07 13:48:02,576] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:315\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 0 tokens and then stores 4114 tokens [2025-03-07 13:48:02,577] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.65s/it, est. speed input: 2502.47 toks/s, output: 21.29 toks/s]\u001b[A\n",
      "Processing Queries:  11%|â–ˆ         | 111/1000 [01:49<43:11,  2.91s/it]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:48:03,582] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:48:03,584] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.0003055855631828308 [2025-03-07 13:48:03,586] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4114 tokens in total) --hit rate 89.93% -- elapsed time 0.0021325256675481796 [2025-03-07 13:48:03,586] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4113 [2025-03-07 13:48:03,587] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:48:03,592] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:48:03,618] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4114 tokens and then stores 0 tokens [2025-03-07 13:48:03,619] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 4223.64 toks/s, output: 35.93 toks/s]\u001b[A\n",
      "Processing Queries:  11%|â–ˆ         | 112/1000 [01:50<34:42,  2.35s/it]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:48:04,598] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:48:04,600] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.00024846941232681274 [2025-03-07 13:48:04,601] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4114 tokens in total) --hit rate 89.94% -- elapsed time 0.00206596776843071 [2025-03-07 13:48:04,602] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4113 [2025-03-07 13:48:04,603] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:48:04,608] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:48:04,633] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4114 tokens and then stores 0 tokens [2025-03-07 13:48:04,635] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.03it/s, est. speed input: 4233.10 toks/s, output: 36.01 toks/s]\u001b[A\n",
      "Processing Queries:  11%|â–ˆâ–        | 113/1000 [01:51<28:45,  1.95s/it]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:48:05,610] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:48:05,612] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.0002554692327976227 [2025-03-07 13:48:05,613] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4114 tokens in total) --hit rate 89.95% -- elapsed time 0.002184709534049034 [2025-03-07 13:48:05,614] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4113 [2025-03-07 13:48:05,615] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:48:05,620] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:48:05,645] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4114 tokens and then stores 0 tokens [2025-03-07 13:48:05,647] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 4227.75 toks/s, output: 35.97 toks/s]\u001b[A\n",
      "Processing Queries:  11%|â–ˆâ–        | 114/1000 [01:52<24:35,  1.67s/it]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:48:06,624] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:48:06,626] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.00025988370180130005 [2025-03-07 13:48:06,627] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4114 tokens in total) --hit rate 89.95% -- elapsed time 0.002105148509144783 [2025-03-07 13:48:06,628] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4113 [2025-03-07 13:48:06,629] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:48:06,634] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:48:06,659] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4114 tokens and then stores 0 tokens [2025-03-07 13:48:06,661] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.03it/s, est. speed input: 4232.88 toks/s, output: 36.01 toks/s]\u001b[A\n",
      "Processing Queries:  12%|â–ˆâ–        | 115/1000 [01:53<21:40,  1.47s/it]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:48:07,641] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:48:07,643] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.0002529006451368332 [2025-03-07 13:48:07,644] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4114 tokens in total) --hit rate 89.96% -- elapsed time 0.0020867250859737396 [2025-03-07 13:48:07,645] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4113 [2025-03-07 13:48:07,646] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:48:07,651] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:48:07,676] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4114 tokens and then stores 0 tokens [2025-03-07 13:48:07,678] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 4222.81 toks/s, output: 35.92 toks/s]\u001b[A\n",
      "Processing Queries:  12%|â–ˆâ–        | 116/1000 [01:54<19:39,  1.33s/it]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:48:08,655] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:48:08,656] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.00025596097111701965 [2025-03-07 13:48:08,657] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4114 tokens in total) --hit rate 89.97% -- elapsed time 0.001372218132019043 [2025-03-07 13:48:08,657] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4113 [2025-03-07 13:48:08,658] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:48:08,663] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:48:08,688] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4114 tokens and then stores 0 tokens [2025-03-07 13:48:08,689] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.03it/s, est. speed input: 4248.92 toks/s, output: 36.15 toks/s]\u001b[A\n",
      "Processing Queries:  12%|â–ˆâ–        | 117/1000 [01:55<18:12,  1.24s/it]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:48:09,664] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:48:09,665] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.0002500806003808975 [2025-03-07 13:48:09,666] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4114 tokens in total) --hit rate 89.98% -- elapsed time 0.0014115646481513977 [2025-03-07 13:48:09,666] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4113 [2025-03-07 13:48:09,667] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:48:09,672] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:48:09,697] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4114 tokens and then stores 0 tokens [2025-03-07 13:48:09,698] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.03it/s, est. speed input: 4239.51 toks/s, output: 36.07 toks/s]\u001b[A\n",
      "Processing Queries:  12%|â–ˆâ–        | 118/1000 [01:56<17:10,  1.17s/it]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:48:10,675] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:48:10,676] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.00027114152908325195 [2025-03-07 13:48:10,678] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4114 tokens in total) --hit rate 89.99% -- elapsed time 0.0017750430852174759 [2025-03-07 13:48:10,678] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4113 [2025-03-07 13:48:10,679] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:48:10,683] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:48:10,709] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4114 tokens and then stores 0 tokens [2025-03-07 13:48:10,710] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.03it/s, est. speed input: 4240.12 toks/s, output: 36.07 toks/s]\u001b[A\n",
      "Processing Queries:  12%|â–ˆâ–        | 119/1000 [01:57<16:28,  1.12s/it]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:48:11,682] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:48:11,684] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.0002490431070327759 [2025-03-07 13:48:11,685] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4114 tokens in total) --hit rate 90.00% -- elapsed time 0.002032371237874031 [2025-03-07 13:48:11,686] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4113 [2025-03-07 13:48:11,687] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:48:11,692] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:48:11,717] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4114 tokens and then stores 0 tokens [2025-03-07 13:48:11,719] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.03it/s, est. speed input: 4229.81 toks/s, output: 35.98 toks/s]\u001b[A\n",
      "Processing Queries:  12%|â–ˆâ–        | 120/1000 [01:58<15:57,  1.09s/it]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:48:12,696] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:48:12,698] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 0 chunks [2025-03-07 13:48:12,698] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:385\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mReturning the original input! [2025-03-07 13:48:12,700] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:663\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.PREFILL: 1>] [2025-03-07 13:48:13,289] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the put() method [2025-03-07 13:48:13,374] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:99\n",
      "\u001b[33mINFO LMCache: \u001b[0mStored/updated 17 chunks, total time 0.00s, make chunks time 0.00s [2025-03-07 13:48:13,375] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:315\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 0 tokens and then stores 4116 tokens [2025-03-07 13:48:13,376] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.19s/it, est. speed input: 3466.92 toks/s, output: 15.16 toks/s]\u001b[A\n",
      "Processing Queries:  12%|â–ˆâ–        | 121/1000 [01:59<16:33,  1.13s/it]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:48:13,922] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:48:13,924] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.00025070272386074066 [2025-03-07 13:48:13,925] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4116 tokens in total) --hit rate 89.93% -- elapsed time 0.002103818580508232 [2025-03-07 13:48:13,926] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4115 [2025-03-07 13:48:13,928] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:48:13,934] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:48:13,959] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4116 tokens and then stores 0 tokens [2025-03-07 13:48:13,960] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.91it/s, est. speed input: 7882.94 toks/s, output: 34.47 toks/s]\u001b[A\n",
      "Processing Queries:  12%|â–ˆâ–        | 122/1000 [02:00<14:02,  1.04it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:48:14,485] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:48:14,487] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.00041332468390464783 [2025-03-07 13:48:14,489] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4116 tokens in total) --hit rate 89.94% -- elapsed time 0.0025400500744581223 [2025-03-07 13:48:14,490] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4115 [2025-03-07 13:48:14,491] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:48:14,497] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:48:14,523] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4116 tokens and then stores 0 tokens [2025-03-07 13:48:14,525] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.90it/s, est. speed input: 7837.96 toks/s, output: 34.28 toks/s]\u001b[A\n",
      "Processing Queries:  12%|â–ˆâ–        | 123/1000 [02:00<12:16,  1.19it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:48:15,048] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:48:15,049] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.000252431258559227 [2025-03-07 13:48:15,050] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4116 tokens in total) --hit rate 89.95% -- elapsed time 0.001451600342988968 [2025-03-07 13:48:15,050] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4115 [2025-03-07 13:48:15,051] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:48:15,055] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:48:15,080] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4116 tokens and then stores 0 tokens [2025-03-07 13:48:15,082] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.93it/s, est. speed input: 7950.32 toks/s, output: 34.77 toks/s]\u001b[A\n",
      "Processing Queries:  12%|â–ˆâ–        | 124/1000 [02:01<11:01,  1.32it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:48:15,602] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:48:15,603] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.00026156753301620483 [2025-03-07 13:48:15,604] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4116 tokens in total) --hit rate 89.95% -- elapsed time 0.0012405980378389359 [2025-03-07 13:48:15,604] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4115 [2025-03-07 13:48:15,605] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:48:15,609] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:48:15,634] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4116 tokens and then stores 0 tokens [2025-03-07 13:48:15,635] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.92it/s, est. speed input: 7964.63 toks/s, output: 34.83 toks/s]\u001b[A\n",
      "Processing Queries:  12%|â–ˆâ–Ž        | 125/1000 [02:02<10:08,  1.44it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:48:16,160] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:48:16,162] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.0002477671951055527 [2025-03-07 13:48:16,163] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4116 tokens in total) --hit rate 89.96% -- elapsed time 0.001788044348359108 [2025-03-07 13:48:16,164] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4115 [2025-03-07 13:48:16,165] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:48:16,169] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:48:16,195] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4116 tokens and then stores 0 tokens [2025-03-07 13:48:16,196] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.92it/s, est. speed input: 7925.96 toks/s, output: 34.66 toks/s]\u001b[A\n",
      "Processing Queries:  13%|â–ˆâ–Ž        | 126/1000 [02:02<09:31,  1.53it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:48:16,715] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:48:16,716] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.00025508739054203033 [2025-03-07 13:48:16,717] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4116 tokens in total) --hit rate 89.97% -- elapsed time 0.0011213049292564392 [2025-03-07 13:48:16,717] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4115 [2025-03-07 13:48:16,717] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:48:16,722] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:48:16,747] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4116 tokens and then stores 0 tokens [2025-03-07 13:48:16,749] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.92it/s, est. speed input: 7955.77 toks/s, output: 34.79 toks/s]\u001b[A\n",
      "Processing Queries:  13%|â–ˆâ–Ž        | 127/1000 [02:03<09:05,  1.60it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:48:17,275] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:48:17,277] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.00025539472699165344 [2025-03-07 13:48:17,278] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4116 tokens in total) --hit rate 89.98% -- elapsed time 0.001454956829547882 [2025-03-07 13:48:17,278] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4115 [2025-03-07 13:48:17,279] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:48:17,283] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:48:17,308] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4116 tokens and then stores 0 tokens [2025-03-07 13:48:17,309] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.93it/s, est. speed input: 7961.21 toks/s, output: 34.81 toks/s]\u001b[A\n",
      "Processing Queries:  13%|â–ˆâ–Ž        | 128/1000 [02:03<08:47,  1.65it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:48:17,827] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:48:17,829] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.0002516079694032669 [2025-03-07 13:48:17,830] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4116 tokens in total) --hit rate 89.99% -- elapsed time 0.0021033361554145813 [2025-03-07 13:48:17,831] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4115 [2025-03-07 13:48:17,832] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:48:17,837] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:48:17,862] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4116 tokens and then stores 0 tokens [2025-03-07 13:48:17,864] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.91it/s, est. speed input: 7874.59 toks/s, output: 34.44 toks/s]\u001b[A\n",
      "Processing Queries:  13%|â–ˆâ–Ž        | 129/1000 [02:04<08:34,  1.69it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:48:18,389] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:48:18,391] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.0003073718398809433 [2025-03-07 13:48:18,392] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4116 tokens in total) --hit rate 90.00% -- elapsed time 0.0014489386230707169 [2025-03-07 13:48:18,392] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4115 [2025-03-07 13:48:18,393] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:48:18,398] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:48:18,423] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4116 tokens and then stores 0 tokens [2025-03-07 13:48:18,425] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.89it/s, est. speed input: 7825.48 toks/s, output: 34.22 toks/s]\u001b[A\n",
      "Processing Queries:  13%|â–ˆâ–Ž        | 130/1000 [02:04<08:28,  1.71it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:48:18,967] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:48:18,970] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 0 chunks [2025-03-07 13:48:18,971] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:385\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mReturning the original input! [2025-03-07 13:48:18,973] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:663\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.PREFILL: 1>] [2025-03-07 13:48:19,563] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the put() method [2025-03-07 13:48:19,666] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:99\n",
      "\u001b[33mINFO LMCache: \u001b[0mStored/updated 17 chunks, total time 0.00s, make chunks time 0.00s [2025-03-07 13:48:19,667] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:315\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 0 tokens and then stores 4112 tokens [2025-03-07 13:48:19,669] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.17s/it, est. speed input: 3528.30 toks/s, output: 13.73 toks/s]\u001b[A\n",
      "Processing Queries:  13%|â–ˆâ–Ž        | 131/1000 [02:06<11:12,  1.29it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:48:20,190] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:48:20,192] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.00032322853803634644 [2025-03-07 13:48:20,194] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4112 tokens in total) --hit rate 89.93% -- elapsed time 0.0029281210154294968 [2025-03-07 13:48:20,195] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4111 [2025-03-07 13:48:20,197] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:48:20,202] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:48:20,228] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4112 tokens and then stores 0 tokens [2025-03-07 13:48:20,230] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.08it/s, est. speed input: 8597.94 toks/s, output: 33.45 toks/s]\u001b[A\n",
      "Processing Queries:  13%|â–ˆâ–Ž        | 132/1000 [02:06<10:09,  1.42it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:48:20,719] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:48:20,722] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.0002986490726470947 [2025-03-07 13:48:20,724] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4112 tokens in total) --hit rate 89.94% -- elapsed time 0.0029558036476373672 [2025-03-07 13:48:20,725] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4111 [2025-03-07 13:48:20,726] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:48:20,731] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:48:20,757] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4112 tokens and then stores 0 tokens [2025-03-07 13:48:20,760] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.06it/s, est. speed input: 8508.35 toks/s, output: 33.10 toks/s]\u001b[A\n",
      "Processing Queries:  13%|â–ˆâ–Ž        | 133/1000 [02:07<09:24,  1.53it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:48:21,276] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:48:21,279] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.00047657638788223267 [2025-03-07 13:48:21,281] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4112 tokens in total) --hit rate 89.95% -- elapsed time 0.003403913229703903 [2025-03-07 13:48:21,282] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4111 [2025-03-07 13:48:21,284] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:48:21,292] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:48:21,319] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4112 tokens and then stores 0 tokens [2025-03-07 13:48:21,321] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.05it/s, est. speed input: 8472.48 toks/s, output: 32.97 toks/s]\u001b[A\n",
      "Processing Queries:  13%|â–ˆâ–Ž        | 134/1000 [02:07<08:59,  1.60it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:48:21,810] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:48:21,812] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.00025193579494953156 [2025-03-07 13:48:21,813] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4112 tokens in total) --hit rate 89.96% -- elapsed time 0.002046791836619377 [2025-03-07 13:48:21,814] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4111 [2025-03-07 13:48:21,815] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:48:21,820] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:48:21,845] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4112 tokens and then stores 0 tokens [2025-03-07 13:48:21,847] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.12it/s, est. speed input: 8763.77 toks/s, output: 34.10 toks/s]\u001b[A\n",
      "Processing Queries:  14%|â–ˆâ–Ž        | 135/1000 [02:08<08:32,  1.69it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:48:22,321] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:48:22,323] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.00024828314781188965 [2025-03-07 13:48:22,324] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4112 tokens in total) --hit rate 89.96% -- elapsed time 0.0020630396902561188 [2025-03-07 13:48:22,325] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4111 [2025-03-07 13:48:22,326] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:48:22,331] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:48:22,356] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4112 tokens and then stores 0 tokens [2025-03-07 13:48:22,358] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.13it/s, est. speed input: 8801.72 toks/s, output: 34.25 toks/s]\u001b[A\n",
      "Processing Queries:  14%|â–ˆâ–Ž        | 136/1000 [02:08<08:10,  1.76it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:48:22,825] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:48:22,827] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.0002507828176021576 [2025-03-07 13:48:22,829] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4112 tokens in total) --hit rate 89.97% -- elapsed time 0.0020780470222234726 [2025-03-07 13:48:22,829] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4111 [2025-03-07 13:48:22,830] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:48:22,835] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:48:22,861] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4112 tokens and then stores 0 tokens [2025-03-07 13:48:22,864] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.12it/s, est. speed input: 8749.71 toks/s, output: 34.04 toks/s]\u001b[A\n",
      "Processing Queries:  14%|â–ˆâ–Ž        | 137/1000 [02:09<07:54,  1.82it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:48:23,345] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:48:23,347] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.0002642795443534851 [2025-03-07 13:48:23,348] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4112 tokens in total) --hit rate 89.98% -- elapsed time 0.0021435413509607315 [2025-03-07 13:48:23,349] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4111 [2025-03-07 13:48:23,350] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:48:23,355] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:48:23,380] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4112 tokens and then stores 0 tokens [2025-03-07 13:48:23,382] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.12it/s, est. speed input: 8752.28 toks/s, output: 34.05 toks/s]\u001b[A\n",
      "Processing Queries:  14%|â–ˆâ–        | 138/1000 [02:09<07:45,  1.85it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:48:23,855] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:48:23,856] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.00025947391986846924 [2025-03-07 13:48:23,858] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4112 tokens in total) --hit rate 89.99% -- elapsed time 0.002095421776175499 [2025-03-07 13:48:23,858] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4111 [2025-03-07 13:48:23,860] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:48:23,864] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:48:23,890] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4112 tokens and then stores 0 tokens [2025-03-07 13:48:23,891] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.12it/s, est. speed input: 8782.13 toks/s, output: 34.17 toks/s]\u001b[A\n",
      "Processing Queries:  14%|â–ˆâ–        | 139/1000 [02:10<07:36,  1.88it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:48:24,367] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:48:24,368] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.00039970502257347107 [2025-03-07 13:48:24,370] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4112 tokens in total) --hit rate 90.00% -- elapsed time 0.0021391045302152634 [2025-03-07 13:48:24,370] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4111 [2025-03-07 13:48:24,372] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:48:24,377] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:48:24,402] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4112 tokens and then stores 0 tokens [2025-03-07 13:48:24,404] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.12it/s, est. speed input: 8751.71 toks/s, output: 34.05 toks/s]\u001b[A\n",
      "Processing Queries:  14%|â–ˆâ–        | 140/1000 [02:10<07:31,  1.90it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:48:24,876] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:48:24,878] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 0 chunks [2025-03-07 13:48:24,879] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:385\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mReturning the original input! [2025-03-07 13:48:24,880] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:663\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.PREFILL: 1>] [2025-03-07 13:48:25,469] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the put() method [2025-03-07 13:48:25,552] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:99\n",
      "\u001b[33mINFO LMCache: \u001b[0mStored/updated 17 chunks, total time 0.00s, make chunks time 0.00s [2025-03-07 13:48:25,553] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:315\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 0 tokens and then stores 4112 tokens [2025-03-07 13:48:25,554] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.16s/it, est. speed input: 3553.16 toks/s, output: 14.69 toks/s]\u001b[A\n",
      "Processing Queries:  14%|â–ˆâ–        | 141/1000 [02:11<10:24,  1.38it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:48:26,077] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:48:26,078] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.00025704875588417053 [2025-03-07 13:48:26,080] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4112 tokens in total) --hit rate 89.93% -- elapsed time 0.002167666330933571 [2025-03-07 13:48:26,080] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4111 [2025-03-07 13:48:26,082] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:48:26,087] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:48:26,112] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4112 tokens and then stores 0 tokens [2025-03-07 13:48:26,114] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.00it/s, est. speed input: 8269.63 toks/s, output: 34.19 toks/s]\u001b[A\n",
      "Processing Queries:  14%|â–ˆâ–        | 142/1000 [02:12<09:35,  1.49it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:48:26,613] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:48:26,615] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.00025145336985588074 [2025-03-07 13:48:26,616] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4112 tokens in total) --hit rate 89.94% -- elapsed time 0.002085026353597641 [2025-03-07 13:48:26,617] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4111 [2025-03-07 13:48:26,618] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:48:26,623] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:48:26,648] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4112 tokens and then stores 0 tokens [2025-03-07 13:48:26,650] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.00it/s, est. speed input: 8300.79 toks/s, output: 34.32 toks/s]\u001b[A\n",
      "Processing Queries:  14%|â–ˆâ–        | 143/1000 [02:13<09:00,  1.59it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:48:27,148] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:48:27,150] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.0002563372254371643 [2025-03-07 13:48:27,151] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4112 tokens in total) --hit rate 89.95% -- elapsed time 0.0021852608770132065 [2025-03-07 13:48:27,152] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4111 [2025-03-07 13:48:27,153] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:48:27,158] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:48:27,184] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4112 tokens and then stores 0 tokens [2025-03-07 13:48:27,186] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.01it/s, est. speed input: 8293.05 toks/s, output: 34.28 toks/s]\u001b[A\n",
      "Processing Queries:  14%|â–ˆâ–        | 144/1000 [02:13<08:34,  1.66it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:48:27,684] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:48:27,685] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.0002528410404920578 [2025-03-07 13:48:27,687] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4112 tokens in total) --hit rate 89.96% -- elapsed time 0.002102028578519821 [2025-03-07 13:48:27,687] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4111 [2025-03-07 13:48:27,689] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:48:27,694] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:48:27,719] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4112 tokens and then stores 0 tokens [2025-03-07 13:48:27,721] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.01it/s, est. speed input: 8314.68 toks/s, output: 34.37 toks/s]\u001b[A\n",
      "Processing Queries:  14%|â–ˆâ–        | 145/1000 [02:14<08:16,  1.72it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:48:28,215] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:48:28,217] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.00025854259729385376 [2025-03-07 13:48:28,218] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4112 tokens in total) --hit rate 89.96% -- elapsed time 0.0019093547016382217 [2025-03-07 13:48:28,219] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4111 [2025-03-07 13:48:28,220] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:48:28,224] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:48:28,250] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4112 tokens and then stores 0 tokens [2025-03-07 13:48:28,251] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.01it/s, est. speed input: 8315.97 toks/s, output: 34.38 toks/s]\u001b[A\n",
      "Processing Queries:  15%|â–ˆâ–        | 146/1000 [02:14<08:03,  1.76it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:48:28,773] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:48:28,775] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.0003956872969865799 [2025-03-07 13:48:28,776] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4112 tokens in total) --hit rate 89.97% -- elapsed time 0.001986641436815262 [2025-03-07 13:48:28,777] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4111 [2025-03-07 13:48:28,778] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:48:28,783] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:48:28,808] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4112 tokens and then stores 0 tokens [2025-03-07 13:48:28,811] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.00it/s, est. speed input: 8281.63 toks/s, output: 34.24 toks/s]\u001b[A\n",
      "Processing Queries:  15%|â–ˆâ–        | 147/1000 [02:15<08:01,  1.77it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:48:29,309] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:48:29,311] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.00025611184537410736 [2025-03-07 13:48:29,313] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4112 tokens in total) --hit rate 89.98% -- elapsed time 0.0021695811301469803 [2025-03-07 13:48:29,313] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4111 [2025-03-07 13:48:29,315] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:48:29,320] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:48:29,345] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4112 tokens and then stores 0 tokens [2025-03-07 13:48:29,347] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.01it/s, est. speed input: 8300.99 toks/s, output: 34.32 toks/s]\u001b[A\n",
      "Processing Queries:  15%|â–ˆâ–        | 148/1000 [02:15<07:53,  1.80it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:48:29,861] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:48:29,863] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.0002587791532278061 [2025-03-07 13:48:29,865] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4112 tokens in total) --hit rate 89.99% -- elapsed time 0.002121102064847946 [2025-03-07 13:48:29,865] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4111 [2025-03-07 13:48:29,866] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:48:29,871] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:48:29,897] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4112 tokens and then stores 0 tokens [2025-03-07 13:48:29,899] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.01it/s, est. speed input: 8307.34 toks/s, output: 34.34 toks/s]\u001b[A\n",
      "Processing Queries:  15%|â–ˆâ–        | 149/1000 [02:16<07:51,  1.80it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:48:30,396] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:48:30,397] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.00029096007347106934 [2025-03-07 13:48:30,399] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4112 tokens in total) --hit rate 90.00% -- elapsed time 0.002100810408592224 [2025-03-07 13:48:30,399] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4111 [2025-03-07 13:48:30,401] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:48:30,405] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:48:30,431] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4112 tokens and then stores 0 tokens [2025-03-07 13:48:30,433] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.01it/s, est. speed input: 8301.93 toks/s, output: 34.32 toks/s]\u001b[A\n",
      "Processing Queries:  15%|â–ˆâ–Œ        | 150/1000 [02:16<07:46,  1.82it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:48:30,944] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:48:30,945] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 0 chunks [2025-03-07 13:48:30,946] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:385\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mReturning the original input! [2025-03-07 13:48:30,946] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:663\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.PREFILL: 1>] [2025-03-07 13:48:31,535] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the put() method [2025-03-07 13:48:31,620] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:99\n",
      "\u001b[33mINFO LMCache: \u001b[0mStored/updated 17 chunks, total time 0.00s, make chunks time 0.00s [2025-03-07 13:48:31,621] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:315\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 0 tokens and then stores 4119 tokens [2025-03-07 13:48:31,622] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.13s/it, est. speed input: 3641.07 toks/s, output: 14.14 toks/s]\u001b[A\n",
      "Processing Queries:  15%|â–ˆâ–Œ        | 151/1000 [02:18<10:28,  1.35it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:48:32,117] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:48:32,119] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.00025248900055885315 [2025-03-07 13:48:32,120] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4119 tokens in total) --hit rate 89.93% -- elapsed time 0.002118421718478203 [2025-03-07 13:48:32,121] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4118 [2025-03-07 13:48:32,122] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:48:32,127] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:48:32,152] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4119 tokens and then stores 0 tokens [2025-03-07 13:48:32,154] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.13it/s, est. speed input: 8801.45 toks/s, output: 34.19 toks/s]\u001b[A\n",
      "Processing Queries:  15%|â–ˆâ–Œ        | 152/1000 [02:18<09:28,  1.49it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:48:32,626] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:48:32,628] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.0002562180161476135 [2025-03-07 13:48:32,630] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4119 tokens in total) --hit rate 89.94% -- elapsed time 0.0020726099610328674 [2025-03-07 13:48:32,630] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4118 [2025-03-07 13:48:32,631] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:48:32,636] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:48:32,662] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4119 tokens and then stores 0 tokens [2025-03-07 13:48:32,663] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.13it/s, est. speed input: 8823.19 toks/s, output: 34.27 toks/s]\u001b[A\n",
      "Processing Queries:  15%|â–ˆâ–Œ        | 153/1000 [02:19<08:46,  1.61it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:48:33,150] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:48:33,152] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.0002559442073106766 [2025-03-07 13:48:33,153] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4119 tokens in total) --hit rate 89.95% -- elapsed time 0.0020522549748420715 [2025-03-07 13:48:33,154] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4118 [2025-03-07 13:48:33,155] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:48:33,160] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:48:33,185] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4119 tokens and then stores 0 tokens [2025-03-07 13:48:33,187] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.12it/s, est. speed input: 8794.70 toks/s, output: 34.16 toks/s]\u001b[A\n",
      "Processing Queries:  15%|â–ˆâ–Œ        | 154/1000 [02:19<08:21,  1.69it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:48:33,661] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:48:33,662] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.0002675168216228485 [2025-03-07 13:48:33,664] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4119 tokens in total) --hit rate 89.96% -- elapsed time 0.0021467283368110657 [2025-03-07 13:48:33,664] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4118 [2025-03-07 13:48:33,666] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:48:33,671] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:48:33,696] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4119 tokens and then stores 0 tokens [2025-03-07 13:48:33,698] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.13it/s, est. speed input: 8800.26 toks/s, output: 34.18 toks/s]\u001b[A\n",
      "Processing Queries:  16%|â–ˆâ–Œ        | 155/1000 [02:20<07:59,  1.76it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:48:34,169] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:48:34,171] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.00026537105441093445 [2025-03-07 13:48:34,172] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4119 tokens in total) --hit rate 89.96% -- elapsed time 0.002139320597052574 [2025-03-07 13:48:34,173] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4118 [2025-03-07 13:48:34,174] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:48:34,179] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:48:34,205] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4119 tokens and then stores 0 tokens [2025-03-07 13:48:34,207] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.12it/s, est. speed input: 8774.24 toks/s, output: 34.08 toks/s]\u001b[A\n",
      "Processing Queries:  16%|â–ˆâ–Œ        | 156/1000 [02:20<07:44,  1.82it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:48:34,681] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:48:34,683] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.000258665531873703 [2025-03-07 13:48:34,684] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4119 tokens in total) --hit rate 89.97% -- elapsed time 0.0021040309220552444 [2025-03-07 13:48:34,685] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4118 [2025-03-07 13:48:34,686] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:48:34,691] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:48:34,716] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4119 tokens and then stores 0 tokens [2025-03-07 13:48:34,718] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.13it/s, est. speed input: 8803.39 toks/s, output: 34.20 toks/s]\u001b[A\n",
      "Processing Queries:  16%|â–ˆâ–Œ        | 157/1000 [02:21<07:33,  1.86it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:48:35,189] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:48:35,191] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.00025665387511253357 [2025-03-07 13:48:35,192] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4119 tokens in total) --hit rate 89.98% -- elapsed time 0.002067066729068756 [2025-03-07 13:48:35,193] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4118 [2025-03-07 13:48:35,194] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:48:35,199] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:48:35,224] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4119 tokens and then stores 0 tokens [2025-03-07 13:48:35,226] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.12it/s, est. speed input: 8803.28 toks/s, output: 34.19 toks/s]\u001b[A\n",
      "Processing Queries:  16%|â–ˆâ–Œ        | 158/1000 [02:21<07:26,  1.89it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:48:35,701] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:48:35,702] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.0002508200705051422 [2025-03-07 13:48:35,704] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4119 tokens in total) --hit rate 89.99% -- elapsed time 0.002193484455347061 [2025-03-07 13:48:35,704] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4118 [2025-03-07 13:48:35,706] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:48:35,711] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:48:35,736] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4119 tokens and then stores 0 tokens [2025-03-07 13:48:35,738] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.12it/s, est. speed input: 8790.35 toks/s, output: 34.14 toks/s]\u001b[A\n",
      "Processing Queries:  16%|â–ˆâ–Œ        | 159/1000 [02:22<07:20,  1.91it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:48:36,213] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:48:36,215] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.00024980492889881134 [2025-03-07 13:48:36,216] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4119 tokens in total) --hit rate 90.00% -- elapsed time 0.0020758025348186493 [2025-03-07 13:48:36,217] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4118 [2025-03-07 13:48:36,218] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:48:36,223] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:48:36,248] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4119 tokens and then stores 0 tokens [2025-03-07 13:48:36,250] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.13it/s, est. speed input: 8805.75 toks/s, output: 34.20 toks/s]\u001b[A\n",
      "Processing Queries:  16%|â–ˆâ–Œ        | 160/1000 [02:22<07:17,  1.92it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:48:36,721] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:48:36,722] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 0 chunks [2025-03-07 13:48:36,723] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:385\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mReturning the original input! [2025-03-07 13:48:36,724] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:663\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.PREFILL: 1>] [2025-03-07 13:48:37,313] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the put() method [2025-03-07 13:48:37,407] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:99\n",
      "\u001b[33mINFO LMCache: \u001b[0mStored/updated 17 chunks, total time 0.00s, make chunks time 0.00s [2025-03-07 13:48:37,407] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:315\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 0 tokens and then stores 4111 tokens [2025-03-07 13:48:37,409] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.12s/it, est. speed input: 3670.02 toks/s, output: 13.39 toks/s]\u001b[A\n",
      "Processing Queries:  16%|â–ˆâ–Œ        | 161/1000 [02:23<09:57,  1.40it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:48:37,885] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:48:37,887] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.00025375373661518097 [2025-03-07 13:48:37,888] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4111 tokens in total) --hit rate 89.93% -- elapsed time 0.0020702220499515533 [2025-03-07 13:48:37,889] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4110 [2025-03-07 13:48:37,890] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:48:37,896] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:48:37,921] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4111 tokens and then stores 0 tokens [2025-03-07 13:48:37,923] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.23it/s, est. speed input: 9238.89 toks/s, output: 33.71 toks/s]\u001b[A\n",
      "Processing Queries:  16%|â–ˆâ–Œ        | 162/1000 [02:24<09:00,  1.55it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:48:38,369] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:48:38,371] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.00025745853781700134 [2025-03-07 13:48:38,372] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4111 tokens in total) --hit rate 89.94% -- elapsed time 0.002074791118502617 [2025-03-07 13:48:38,373] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4110 [2025-03-07 13:48:38,374] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:48:38,379] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:48:38,404] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4111 tokens and then stores 0 tokens [2025-03-07 13:48:38,406] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.25it/s, est. speed input: 9318.75 toks/s, output: 34.00 toks/s]\u001b[A\n",
      "Processing Queries:  16%|â–ˆâ–‹        | 163/1000 [02:24<08:18,  1.68it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:48:38,851] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:48:38,853] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.0004059765487909317 [2025-03-07 13:48:38,854] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4111 tokens in total) --hit rate 89.95% -- elapsed time 0.002039017155766487 [2025-03-07 13:48:38,855] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4110 [2025-03-07 13:48:38,856] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:48:38,861] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:48:38,886] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4111 tokens and then stores 0 tokens [2025-03-07 13:48:38,888] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.25it/s, est. speed input: 9314.78 toks/s, output: 33.99 toks/s]\u001b[A\n",
      "Processing Queries:  16%|â–ˆâ–‹        | 164/1000 [02:25<07:49,  1.78it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:48:39,330] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:48:39,332] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.0002476796507835388 [2025-03-07 13:48:39,333] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4111 tokens in total) --hit rate 89.96% -- elapsed time 0.0020976755768060684 [2025-03-07 13:48:39,334] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4110 [2025-03-07 13:48:39,335] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:48:39,340] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:48:39,365] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4111 tokens and then stores 0 tokens [2025-03-07 13:48:39,367] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.26it/s, est. speed input: 9325.08 toks/s, output: 34.02 toks/s]\u001b[A\n",
      "Processing Queries:  16%|â–ˆâ–‹        | 165/1000 [02:25<07:27,  1.86it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:48:39,806] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:48:39,807] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.0002492852509021759 [2025-03-07 13:48:39,808] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4111 tokens in total) --hit rate 89.97% -- elapsed time 0.0014195479452610016 [2025-03-07 13:48:39,809] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4110 [2025-03-07 13:48:39,809] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:48:39,814] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:48:39,839] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4111 tokens and then stores 0 tokens [2025-03-07 13:48:39,841] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.27it/s, est. speed input: 9398.79 toks/s, output: 34.29 toks/s]\u001b[A\n",
      "Processing Queries:  17%|â–ˆâ–‹        | 166/1000 [02:26<07:11,  1.93it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:48:40,282] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:48:40,283] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.000253044068813324 [2025-03-07 13:48:40,284] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4111 tokens in total) --hit rate 89.97% -- elapsed time 0.0014716088771820068 [2025-03-07 13:48:40,284] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4110 [2025-03-07 13:48:40,285] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:48:40,289] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:48:40,316] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4111 tokens and then stores 0 tokens [2025-03-07 13:48:40,318] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.26it/s, est. speed input: 9358.46 toks/s, output: 34.15 toks/s]\u001b[A\n",
      "Processing Queries:  17%|â–ˆâ–‹        | 167/1000 [02:26<07:01,  1.98it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:48:40,763] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:48:40,765] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.0007720589637756348 [2025-03-07 13:48:40,767] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4111 tokens in total) --hit rate 89.98% -- elapsed time 0.0028961803764104843 [2025-03-07 13:48:40,768] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4110 [2025-03-07 13:48:40,769] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:48:40,775] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:48:40,801] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4111 tokens and then stores 0 tokens [2025-03-07 13:48:40,803] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.23it/s, est. speed input: 9227.80 toks/s, output: 33.67 toks/s]\u001b[A\n",
      "Processing Queries:  17%|â–ˆâ–‹        | 168/1000 [02:27<06:56,  2.00it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:48:41,256] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:48:41,259] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.0007837004959583282 [2025-03-07 13:48:41,261] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4111 tokens in total) --hit rate 89.99% -- elapsed time 0.0029379036277532578 [2025-03-07 13:48:41,261] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4110 [2025-03-07 13:48:41,264] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:48:41,269] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:48:41,295] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4111 tokens and then stores 0 tokens [2025-03-07 13:48:41,297] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.23it/s, est. speed input: 9229.26 toks/s, output: 33.67 toks/s]\u001b[A\n",
      "Processing Queries:  17%|â–ˆâ–‹        | 169/1000 [02:27<06:53,  2.01it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:48:41,751] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:48:41,753] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.0007007569074630737 [2025-03-07 13:48:41,755] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4111 tokens in total) --hit rate 90.00% -- elapsed time 0.0028672702610492706 [2025-03-07 13:48:41,756] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4110 [2025-03-07 13:48:41,757] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:48:41,763] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:48:41,789] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4111 tokens and then stores 0 tokens [2025-03-07 13:48:41,791] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.23it/s, est. speed input: 9241.01 toks/s, output: 33.72 toks/s]\u001b[A\n",
      "Processing Queries:  17%|â–ˆâ–‹        | 170/1000 [02:28<06:52,  2.01it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:48:42,250] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:48:42,252] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 0 chunks [2025-03-07 13:48:42,253] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:385\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mReturning the original input! [2025-03-07 13:48:42,253] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:663\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.PREFILL: 1>] [2025-03-07 13:48:42,842] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the put() method [2025-03-07 13:48:42,928] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:99\n",
      "\u001b[33mINFO LMCache: \u001b[0mStored/updated 17 chunks, total time 0.00s, make chunks time 0.00s [2025-03-07 13:48:42,929] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:315\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 0 tokens and then stores 4112 tokens [2025-03-07 13:48:42,931] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.06s/it, est. speed input: 3906.80 toks/s, output: 12.35 toks/s]\u001b[A\n",
      "Processing Queries:  17%|â–ˆâ–‹        | 171/1000 [02:29<09:23,  1.47it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:48:43,354] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:48:43,356] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.000726575031876564 [2025-03-07 13:48:43,358] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4112 tokens in total) --hit rate 89.93% -- elapsed time 0.0028542783111333847 [2025-03-07 13:48:43,359] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4111 [2025-03-07 13:48:43,360] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:48:43,367] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:48:43,393] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4112 tokens and then stores 0 tokens [2025-03-07 13:48:43,395] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.53it/s, est. speed input: 10463.54 toks/s, output: 33.08 toks/s]\u001b[A\n",
      "Processing Queries:  17%|â–ˆâ–‹        | 172/1000 [02:29<08:23,  1.64it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:48:43,786] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:48:43,788] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.00026647746562957764 [2025-03-07 13:48:43,789] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4112 tokens in total) --hit rate 89.94% -- elapsed time 0.001569356769323349 [2025-03-07 13:48:43,789] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4111 [2025-03-07 13:48:43,790] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:48:43,795] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:48:43,820] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4112 tokens and then stores 0 tokens [2025-03-07 13:48:43,821] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.57it/s, est. speed input: 10657.37 toks/s, output: 33.69 toks/s]\u001b[A\n",
      "Processing Queries:  17%|â–ˆâ–‹        | 173/1000 [02:30<07:38,  1.80it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:48:44,213] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:48:44,214] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.00025615282356739044 [2025-03-07 13:48:44,216] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4112 tokens in total) --hit rate 89.95% -- elapsed time 0.0020786039531230927 [2025-03-07 13:48:44,216] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4111 [2025-03-07 13:48:44,218] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:48:44,222] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:48:44,248] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4112 tokens and then stores 0 tokens [2025-03-07 13:48:44,249] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.56it/s, est. speed input: 10611.66 toks/s, output: 33.55 toks/s]\u001b[A\n",
      "Processing Queries:  17%|â–ˆâ–‹        | 174/1000 [02:30<07:06,  1.94it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:48:44,652] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:48:44,654] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.00025992095470428467 [2025-03-07 13:48:44,655] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4112 tokens in total) --hit rate 89.96% -- elapsed time 0.0022440552711486816 [2025-03-07 13:48:44,656] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4111 [2025-03-07 13:48:44,657] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:48:44,662] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:48:44,688] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4112 tokens and then stores 0 tokens [2025-03-07 13:48:44,689] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.55it/s, est. speed input: 10541.20 toks/s, output: 33.32 toks/s]\u001b[A\n",
      "Processing Queries:  18%|â–ˆâ–Š        | 175/1000 [02:30<06:47,  2.03it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:48:45,080] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:48:45,081] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.00025756843388080597 [2025-03-07 13:48:45,082] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4112 tokens in total) --hit rate 89.97% -- elapsed time 0.0018151160329580307 [2025-03-07 13:48:45,083] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4111 [2025-03-07 13:48:45,084] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:48:45,089] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:48:45,114] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4112 tokens and then stores 0 tokens [2025-03-07 13:48:45,116] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.57it/s, est. speed input: 10643.85 toks/s, output: 33.65 toks/s]\u001b[A\n",
      "Processing Queries:  18%|â–ˆâ–Š        | 176/1000 [02:31<06:29,  2.11it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:48:45,505] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:48:45,506] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.0002489350736141205 [2025-03-07 13:48:45,508] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4112 tokens in total) --hit rate 89.97% -- elapsed time 0.0020594168454408646 [2025-03-07 13:48:45,508] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4111 [2025-03-07 13:48:45,509] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:48:45,514] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:48:45,539] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4112 tokens and then stores 0 tokens [2025-03-07 13:48:45,541] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.56it/s, est. speed input: 10614.33 toks/s, output: 33.56 toks/s]\u001b[A\n",
      "Processing Queries:  18%|â–ˆâ–Š        | 177/1000 [02:31<06:17,  2.18it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:48:45,943] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:48:45,944] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.0002534780651330948 [2025-03-07 13:48:45,946] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4112 tokens in total) --hit rate 89.98% -- elapsed time 0.0020491648465394974 [2025-03-07 13:48:45,946] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4111 [2025-03-07 13:48:45,948] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:48:45,952] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:48:45,978] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4112 tokens and then stores 0 tokens [2025-03-07 13:48:45,981] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.55it/s, est. speed input: 10557.97 toks/s, output: 33.38 toks/s]\u001b[A\n",
      "Processing Queries:  18%|â–ˆâ–Š        | 178/1000 [02:32<06:12,  2.21it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:48:46,376] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:48:46,378] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.0007306169718503952 [2025-03-07 13:48:46,380] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4112 tokens in total) --hit rate 89.99% -- elapsed time 0.002760840579867363 [2025-03-07 13:48:46,381] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4111 [2025-03-07 13:48:46,382] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:48:46,387] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:48:46,413] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4112 tokens and then stores 0 tokens [2025-03-07 13:48:46,415] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.53it/s, est. speed input: 10471.66 toks/s, output: 33.10 toks/s]\u001b[A\n",
      "Processing Queries:  18%|â–ˆâ–Š        | 179/1000 [02:32<06:07,  2.23it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:48:46,811] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:48:46,813] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.00025401823222637177 [2025-03-07 13:48:46,815] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4112 tokens in total) --hit rate 90.00% -- elapsed time 0.0023858435451984406 [2025-03-07 13:48:46,816] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4111 [2025-03-07 13:48:46,817] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:48:46,822] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:48:46,847] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4112 tokens and then stores 0 tokens [2025-03-07 13:48:46,849] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.54it/s, est. speed input: 10522.77 toks/s, output: 33.27 toks/s]\u001b[A\n",
      "Processing Queries:  18%|â–ˆâ–Š        | 180/1000 [02:33<06:03,  2.25it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:48:47,259] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:48:47,262] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 0 chunks [2025-03-07 13:48:47,262] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:385\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mReturning the original input! [2025-03-07 13:48:47,263] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:663\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.PREFILL: 1>] [2025-03-07 13:48:47,855] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the put() method [2025-03-07 13:48:47,939] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:99\n",
      "\u001b[33mINFO LMCache: \u001b[0mStored/updated 17 chunks, total time 0.00s, make chunks time 0.00s [2025-03-07 13:48:47,941] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:315\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 0 tokens and then stores 4126 tokens [2025-03-07 13:48:47,942] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.31s/it, est. speed input: 3165.33 toks/s, output: 16.88 toks/s]\u001b[A\n",
      "Processing Queries:  18%|â–ˆâ–Š        | 181/1000 [02:34<09:48,  1.39it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:48:48,604] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:48:48,606] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.0002890266478061676 [2025-03-07 13:48:48,608] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4126 tokens in total) --hit rate 89.93% -- elapsed time 0.00218958780169487 [2025-03-07 13:48:48,608] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4125 [2025-03-07 13:48:48,609] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:48:48,615] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:48:48,640] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4126 tokens and then stores 0 tokens [2025-03-07 13:48:48,642] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.58it/s, est. speed input: 6562.31 toks/s, output: 34.99 toks/s]\u001b[A\n",
      "Processing Queries:  18%|â–ˆâ–Š        | 182/1000 [02:35<09:36,  1.42it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:48:49,276] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:48:49,278] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.0004567820578813553 [2025-03-07 13:48:49,279] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4126 tokens in total) --hit rate 89.94% -- elapsed time 0.0019915904849767685 [2025-03-07 13:48:49,280] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4125 [2025-03-07 13:48:49,281] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:48:49,286] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:48:49,312] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4126 tokens and then stores 0 tokens [2025-03-07 13:48:49,314] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.58it/s, est. speed input: 6534.52 toks/s, output: 34.84 toks/s]\u001b[A\n",
      "Processing Queries:  18%|â–ˆâ–Š        | 183/1000 [02:35<09:28,  1.44it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:48:49,968] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:48:49,971] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.0003106631338596344 [2025-03-07 13:48:49,973] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4126 tokens in total) --hit rate 89.95% -- elapsed time 0.002828609198331833 [2025-03-07 13:48:49,974] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4125 [2025-03-07 13:48:49,975] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:48:49,981] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:48:50,007] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4126 tokens and then stores 0 tokens [2025-03-07 13:48:50,009] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.55it/s, est. speed input: 6434.84 toks/s, output: 34.31 toks/s]\u001b[A\n",
      "Processing Queries:  18%|â–ˆâ–Š        | 184/1000 [02:36<09:28,  1.43it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:48:50,664] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:48:50,666] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.0003110654652118683 [2025-03-07 13:48:50,667] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4126 tokens in total) --hit rate 89.96% -- elapsed time 0.0023319274187088013 [2025-03-07 13:48:50,668] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4125 [2025-03-07 13:48:50,669] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:48:50,675] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:48:50,701] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4126 tokens and then stores 0 tokens [2025-03-07 13:48:50,703] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.54it/s, est. speed input: 6376.98 toks/s, output: 34.00 toks/s]\u001b[A\n",
      "Processing Queries:  18%|â–ˆâ–Š        | 185/1000 [02:37<09:29,  1.43it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:48:51,361] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:48:51,363] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.00028343498706817627 [2025-03-07 13:48:51,365] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4126 tokens in total) --hit rate 89.97% -- elapsed time 0.0022005606442689896 [2025-03-07 13:48:51,365] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4125 [2025-03-07 13:48:51,366] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:48:51,371] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:48:51,397] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4126 tokens and then stores 0 tokens [2025-03-07 13:48:51,399] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.58it/s, est. speed input: 6551.04 toks/s, output: 34.93 toks/s]\u001b[A\n",
      "Processing Queries:  19%|â–ˆâ–Š        | 186/1000 [02:37<09:24,  1.44it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:48:52,031] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:48:52,033] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.00025267526507377625 [2025-03-07 13:48:52,034] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4126 tokens in total) --hit rate 89.97% -- elapsed time 0.0020732469856739044 [2025-03-07 13:48:52,035] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4125 [2025-03-07 13:48:52,036] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:48:52,041] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:48:52,067] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4126 tokens and then stores 0 tokens [2025-03-07 13:48:52,070] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.55it/s, est. speed input: 6414.58 toks/s, output: 34.20 toks/s]\u001b[A\n",
      "Processing Queries:  19%|â–ˆâ–Š        | 187/1000 [02:38<09:21,  1.45it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:48:52,737] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:48:52,740] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.0007580369710922241 [2025-03-07 13:48:52,742] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4126 tokens in total) --hit rate 89.98% -- elapsed time 0.003843933343887329 [2025-03-07 13:48:52,743] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4125 [2025-03-07 13:48:52,745] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:48:52,752] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:48:52,778] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4126 tokens and then stores 0 tokens [2025-03-07 13:48:52,780] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.54it/s, est. speed input: 6371.96 toks/s, output: 33.97 toks/s]\u001b[A\n",
      "Processing Queries:  19%|â–ˆâ–‰        | 188/1000 [02:39<09:24,  1.44it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:48:53,437] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:48:53,440] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.00029599107801914215 [2025-03-07 13:48:53,442] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4126 tokens in total) --hit rate 89.99% -- elapsed time 0.003459695726633072 [2025-03-07 13:48:53,443] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4125 [2025-03-07 13:48:53,444] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:48:53,450] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:48:53,475] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4126 tokens and then stores 0 tokens [2025-03-07 13:48:53,478] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.56it/s, est. speed input: 6490.77 toks/s, output: 34.61 toks/s]\u001b[A\n",
      "Processing Queries:  19%|â–ˆâ–‰        | 189/1000 [02:40<09:23,  1.44it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:48:54,118] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:48:54,121] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.0003031175583600998 [2025-03-07 13:48:54,122] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4126 tokens in total) --hit rate 90.00% -- elapsed time 0.0026273708790540695 [2025-03-07 13:48:54,123] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4125 [2025-03-07 13:48:54,125] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:48:54,130] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:48:54,156] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4126 tokens and then stores 0 tokens [2025-03-07 13:48:54,158] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.56it/s, est. speed input: 6489.35 toks/s, output: 34.60 toks/s]\u001b[A\n",
      "Processing Queries:  19%|â–ˆâ–‰        | 190/1000 [02:40<09:18,  1.45it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:48:54,797] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:48:54,800] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 0 chunks [2025-03-07 13:48:54,801] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:385\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mReturning the original input! [2025-03-07 13:48:54,803] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:663\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.PREFILL: 1>] [2025-03-07 13:48:55,394] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the put() method [2025-03-07 13:48:55,496] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:99\n",
      "\u001b[33mINFO LMCache: \u001b[0mStored/updated 17 chunks, total time 0.00s, make chunks time 0.00s [2025-03-07 13:48:55,498] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:315\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 0 tokens and then stores 4111 tokens [2025-03-07 13:48:55,499] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.14s/it, est. speed input: 3623.32 toks/s, output: 13.22 toks/s]\u001b[A\n",
      "Processing Queries:  19%|â–ˆâ–‰        | 191/1000 [02:41<11:16,  1.20it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:48:55,994] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:48:55,997] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.0006673280149698257 [2025-03-07 13:48:55,999] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4111 tokens in total) --hit rate 89.93% -- elapsed time 0.003100976347923279 [2025-03-07 13:48:56,000] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4110 [2025-03-07 13:48:56,002] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:48:56,008] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:48:56,035] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4111 tokens and then stores 0 tokens [2025-03-07 13:48:56,037] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.17it/s, est. speed input: 8983.51 toks/s, output: 32.78 toks/s]\u001b[A\n",
      "Processing Queries:  19%|â–ˆâ–‰        | 192/1000 [02:42<09:58,  1.35it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:48:56,510] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:48:56,513] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.00037113577127456665 [2025-03-07 13:48:56,515] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4111 tokens in total) --hit rate 89.94% -- elapsed time 0.003188343718647957 [2025-03-07 13:48:56,516] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4110 [2025-03-07 13:48:56,517] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:48:56,523] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:48:56,550] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4111 tokens and then stores 0 tokens [2025-03-07 13:48:56,552] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.17it/s, est. speed input: 8975.59 toks/s, output: 32.75 toks/s]\u001b[A\n",
      "Processing Queries:  19%|â–ˆâ–‰        | 193/1000 [02:42<09:03,  1.49it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:48:57,032] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:48:57,034] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.000745600089430809 [2025-03-07 13:48:57,036] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4111 tokens in total) --hit rate 89.95% -- elapsed time 0.0032714251428842545 [2025-03-07 13:48:57,038] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4110 [2025-03-07 13:48:57,039] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:48:57,046] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:48:57,073] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4111 tokens and then stores 0 tokens [2025-03-07 13:48:57,076] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.15it/s, est. speed input: 8904.67 toks/s, output: 32.49 toks/s]\u001b[A\n",
      "Processing Queries:  19%|â–ˆâ–‰        | 194/1000 [02:43<08:26,  1.59it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:48:57,555] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:48:57,558] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.00035508163273334503 [2025-03-07 13:48:57,560] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4111 tokens in total) --hit rate 89.96% -- elapsed time 0.0033792443573474884 [2025-03-07 13:48:57,561] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4110 [2025-03-07 13:48:57,563] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:48:57,569] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:48:57,595] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4111 tokens and then stores 0 tokens [2025-03-07 13:48:57,597] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.17it/s, est. speed input: 8987.23 toks/s, output: 32.79 toks/s]\u001b[A\n",
      "Processing Queries:  20%|â–ˆâ–‰        | 195/1000 [02:43<07:59,  1.68it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:48:58,070] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:48:58,073] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.00035225413739681244 [2025-03-07 13:48:58,074] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4111 tokens in total) --hit rate 89.97% -- elapsed time 0.002733662724494934 [2025-03-07 13:48:58,075] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4110 [2025-03-07 13:48:58,077] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:48:58,083] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:48:58,109] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4111 tokens and then stores 0 tokens [2025-03-07 13:48:58,113] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.16it/s, est. speed input: 8945.55 toks/s, output: 32.64 toks/s]\u001b[A\n",
      "Processing Queries:  20%|â–ˆâ–‰        | 196/1000 [02:44<07:40,  1.75it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:48:58,580] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:48:58,582] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.0003164820373058319 [2025-03-07 13:48:58,584] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4111 tokens in total) --hit rate 89.97% -- elapsed time 0.0030956678092479706 [2025-03-07 13:48:58,585] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4110 [2025-03-07 13:48:58,587] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:48:58,592] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:48:58,618] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4111 tokens and then stores 0 tokens [2025-03-07 13:48:58,620] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.20it/s, est. speed input: 9135.86 toks/s, output: 33.33 toks/s]\u001b[A\n",
      "Processing Queries:  20%|â–ˆâ–‰        | 197/1000 [02:44<07:22,  1.81it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:48:59,079] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:48:59,082] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.0003178529441356659 [2025-03-07 13:48:59,083] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4111 tokens in total) --hit rate 89.98% -- elapsed time 0.0023518428206443787 [2025-03-07 13:48:59,084] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4110 [2025-03-07 13:48:59,085] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:48:59,091] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:48:59,117] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4111 tokens and then stores 0 tokens [2025-03-07 13:48:59,119] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.22it/s, est. speed input: 9169.76 toks/s, output: 33.46 toks/s]\u001b[A\n",
      "Processing Queries:  20%|â–ˆâ–‰        | 198/1000 [02:45<07:09,  1.87it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:48:59,575] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:48:59,578] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.00030181556940078735 [2025-03-07 13:48:59,579] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4111 tokens in total) --hit rate 89.99% -- elapsed time 0.002363976091146469 [2025-03-07 13:48:59,580] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4110 [2025-03-07 13:48:59,581] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:48:59,587] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:48:59,613] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4111 tokens and then stores 0 tokens [2025-03-07 13:48:59,615] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.22it/s, est. speed input: 9198.31 toks/s, output: 33.56 toks/s]\u001b[A\n",
      "Processing Queries:  20%|â–ˆâ–‰        | 199/1000 [02:45<06:59,  1.91it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:49:00,071] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:49:00,073] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.00029942579567432404 [2025-03-07 13:49:00,075] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4111 tokens in total) --hit rate 90.00% -- elapsed time 0.0029801465570926666 [2025-03-07 13:49:00,076] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4110 [2025-03-07 13:49:00,078] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:49:00,083] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:49:00,109] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4111 tokens and then stores 0 tokens [2025-03-07 13:49:00,110] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.22it/s, est. speed input: 9185.89 toks/s, output: 33.52 toks/s]\u001b[A\n",
      "Processing Queries:  20%|â–ˆâ–ˆ        | 200/1000 [02:46<06:52,  1.94it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:49:00,571] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:49:00,573] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 0 chunks [2025-03-07 13:49:00,574] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:385\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mReturning the original input! [2025-03-07 13:49:00,575] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:663\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.PREFILL: 1>] [2025-03-07 13:49:01,166] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the put() method [2025-03-07 13:49:01,269] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:99\n",
      "\u001b[33mINFO LMCache: \u001b[0mStored/updated 17 chunks, total time 0.00s, make chunks time 0.00s [2025-03-07 13:49:01,270] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:315\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 0 tokens and then stores 4111 tokens [2025-03-07 13:49:01,272] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.56s/it, est. speed input: 2638.25 toks/s, output: 19.89 toks/s]\u001b[A\n",
      "Processing Queries:  20%|â–ˆâ–ˆ        | 201/1000 [02:48<11:13,  1.19it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:49:02,184] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:49:02,187] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.0003221314400434494 [2025-03-07 13:49:02,189] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4111 tokens in total) --hit rate 89.93% -- elapsed time 0.003055356442928314 [2025-03-07 13:49:02,190] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4110 [2025-03-07 13:49:02,191] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:49:02,197] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:49:02,223] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4111 tokens and then stores 0 tokens [2025-03-07 13:49:02,225] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.13it/s, est. speed input: 4681.94 toks/s, output: 35.30 toks/s]\u001b[A\n",
      "Processing Queries:  20%|â–ˆâ–ˆ        | 202/1000 [02:49<11:35,  1.15it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:49:03,122] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:49:03,124] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.00030584074556827545 [2025-03-07 13:49:03,126] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4111 tokens in total) --hit rate 89.94% -- elapsed time 0.00281730480492115 [2025-03-07 13:49:03,127] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4110 [2025-03-07 13:49:03,128] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:49:03,134] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:49:03,160] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4111 tokens and then stores 0 tokens [2025-03-07 13:49:03,162] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.13it/s, est. speed input: 4685.17 toks/s, output: 35.33 toks/s]\u001b[A\n",
      "Processing Queries:  20%|â–ˆâ–ˆ        | 203/1000 [02:49<11:49,  1.12it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:49:04,063] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:49:04,065] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.0004643350839614868 [2025-03-07 13:49:04,067] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4111 tokens in total) --hit rate 89.95% -- elapsed time 0.0034332275390625 [2025-03-07 13:49:04,068] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4110 [2025-03-07 13:49:04,069] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:49:04,075] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:49:04,101] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4111 tokens and then stores 0 tokens [2025-03-07 13:49:04,103] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.13it/s, est. speed input: 4681.77 toks/s, output: 35.30 toks/s]\u001b[A\n",
      "Processing Queries:  20%|â–ˆâ–ˆ        | 204/1000 [02:50<12:00,  1.10it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:49:05,007] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:49:05,010] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.0003234483301639557 [2025-03-07 13:49:05,012] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4111 tokens in total) --hit rate 89.96% -- elapsed time 0.0032016579061746597 [2025-03-07 13:49:05,013] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4110 [2025-03-07 13:49:05,014] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:49:05,020] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:49:05,045] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4111 tokens and then stores 0 tokens [2025-03-07 13:49:05,047] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.14it/s, est. speed input: 4694.91 toks/s, output: 35.40 toks/s]\u001b[A\n",
      "Processing Queries:  20%|â–ˆâ–ˆ        | 205/1000 [02:51<12:08,  1.09it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:49:05,946] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:49:05,948] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.00029493123292922974 [2025-03-07 13:49:05,950] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4111 tokens in total) --hit rate 89.97% -- elapsed time 0.00304417684674263 [2025-03-07 13:49:05,951] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4110 [2025-03-07 13:49:05,953] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:49:05,958] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:49:05,984] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4111 tokens and then stores 0 tokens [2025-03-07 13:49:05,986] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.14it/s, est. speed input: 4684.81 toks/s, output: 35.33 toks/s]\u001b[A\n",
      "Processing Queries:  21%|â–ˆâ–ˆ        | 206/1000 [02:52<12:13,  1.08it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:49:06,882] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:49:06,885] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.00030729547142982483 [2025-03-07 13:49:06,886] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4111 tokens in total) --hit rate 89.97% -- elapsed time 0.002675611525774002 [2025-03-07 13:49:06,887] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4110 [2025-03-07 13:49:06,888] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:49:06,894] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:49:06,919] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4111 tokens and then stores 0 tokens [2025-03-07 13:49:06,922] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.15it/s, est. speed input: 4727.57 toks/s, output: 35.65 toks/s]\u001b[A\n",
      "Processing Queries:  21%|â–ˆâ–ˆ        | 207/1000 [02:53<12:13,  1.08it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:49:07,793] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:49:07,795] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.00025077909231185913 [2025-03-07 13:49:07,797] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4111 tokens in total) --hit rate 89.98% -- elapsed time 0.0020837225019931793 [2025-03-07 13:49:07,797] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4110 [2025-03-07 13:49:07,798] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:49:07,803] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:49:07,828] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4111 tokens and then stores 0 tokens [2025-03-07 13:49:07,830] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.15it/s, est. speed input: 4754.49 toks/s, output: 35.85 toks/s]\u001b[A\n",
      "Processing Queries:  21%|â–ˆâ–ˆ        | 208/1000 [02:54<12:08,  1.09it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:49:08,698] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:49:08,700] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.00025942549109458923 [2025-03-07 13:49:08,701] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4111 tokens in total) --hit rate 89.99% -- elapsed time 0.002092767506837845 [2025-03-07 13:49:08,701] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4110 [2025-03-07 13:49:08,703] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:49:08,708] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:49:08,733] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4111 tokens and then stores 0 tokens [2025-03-07 13:49:08,735] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.15it/s, est. speed input: 4746.64 toks/s, output: 35.79 toks/s]\u001b[A\n",
      "Processing Queries:  21%|â–ˆâ–ˆ        | 209/1000 [02:55<12:04,  1.09it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:49:09,602] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:49:09,604] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.00025404058396816254 [2025-03-07 13:49:09,605] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4111 tokens in total) --hit rate 90.00% -- elapsed time 0.002042330801486969 [2025-03-07 13:49:09,606] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4110 [2025-03-07 13:49:09,607] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:49:09,612] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:49:09,637] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4111 tokens and then stores 0 tokens [2025-03-07 13:49:09,639] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.15it/s, est. speed input: 4744.37 toks/s, output: 35.78 toks/s]\u001b[A\n",
      "Processing Queries:  21%|â–ˆâ–ˆ        | 210/1000 [02:56<12:00,  1.10it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:49:10,509] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:49:10,511] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 0 chunks [2025-03-07 13:49:10,511] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:385\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mReturning the original input! [2025-03-07 13:49:10,512] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:663\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.PREFILL: 1>] [2025-03-07 13:49:11,103] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the put() method [2025-03-07 13:49:11,187] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:99\n",
      "\u001b[33mINFO LMCache: \u001b[0mStored/updated 17 chunks, total time 0.00s, make chunks time 0.00s [2025-03-07 13:49:11,188] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:315\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 0 tokens and then stores 4114 tokens [2025-03-07 13:49:11,189] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.08s/it, est. speed input: 3810.31 toks/s, output: 12.97 toks/s]\u001b[A\n",
      "Processing Queries:  21%|â–ˆâ–ˆ        | 211/1000 [02:57<12:48,  1.03it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:49:11,628] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:49:11,630] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.00024757906794548035 [2025-03-07 13:49:11,632] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4114 tokens in total) --hit rate 89.93% -- elapsed time 0.0021136868745088577 [2025-03-07 13:49:11,632] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4113 [2025-03-07 13:49:11,633] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:49:11,638] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:49:11,664] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4114 tokens and then stores 0 tokens [2025-03-07 13:49:11,665] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.40it/s, est. speed input: 9921.09 toks/s, output: 33.76 toks/s]\u001b[A\n",
      "Processing Queries:  21%|â–ˆâ–ˆ        | 212/1000 [02:57<10:44,  1.22it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:49:12,081] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:49:12,083] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.00025955215096473694 [2025-03-07 13:49:12,085] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4114 tokens in total) --hit rate 89.94% -- elapsed time 0.0021048635244369507 [2025-03-07 13:49:12,085] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4113 [2025-03-07 13:49:12,086] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:49:12,091] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:49:12,116] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4114 tokens and then stores 0 tokens [2025-03-07 13:49:12,118] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.40it/s, est. speed input: 9920.81 toks/s, output: 33.76 toks/s]\u001b[A\n",
      "Processing Queries:  21%|â–ˆâ–ˆâ–       | 213/1000 [02:58<09:17,  1.41it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:49:12,548] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:49:12,551] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.00030366890132427216 [2025-03-07 13:49:12,553] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4114 tokens in total) --hit rate 89.95% -- elapsed time 0.003147568553686142 [2025-03-07 13:49:12,554] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4113 [2025-03-07 13:49:12,555] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:49:12,561] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:49:12,586] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4114 tokens and then stores 0 tokens [2025-03-07 13:49:12,588] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.36it/s, est. speed input: 9776.62 toks/s, output: 33.27 toks/s]\u001b[A\n",
      "Processing Queries:  21%|â–ˆâ–ˆâ–       | 214/1000 [02:58<08:21,  1.57it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:49:13,010] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:49:13,012] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.0002917144447565079 [2025-03-07 13:49:13,013] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4114 tokens in total) --hit rate 89.96% -- elapsed time 0.002124173566699028 [2025-03-07 13:49:13,014] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4113 [2025-03-07 13:49:13,015] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:49:13,020] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:49:13,046] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4114 tokens and then stores 0 tokens [2025-03-07 13:49:13,048] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.35it/s, est. speed input: 9767.38 toks/s, output: 33.24 toks/s]\u001b[A\n",
      "Processing Queries:  22%|â–ˆâ–ˆâ–       | 215/1000 [02:59<07:39,  1.71it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:49:13,478] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:49:13,480] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.00029415637254714966 [2025-03-07 13:49:13,481] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4114 tokens in total) --hit rate 89.97% -- elapsed time 0.0021896418184041977 [2025-03-07 13:49:13,482] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4113 [2025-03-07 13:49:13,483] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:49:13,488] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:49:13,514] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4114 tokens and then stores 0 tokens [2025-03-07 13:49:13,516] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.36it/s, est. speed input: 9776.66 toks/s, output: 33.27 toks/s]\u001b[A\n",
      "Processing Queries:  22%|â–ˆâ–ˆâ–       | 216/1000 [02:59<07:10,  1.82it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:49:13,949] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:49:13,951] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.0004177019000053406 [2025-03-07 13:49:13,953] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4114 tokens in total) --hit rate 89.98% -- elapsed time 0.002493053674697876 [2025-03-07 13:49:13,954] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4113 [2025-03-07 13:49:13,954] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:49:13,961] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:49:13,986] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4114 tokens and then stores 0 tokens [2025-03-07 13:49:13,989] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.34it/s, est. speed input: 9724.29 toks/s, output: 33.09 toks/s]\u001b[A\n",
      "Processing Queries:  22%|â–ˆâ–ˆâ–       | 217/1000 [03:00<06:52,  1.90it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:49:14,421] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:49:14,423] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.00029512494802474976 [2025-03-07 13:49:14,424] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4114 tokens in total) --hit rate 89.98% -- elapsed time 0.0021987799555063248 [2025-03-07 13:49:14,425] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4113 [2025-03-07 13:49:14,426] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:49:14,431] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:49:14,457] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4114 tokens and then stores 0 tokens [2025-03-07 13:49:14,459] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.37it/s, est. speed input: 9797.99 toks/s, output: 33.34 toks/s]\u001b[A\n",
      "Processing Queries:  22%|â–ˆâ–ˆâ–       | 218/1000 [03:00<06:38,  1.96it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:49:14,894] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:49:14,896] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.0003108363598585129 [2025-03-07 13:49:14,897] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4114 tokens in total) --hit rate 89.99% -- elapsed time 0.0021874699741601944 [2025-03-07 13:49:14,898] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4113 [2025-03-07 13:49:14,899] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:49:14,904] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:49:14,930] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4114 tokens and then stores 0 tokens [2025-03-07 13:49:14,932] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.37it/s, est. speed input: 9808.15 toks/s, output: 33.38 toks/s]\u001b[A\n",
      "Processing Queries:  22%|â–ˆâ–ˆâ–       | 219/1000 [03:01<06:29,  2.01it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:49:15,369] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:49:15,371] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.0006210189312696457 [2025-03-07 13:49:15,373] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4114 tokens in total) --hit rate 90.00% -- elapsed time 0.0025668442249298096 [2025-03-07 13:49:15,373] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4113 [2025-03-07 13:49:15,375] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:49:15,381] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:49:15,406] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4114 tokens and then stores 0 tokens [2025-03-07 13:49:15,409] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.35it/s, est. speed input: 9753.21 toks/s, output: 33.19 toks/s]\u001b[A\n",
      "Processing Queries:  22%|â–ˆâ–ˆâ–       | 220/1000 [03:01<06:23,  2.03it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:49:15,845] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:49:15,848] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 0 chunks [2025-03-07 13:49:15,849] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:385\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mReturning the original input! [2025-03-07 13:49:15,851] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:663\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.PREFILL: 1>] [2025-03-07 13:49:16,442] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the put() method [2025-03-07 13:49:16,543] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:99\n",
      "\u001b[33mINFO LMCache: \u001b[0mStored/updated 17 chunks, total time 0.00s, make chunks time 0.00s [2025-03-07 13:49:16,544] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:315\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 0 tokens and then stores 4113 tokens [2025-03-07 13:49:16,546] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.06s/it, est. speed input: 3884.99 toks/s, output: 11.33 toks/s]\u001b[A\n",
      "Processing Queries:  22%|â–ˆâ–ˆâ–       | 221/1000 [03:02<08:48,  1.47it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:49:16,965] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:49:16,967] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.000315837562084198 [2025-03-07 13:49:16,969] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4113 tokens in total) --hit rate 89.93% -- elapsed time 0.002266349270939827 [2025-03-07 13:49:16,969] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4112 [2025-03-07 13:49:16,971] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:49:16,977] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:49:17,002] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4113 tokens and then stores 0 tokens [2025-03-07 13:49:17,004] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.70it/s, est. speed input: 11192.63 toks/s, output: 32.65 toks/s]\u001b[A\n",
      "Processing Queries:  22%|â–ˆâ–ˆâ–       | 222/1000 [03:03<07:49,  1.66it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:49:17,381] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:49:17,383] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.0002957768738269806 [2025-03-07 13:49:17,385] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4113 tokens in total) --hit rate 89.94% -- elapsed time 0.0022064149379730225 [2025-03-07 13:49:17,385] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4112 [2025-03-07 13:49:17,387] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:49:17,392] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:49:17,418] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4113 tokens and then stores 0 tokens [2025-03-07 13:49:17,420] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.70it/s, est. speed input: 11197.63 toks/s, output: 32.67 toks/s]\u001b[A\n",
      "Processing Queries:  22%|â–ˆâ–ˆâ–       | 223/1000 [03:03<07:05,  1.83it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:49:17,791] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:49:17,793] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.00031165964901447296 [2025-03-07 13:49:17,794] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4113 tokens in total) --hit rate 89.95% -- elapsed time 0.0022678691893815994 [2025-03-07 13:49:17,795] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4112 [2025-03-07 13:49:17,796] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:49:17,802] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:49:17,827] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4113 tokens and then stores 0 tokens [2025-03-07 13:49:17,829] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.70it/s, est. speed input: 11226.02 toks/s, output: 32.75 toks/s]\u001b[A\n",
      "Processing Queries:  22%|â–ˆâ–ˆâ–       | 224/1000 [03:04<06:32,  1.97it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:49:18,207] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:49:18,210] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.0003155246376991272 [2025-03-07 13:49:18,212] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4113 tokens in total) --hit rate 89.96% -- elapsed time 0.0030567962676286697 [2025-03-07 13:49:18,213] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4112 [2025-03-07 13:49:18,214] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:49:18,220] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:49:18,246] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4113 tokens and then stores 0 tokens [2025-03-07 13:49:18,248] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.67it/s, est. speed input: 11079.37 toks/s, output: 32.32 toks/s]\u001b[A\n",
      "Processing Queries:  22%|â–ˆâ–ˆâ–Ž       | 225/1000 [03:04<06:12,  2.08it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:49:18,627] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:49:18,629] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.0002998746931552887 [2025-03-07 13:49:18,631] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4113 tokens in total) --hit rate 89.97% -- elapsed time 0.0026751216500997543 [2025-03-07 13:49:18,632] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4112 [2025-03-07 13:49:18,633] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:49:18,638] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:49:18,664] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4113 tokens and then stores 0 tokens [2025-03-07 13:49:18,666] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.68it/s, est. speed input: 11134.06 toks/s, output: 32.48 toks/s]\u001b[A\n",
      "Processing Queries:  23%|â–ˆâ–ˆâ–Ž       | 226/1000 [03:04<05:57,  2.17it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:49:19,057] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:49:19,060] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.0003419797867536545 [2025-03-07 13:49:19,062] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4113 tokens in total) --hit rate 89.98% -- elapsed time 0.0034682955592870712 [2025-03-07 13:49:19,063] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4112 [2025-03-07 13:49:19,064] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:49:19,070] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:49:19,096] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4113 tokens and then stores 0 tokens [2025-03-07 13:49:19,099] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.68it/s, est. speed input: 11099.93 toks/s, output: 32.38 toks/s]\u001b[A\n",
      "Processing Queries:  23%|â–ˆâ–ˆâ–Ž       | 227/1000 [03:05<05:49,  2.21it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:49:19,475] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:49:19,478] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.0005849245935678482 [2025-03-07 13:49:19,479] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4113 tokens in total) --hit rate 89.98% -- elapsed time 0.0032240524888038635 [2025-03-07 13:49:19,481] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4112 [2025-03-07 13:49:19,482] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:49:19,488] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:49:19,514] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4113 tokens and then stores 0 tokens [2025-03-07 13:49:19,517] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.67it/s, est. speed input: 11080.61 toks/s, output: 32.33 toks/s]\u001b[A\n",
      "Processing Queries:  23%|â–ˆâ–ˆâ–Ž       | 228/1000 [03:05<05:41,  2.26it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:49:19,900] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:49:19,903] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.0003618486225605011 [2025-03-07 13:49:19,905] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4113 tokens in total) --hit rate 89.99% -- elapsed time 0.003475986421108246 [2025-03-07 13:49:19,906] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4112 [2025-03-07 13:49:19,907] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:49:19,913] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:49:19,939] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4113 tokens and then stores 0 tokens [2025-03-07 13:49:19,941] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.68it/s, est. speed input: 11109.12 toks/s, output: 32.41 toks/s]\u001b[A\n",
      "Processing Queries:  23%|â–ˆâ–ˆâ–Ž       | 229/1000 [03:06<05:36,  2.29it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:49:20,319] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:49:20,321] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.0002974029630422592 [2025-03-07 13:49:20,323] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4113 tokens in total) --hit rate 90.00% -- elapsed time 0.0031142160296440125 [2025-03-07 13:49:20,324] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4112 [2025-03-07 13:49:20,326] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:49:20,331] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:49:20,357] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4113 tokens and then stores 0 tokens [2025-03-07 13:49:20,359] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.68it/s, est. speed input: 11113.14 toks/s, output: 32.42 toks/s]\u001b[A\n",
      "Processing Queries:  23%|â–ˆâ–ˆâ–Ž       | 230/1000 [03:06<05:32,  2.32it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:49:20,736] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:49:20,738] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 0 chunks [2025-03-07 13:49:20,739] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:385\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mReturning the original input! [2025-03-07 13:49:20,740] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:663\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.PREFILL: 1>] [2025-03-07 13:49:21,328] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the put() method [2025-03-07 13:49:21,431] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:99\n",
      "\u001b[33mINFO LMCache: \u001b[0mStored/updated 17 chunks, total time 0.00s, make chunks time 0.00s [2025-03-07 13:49:21,432] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:315\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 0 tokens and then stores 4114 tokens [2025-03-07 13:49:21,433] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.16s/it, est. speed input: 3556.61 toks/s, output: 13.83 toks/s]\u001b[A\n",
      "Processing Queries:  23%|â–ˆâ–ˆâ–Ž       | 231/1000 [03:07<08:30,  1.51it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:49:21,954] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:49:21,956] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.00031799450516700745 [2025-03-07 13:49:21,958] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4114 tokens in total) --hit rate 89.93% -- elapsed time 0.0022585466504096985 [2025-03-07 13:49:21,958] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4113 [2025-03-07 13:49:21,959] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:49:21,965] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:49:21,991] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4114 tokens and then stores 0 tokens [2025-03-07 13:49:21,993] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.22it/s, est. speed input: 9190.95 toks/s, output: 33.51 toks/s]\u001b[A\n",
      "Processing Queries:  23%|â–ˆâ–ˆâ–Ž       | 232/1000 [03:08<07:53,  1.62it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:49:22,469] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:49:22,472] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.0004741847515106201 [2025-03-07 13:49:22,474] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4114 tokens in total) --hit rate 89.94% -- elapsed time 0.004147004336118698 [2025-03-07 13:49:22,475] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4113 [2025-03-07 13:49:22,477] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:49:22,485] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:49:22,512] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4114 tokens and then stores 0 tokens [2025-03-07 13:49:22,514] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.19it/s, est. speed input: 9035.27 toks/s, output: 32.94 toks/s]\u001b[A\n",
      "Processing Queries:  23%|â–ˆâ–ˆâ–Ž       | 233/1000 [03:08<07:30,  1.70it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:49:22,975] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:49:22,977] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.0003210678696632385 [2025-03-07 13:49:22,978] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4114 tokens in total) --hit rate 89.95% -- elapsed time 0.002026813104748726 [2025-03-07 13:49:22,978] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4113 [2025-03-07 13:49:22,979] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:49:22,984] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:49:23,009] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4114 tokens and then stores 0 tokens [2025-03-07 13:49:23,012] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.22it/s, est. speed input: 9208.15 toks/s, output: 33.57 toks/s]\u001b[A\n",
      "Processing Queries:  23%|â–ˆâ–ˆâ–Ž       | 234/1000 [03:09<07:10,  1.78it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:49:23,471] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:49:23,474] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.00035803578794002533 [2025-03-07 13:49:23,476] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4114 tokens in total) --hit rate 89.96% -- elapsed time 0.0035559460520744324 [2025-03-07 13:49:23,477] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4113 [2025-03-07 13:49:23,478] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:49:23,484] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:49:23,510] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4114 tokens and then stores 0 tokens [2025-03-07 13:49:23,513] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.20it/s, est. speed input: 9121.95 toks/s, output: 33.26 toks/s]\u001b[A\n",
      "Processing Queries:  24%|â–ˆâ–ˆâ–Ž       | 235/1000 [03:09<06:55,  1.84it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:49:23,973] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:49:23,976] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.00048166513442993164 [2025-03-07 13:49:23,977] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4114 tokens in total) --hit rate 89.97% -- elapsed time 0.0034504197537899017 [2025-03-07 13:49:23,978] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4113 [2025-03-07 13:49:23,980] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:49:23,985] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:49:24,011] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4114 tokens and then stores 0 tokens [2025-03-07 13:49:24,013] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.21it/s, est. speed input: 9165.04 toks/s, output: 33.42 toks/s]\u001b[A\n",
      "Processing Queries:  24%|â–ˆâ–ˆâ–Ž       | 236/1000 [03:10<06:45,  1.89it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:49:24,473] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:49:24,475] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.0003222953528165817 [2025-03-07 13:49:24,477] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4114 tokens in total) --hit rate 89.98% -- elapsed time 0.0030334237962961197 [2025-03-07 13:49:24,478] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4113 [2025-03-07 13:49:24,479] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:49:24,485] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:49:24,511] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4114 tokens and then stores 0 tokens [2025-03-07 13:49:24,513] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.19it/s, est. speed input: 9066.14 toks/s, output: 33.05 toks/s]\u001b[A\n",
      "Processing Queries:  24%|â–ˆâ–ˆâ–Ž       | 237/1000 [03:10<06:38,  1.91it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:49:24,988] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:49:24,991] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.0003538522869348526 [2025-03-07 13:49:24,993] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4114 tokens in total) --hit rate 89.98% -- elapsed time 0.0033427923917770386 [2025-03-07 13:49:24,994] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4113 [2025-03-07 13:49:24,996] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:49:25,002] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:49:25,028] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4114 tokens and then stores 0 tokens [2025-03-07 13:49:25,030] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.17it/s, est. speed input: 9002.39 toks/s, output: 32.82 toks/s]\u001b[A\n",
      "Processing Queries:  24%|â–ˆâ–ˆâ–       | 238/1000 [03:11<06:36,  1.92it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:49:25,610] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:49:25,613] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.00046949274837970734 [2025-03-07 13:49:25,615] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4114 tokens in total) --hit rate 89.99% -- elapsed time 0.0028844736516475677 [2025-03-07 13:49:25,616] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4113 [2025-03-07 13:49:25,617] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:49:25,625] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:49:25,652] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4114 tokens and then stores 0 tokens [2025-03-07 13:49:25,654] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.18it/s, est. speed input: 9042.57 toks/s, output: 32.97 toks/s]\u001b[A\n",
      "Processing Queries:  24%|â–ˆâ–ˆâ–       | 239/1000 [03:12<06:59,  1.81it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:49:26,114] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:49:26,116] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.0002943519502878189 [2025-03-07 13:49:26,118] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4114 tokens in total) --hit rate 90.00% -- elapsed time 0.003164183348417282 [2025-03-07 13:49:26,119] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4113 [2025-03-07 13:49:26,120] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:49:26,126] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:49:26,152] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4114 tokens and then stores 0 tokens [2025-03-07 13:49:26,154] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.21it/s, est. speed input: 9162.61 toks/s, output: 33.41 toks/s]\u001b[A\n",
      "Processing Queries:  24%|â–ˆâ–ˆâ–       | 240/1000 [03:12<06:46,  1.87it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:49:26,610] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:49:26,613] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 0 chunks [2025-03-07 13:49:26,614] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:385\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mReturning the original input! [2025-03-07 13:49:26,615] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:663\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.PREFILL: 1>] [2025-03-07 13:49:27,205] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the put() method [2025-03-07 13:49:27,310] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:99\n",
      "\u001b[33mINFO LMCache: \u001b[0mStored/updated 17 chunks, total time 0.00s, make chunks time 0.00s [2025-03-07 13:49:27,311] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:315\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 0 tokens and then stores 4118 tokens [2025-03-07 13:49:27,313] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.29s/it, est. speed input: 3189.95 toks/s, output: 15.49 toks/s]\u001b[A\n",
      "Processing Queries:  24%|â–ˆâ–ˆâ–       | 241/1000 [03:13<09:49,  1.29it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:49:27,974] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:49:27,977] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.00038464367389678955 [2025-03-07 13:49:27,979] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4118 tokens in total) --hit rate 89.93% -- elapsed time 0.0034415870904922485 [2025-03-07 13:49:27,980] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4117 [2025-03-07 13:49:27,982] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:49:27,990] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:49:28,016] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4118 tokens and then stores 0 tokens [2025-03-07 13:49:28,019] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.69it/s, est. speed input: 6969.71 toks/s, output: 33.85 toks/s]\u001b[A\n",
      "Processing Queries:  24%|â–ˆâ–ˆâ–       | 242/1000 [03:14<09:22,  1.35it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:49:28,610] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:49:28,612] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.0003025177866220474 [2025-03-07 13:49:28,613] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4118 tokens in total) --hit rate 89.94% -- elapsed time 0.001337418332695961 [2025-03-07 13:49:28,613] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4117 [2025-03-07 13:49:28,614] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:49:28,619] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:49:28,644] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4118 tokens and then stores 0 tokens [2025-03-07 13:49:28,645] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.72it/s, est. speed input: 7136.41 toks/s, output: 34.66 toks/s]\u001b[A\n",
      "Processing Queries:  24%|â–ˆâ–ˆâ–       | 243/1000 [03:15<08:54,  1.42it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:49:29,231] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:49:29,233] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.0002996344119310379 [2025-03-07 13:49:29,235] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4118 tokens in total) --hit rate 89.95% -- elapsed time 0.0027573127299547195 [2025-03-07 13:49:29,236] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4117 [2025-03-07 13:49:29,237] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:49:29,243] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:49:29,268] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4118 tokens and then stores 0 tokens [2025-03-07 13:49:29,270] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.70it/s, est. speed input: 7037.82 toks/s, output: 34.18 toks/s]\u001b[A\n",
      "Processing Queries:  24%|â–ˆâ–ˆâ–       | 244/1000 [03:15<08:36,  1.46it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:49:29,881] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:49:29,883] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.0003903917968273163 [2025-03-07 13:49:29,885] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4118 tokens in total) --hit rate 89.96% -- elapsed time 0.0029164236038923264 [2025-03-07 13:49:29,886] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4117 [2025-03-07 13:49:29,888] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:49:29,895] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:49:29,922] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4118 tokens and then stores 0 tokens [2025-03-07 13:49:29,925] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.66it/s, est. speed input: 6897.62 toks/s, output: 33.50 toks/s]\u001b[A\n",
      "Processing Queries:  24%|â–ˆâ–ˆâ–       | 245/1000 [03:16<08:30,  1.48it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:49:30,537] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:49:30,541] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.0003563482314348221 [2025-03-07 13:49:30,543] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4118 tokens in total) --hit rate 89.97% -- elapsed time 0.003579961135983467 [2025-03-07 13:49:30,544] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4117 [2025-03-07 13:49:30,546] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:49:30,553] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:49:30,579] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4118 tokens and then stores 0 tokens [2025-03-07 13:49:30,581] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.69it/s, est. speed input: 6984.89 toks/s, output: 33.92 toks/s]\u001b[A\n",
      "Processing Queries:  25%|â–ˆâ–ˆâ–       | 246/1000 [03:17<08:23,  1.50it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:49:31,175] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:49:31,177] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.00032496266067028046 [2025-03-07 13:49:31,179] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4118 tokens in total) --hit rate 89.98% -- elapsed time 0.002409251406788826 [2025-03-07 13:49:31,180] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4117 [2025-03-07 13:49:31,181] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:49:31,187] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:49:31,212] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4118 tokens and then stores 0 tokens [2025-03-07 13:49:31,214] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.71it/s, est. speed input: 7075.12 toks/s, output: 34.36 toks/s]\u001b[A\n",
      "Processing Queries:  25%|â–ˆâ–ˆâ–       | 247/1000 [03:17<08:14,  1.52it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:49:31,802] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:49:31,804] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.0003113076090812683 [2025-03-07 13:49:31,805] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4118 tokens in total) --hit rate 89.98% -- elapsed time 0.00267598032951355 [2025-03-07 13:49:31,806] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4117 [2025-03-07 13:49:31,807] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:49:31,813] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:49:31,838] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4118 tokens and then stores 0 tokens [2025-03-07 13:49:31,840] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.69it/s, est. speed input: 7018.91 toks/s, output: 34.09 toks/s]\u001b[A\n",
      "Processing Queries:  25%|â–ˆâ–ˆâ–       | 248/1000 [03:18<08:08,  1.54it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:49:32,459] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:49:32,462] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.0003576483577489853 [2025-03-07 13:49:32,465] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4118 tokens in total) --hit rate 89.99% -- elapsed time 0.0033225342631340027 [2025-03-07 13:49:32,466] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4117 [2025-03-07 13:49:32,467] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:49:32,474] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:49:32,501] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4118 tokens and then stores 0 tokens [2025-03-07 13:49:32,504] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.67it/s, est. speed input: 6905.15 toks/s, output: 33.54 toks/s]\u001b[A\n",
      "Processing Queries:  25%|â–ˆâ–ˆâ–       | 249/1000 [03:19<08:11,  1.53it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:49:33,105] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:49:33,108] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.000302143394947052 [2025-03-07 13:49:33,109] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 17 chunks (4118 tokens in total) --hit rate 90.00% -- elapsed time 0.0029737818986177444 [2025-03-07 13:49:33,110] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:415\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mInjected token number: 4117 [2025-03-07 13:49:33,112] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:615\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mRebuilt the input! [2025-03-07 13:49:33,117] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:660\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.SUFFIX_PREFILL: 4>] [2025-03-07 13:49:33,143] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 4118 tokens and then stores 0 tokens [2025-03-07 13:49:33,145] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.71it/s, est. speed input: 7049.28 toks/s, output: 34.24 toks/s]\u001b[A\n",
      "Processing Queries:  25%|â–ˆâ–ˆâ–Œ       | 250/1000 [03:19<08:05,  1.54it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:49:33,735] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:49:33,737] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mRetrieved 0 chunks [2025-03-07 13:49:33,738] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:385\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mReturning the original input! [2025-03-07 13:49:33,739] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:663\n",
      "\u001b[33mINFO LMCache: \u001b[0mKV cache saving mode: [<StoreStatus.PREFILL: 1>] [2025-03-07 13:49:34,327] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:131\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the put() method [2025-03-07 13:49:34,431] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:99\n",
      "\u001b[33mINFO LMCache: \u001b[0mStored/updated 17 chunks, total time 0.00s, make chunks time 0.00s [2025-03-07 13:49:34,432] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:315\n",
      "\u001b[33mDEBUG LMCache: \u001b[0mStore skips 0 tokens and then stores 4110 tokens [2025-03-07 13:49:34,434] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_adapter.py:490\n",
      "\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.45s/it, est. speed input: 2836.72 toks/s, output: 17.94 toks/s]\u001b[A\n",
      "Processing Queries:  25%|â–ˆâ–ˆâ–Œ       | 251/1000 [03:21<11:15,  1.11it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[33mINFO LMCache: \u001b[0mKV cache retrieving mode: RetrieveStatus.PREFILL [2025-03-07 13:49:35,224] -- /usr/local/lib/python3.11/dist-packages/lmcache_vllm/vllm_injection.py:52\n",
      "\u001b[33mINFO LMCache: \u001b[0mUsing default batched implementation of the get() method [2025-03-07 13:49:35,226] -- /usr/local/lib/python3.11/dist-packages/lmcache/storage_backend/abstract_backend.py:120\n",
      "\u001b[33mINFO LMCache: \u001b[0mConcatenated 17 chunks -- elapsed time 0.00041226670145988464 [2025-03-07 13:49:35,227] -- /usr/local/lib/python3.11/dist-packages/lmcache/cache_engine.py:402\n"
     ]
    }
   ],
   "source": [
    "print(\"Running Advanced RAG (Raptor + LM Cache Optimized)\")\n",
    "advanced_rag_cache_responses = run_advanced_rag(repeated_summaries, vector_store_raptor, tokenizer, generator_optimized_model, \"advanced_rag_cache_raptor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "del generator_optimized_model\n",
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
